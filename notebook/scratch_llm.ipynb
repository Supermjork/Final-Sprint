{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM From Scratch\n",
    "#### (By: Mark Ehab Aziz)\n",
    "##### (Built under Python 3.11.4)\n",
    "\n",
    "Graduation Project for ***Sprints***, to build a LLM (Large Language Model) from scratch without the use of any Transformer related libraries, in order to classify comments scraped from some given website.\n",
    "\n",
    "Notebook will include:\n",
    "- List of imported libraries.\n",
    "- Deep dive into the data.\n",
    "- Markdown cells to explain the detail of every step, along with reason.\n",
    "- Custom defined functions and classes.\n",
    "- Own Transformer from scratch.\n",
    "\n",
    "List of Libraries/Dependencies:\n",
    "- [Pandas](https://pandas.pydata.org/docs/index.html)\n",
    "- [NumPy](https://numpy.org/doc/stable/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - [NLTK Regex Tokenizer](https://www.nltk.org/howto/tokenize.html)\n",
    "    - [NLTK Snowball Stemmer](https://www.nltk.org/howto/stem.html)\n",
    "    - [NLTK WordNet Lemmatizer](https://www.nltk.org/howto/wordnet.html)\n",
    "    - [NLTK English Words (Stopwords too)](https://www.nltk.org/howto/corpus.html)\n",
    "- [PyTorch (`torch` and `torch.nn`)](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Note: NLTK will be used later for tokenization using RegEx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "Importing libraries that will be used in order to implement our own LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                             # Pandas for DataFrame manipulation\n",
    "import numpy as np                              # Linear Algebra and Mathematical Operations\n",
    "import nltk                                     # Downloading word sets\n",
    "from nltk.stem import SnowballStemmer           # Stemming\n",
    "from nltk.stem import WordNetLemmatizer         # Lemmatization (Better word yields)\n",
    "from nltk.corpus import stopwords               # Stopwords\n",
    "from nltk.tokenize import regexp_tokenize       # Tokenization using RegEx (Regular Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading external dependencies to be used\n",
    "# Such as Stopwords, English-only words\n",
    "\n",
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# English Only\n",
    "nltk.download('words')\n",
    "\n",
    "# Wordnet for Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setting constants for both normal and stopwords in English\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "ENGLISH_WORDS = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "Loading the data using Panda's `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data with 2 ways\n",
    "# 1 - Reading within my Github Repo\n",
    "txt_dat = pd.read_csv('../dataset/train.csv')\n",
    "\n",
    "# 2 - Reading within the same folder\n",
    "#txt_dat = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Looking at the data from multiple perspectives.\n",
    "\n",
    "Using `head()` and `tail()` methods to look at what columns there are within the dataframe, which may be useful and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining n rows\n",
    "n = 5\n",
    "\n",
    "# Calling and showing first and last n rows\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we can see that we have about $159571$ entries.\n",
    "We can also see that there are columns as such:\n",
    "- `id`: Shows message id code from whatever platform this was scraped from. (Will be dropped)\n",
    "- `comment_text`: main star of the show, text we have to process, it has upper-case letters, escape characters (`\\n`,`\\r`, etc.), and most likely special characters (non-latin-alphabet), URLs, IP addresses; removal needed.\n",
    "- `toxic` (And its derivatives): Labelling the comment if it were a toxic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining What To Clean\n",
    "In the above mentioned description of the data, there exists aspects that need removal, explanation will follow within this cell.\n",
    "\n",
    "- Removal of id: Due to the nature of what this project is about, and usually a general case, IDs are not usually if at all used to gain insights from data, due to them being unique per entry and being nothing but an enumeration of entries.\n",
    "- Cleaning within `comment_text`: Cleaning the comments will be separated into $x$ steps, namely:\n",
    "    - Space and Tab removal: Removing of Newline Characters (`\\n`) and Tabs (`\\t`) will help remove special characters from text and avoiding noise within.\n",
    "    - Uppercase: Due to standarisations within the NLP field, it has been agreed upon to change any uppercase letter into lowercase, to mitigate the face that words can be written using multiple permutations of the same letters but with different cases, so in order to for the machine to recognise the word and not have to account for $n^{52}$ different combinations for the word (Where $n$ is number of characters to represent a word, and $52$ is due to both upper and lower cases of a letter)\n",
    "    - URL: Removal of URLs will prove beneficial, as it doesn't contribute much to the corpus nor is considered a baseline for labelling the comment.\n",
    "    - IP Address: For security reasons.\n",
    "    - Special Characters: Due to the non-existence of any in the Latin Alphabet which English uses, it would be useless to bother with them, although if this was a multi-lingual dataset, some characters from different languages would be needed to keep.\n",
    "- Derivatives of `toxic`: For purposes of simplicity, I have decided to *\"collapse\"* the values that follow after the *`toxic`* column, as in summing the values into said column, then swapping values $>1$ to be just $1$ to indicate toxicity, implying that $0$ would be for non-toxic comments; as a result, this will water down into just a \"Binary Classification\" problem based on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsing the column values onto toxic\n",
    "txt_dat['toxic'] = txt_dat.iloc[:, 2:].sum(axis = 1)\n",
    "\n",
    "# Drop the collapsed columns\n",
    "# Along with the id column\n",
    "txt_dat.drop(columns = ['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the head and describing the toxic column\n",
    "display(txt_dat.head(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, due to collapsing the values onto `toxic`, values over 1 arose hence they will need handling in order to keep the labels as $1$ and $0$. A function doing just so will be implemented later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Patterns\n",
    "Identified Patterns that will be required to be used to capture specific instances of removable slices of text within the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newline, Tab spaces, etc.\n",
    "newline_tabspace = r'[\\r\\n\\t]'\n",
    "\n",
    "# Match words starting with Uppercase letters\n",
    "upper_words = r'([A-Z])\\w+'\n",
    "\n",
    "# Match Words that start with either Upper/lowercase letters\n",
    "upperlower_words = r'[A-Za-z]\\w+'\n",
    "\n",
    "# Sub/Superscript characters\n",
    "# Encountered previously\n",
    "sub_sup_scripts = r'\\w[²³¹⁰ⁱ⁴⁵⁶⁷⁸⁹⁺⁻⁼⁽⁾ⁿ]+'\n",
    "\n",
    "# Punctuation\n",
    "punc_pattern = r'[!\\?.,\\':;\"]'\n",
    "\n",
    "# Single Letters\n",
    "single_letter = r'((?<=^)|(?<= )).((?=$)|(?= ))'\n",
    "\n",
    "# Match URLs\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w+?\\.\\w+])+([a-zA-Z0-9\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)_\\-\\=\\+\\\\\\/\\?\\.\\:\\;\\'\\,]*)?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and such\n",
    "Functions to be used and applied on the dataframe.\n",
    "\n",
    "Each function will be commented, whilst also writing an explanation to a grouped cell of functions indicating their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df: pd.DataFrame, colname: str):\n",
    "    return np.where(df[colname] > 0, 1, 0)\n",
    "\n",
    "def clean_comment(df: pd.DataFrame, colname: str):\n",
    "    # Remove the newlines, tabs, etc.\n",
    "    df[colname].replace(newline_tabspace, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove URLs\n",
    "    df[colname].replace(url_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove superscript, subscript\n",
    "    df[colname].replace(sub_sup_scripts, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove punctuation\n",
    "    df[colname].replace(punc_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove Single Letters\n",
    "    df[colname].replace(single_letter, ' ', regex = True, inplace = True)\n",
    "\n",
    "    return df[colname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A walkthrough the above functions:\n",
    "- `binarize(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "Due to the rise of values larger than 1 after collapsing the other column values, a requirement for binarizing (Chose to make the labels binary, for simplicity's sake) the values, $1$ for Toxic and $0$ for Non-Toxic, this is achieved by assigning any value larger than 0 to 1 otherwise stays as 0.\n",
    "\n",
    "- `clean_comment(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "The nature of the data given, is that it has a lot of *whitespaces*, *tabs*, *special characters*, *urls*, *punctuations*, *single letters*, which are all bound to be removed.\n",
    "The function removes:\n",
    "    - Newlines\n",
    "    - Tabs\n",
    "    - URLs\n",
    "    - Sub/Super Scripts\n",
    "    - Punctuation\n",
    "    - Single Letters (From removal of some punctuation symbols)\n",
    "\n",
    "Both functions return the updated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dat['comment_text'] = clean_comment(txt_dat, 'comment_text')\n",
    "\n",
    "txt_dat['toxic'] = binarize(txt_dat, 'toxic')\n",
    "\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no extra characters, like apostrophe's, extra punctuations, etc.\n",
    "\n",
    "As well as having the maximum value to be 1 within the `toxic` column, although the mean is closer to 0 which suggest that the data is really imbalanced, which will require fixing later down the line, ideally it would be better to be closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing\n",
    "Preprocessing the data with ways that are under the NLP PreProcessing Standards, as in:\n",
    "- Tokenizing.\n",
    "- Lowercasing.\n",
    "- Removal of Stopwords.\n",
    "- Removing Non-English words.\n",
    "- Lemmatization/Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting up a sentence or a corpus into plain words (tokens) for variable reasons:\n",
    "- To know which words are present.\n",
    "- Count them.\n",
    "- Et cetera.\n",
    "\n",
    "In my implementation, I will be using the `regexp_tokenizer()` in order to tokenize the text within the `comment_text` column.\n",
    "\n",
    "As well as keeping the tokens within the same dataframe as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a specific column within a passed DataFrame\n",
    "def tokenize(dataframe: pd.DataFrame, colname: str):\n",
    "    return [regexp_tokenize(row, upperlower_words) for row in dataframe[colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the tokenize() function on a new column within the main DataFrame\n",
    "txt_dat['tokens'] = tokenize(txt_dat, 'comment_text')\n",
    "\n",
    "# Checking if the function worked\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the sentences have been tokenised, each word is its own string, within a list of strings under the new `tokens` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case Words\n",
    "By conventional standards, changing the case of the words into lowercase has been agreed upon to negate the need to account for case sensitivity with the operations that follow to preprocess text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lowercase text\n",
    "def token_lower(token_list: list):\n",
    "    return [token.lower() for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the column\n",
    "# Iterable is of type \"list\"\n",
    "txt_dat['tokens'] = [token_lower(row) for row in txt_dat['tokens']]\n",
    "\n",
    "# Displaying results of applying the above function\n",
    "txt_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining how the function works first before observation.\n",
    "\n",
    "Take the column `tokens`, it contains lists of tokens for each comment like: `[Explanation, Why, the, edits, made, under, my...]`.\n",
    "\n",
    "Said list is what is being iterated on, which gets passed to the function, so for every iteration a new list of words gets passed to the function; but within the function, we iterate over each item within said list, so `'Explanantion` and `'Why'`, etc..\n",
    "\n",
    "Therefore, passing over each token, we apply the `.lower()` function for the `str` data type; thus we get a lowercase version of the string.\n",
    "\n",
    "After every iteration of lists, they get returned and reassigned where the original list used to be.\n",
    "\n",
    "As for observation, we can see that the words did indeed become lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "Removing of stopwords drastically decreases the amount of words that need to be taken into consideration by the model, and contribute little to nothing regarding meaning.\n",
    "\n",
    "Removing them would shave off redundant computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stopwords\n",
    "def stopword_remover(wordlist: list):\n",
    "    return [word for word in wordlist if word not in ENGLISH_STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying word count for first few lists\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "# Iterating over column\n",
    "# Iterable is the list of\n",
    "# Lowercase english words\n",
    "txt_dat['tokens'] = [stopword_remover(row) for row in txt_dat['tokens']]\n",
    "\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by the decrease in the word count per token list, there was indeed a removal of redundant words.\n",
    "\n",
    "As for how the function works, it is similar to how the previous ones work, passing the iterated list to iterate over the tokens within and apply a function; in our case it is a conditional to just include words that are not within the collection of words which are considered stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization & Stemming\n",
    "Lemmatising/Stemming both have the same target in mind, to reach the root of the word but their difference is the algorithm used.\n",
    "\n",
    "Stemming utilises a \"Porter\" algorithm which essentially just chops off common word endings, due to Stemmer using a crude old method, which is aimed for speed and efficiency, unlike lemmatizaton which morphologically analyses lexical changes in words to revert them back to their roots, unlike the chopping of \"commonly found prefixes/suffixes\" which stemming does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(wordlist: list):\n",
    "    return [WordNetLemmatizer().lemmatize(token) for token in wordlist]\n",
    "\n",
    "def stem(wordlist):\n",
    "    return [SnowballStemmer(\"english\").stem(token) for token in wordlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, after some look up, lemmatization is beneficial to reduce the word to its base, in a more human context and understanding, mainly used in chatbots, etc.\n",
    "\n",
    "Whilest the stemmer is used to purely chop off the words' endings to get to the base, used more in sentiment analysis, which is our current case with the binary classification, hence it will be used after lemmatizer and english word remover are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-English Words Removal\n",
    "Due to the nature of the internet, it is bound that people don't use proper English words, but due to current knowledge of NLP process, it would be better to remove words that do not belong to the English language as a whole.\n",
    "\n",
    "An important note to keep in mind is, calling this after lemmatization/stemming, due to `words` not having all forms of a singular word which may cause it to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Non-English word remover\n",
    "def englishify(wordlist: list):\n",
    "    return [word for word in wordlist if word in ENGLISH_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "txt_dat['tokens'] = [englishify(row) for row in txt_dat['tokens']]\n",
    "\n",
    "# Print some entries\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up NLP Preprocessing\n",
    "In this section, a whole function will take in a column in order to apply ***all*** the above functions in one go.\n",
    "\n",
    "Description and explanation for function order will be explained in either comments or a markdown cell following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, colname: str):\n",
    "    # List to hold the lists of tokens\n",
    "    tokenlist = tokenize(df, colname)\n",
    "\n",
    "    # Get number of tokens originally\n",
    "    token_num = sum([len(t_list) for t_list in tokenlist])\n",
    "\n",
    "    # Lowercase list of token lists\n",
    "    # t_list -> (t)oken_list\n",
    "    lower_tokens = [token_lower(t_list) for t_list in tokenlist]\n",
    "\n",
    "    # Stopword-free list\n",
    "    # tl_list -> (t)oken(l)lowercase_list\n",
    "    stopwordless = [stopword_remover(tl_list) for tl_list in lower_tokens]\n",
    "\n",
    "    # List of token lemmas\n",
    "    # tls_list -> (t)oken(l)owercase(s)topwordless_list\n",
    "    lemmas = [lemmatize(tls_list) for tls_list in stopwordless]\n",
    "\n",
    "    # List of Lemmas that only exist in english\n",
    "    # tlsl_list -> (t)oken(l)owercase(s)topwordless(l)lemma_list\n",
    "    englishified = [englishify(tlsl_list) for tlsl_list in lemmas]\n",
    "\n",
    "    # Get number of tokens after processing\n",
    "    # (Englishify is the last function to remove)\n",
    "    proc_num = sum([len(tlsle_list) for tlsle_list in englishified])\n",
    "\n",
    "    # Reduction\n",
    "    reduced_by = 1 - proc_num / token_num\n",
    "\n",
    "    # Print counts and percentage\n",
    "    print(\"Original Token Count : {}\\nProcessed Token Count: {}\\nReduction Percentage : {:.2%}\".format(token_num, proc_num, reduced_by))\n",
    "\n",
    "    # Return tuple of token list and processed tokens\n",
    "    return [stem(word) for word in englishified]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation for how the wrap-up function works:\n",
    "- Passing the DataFrame to be processed, along with the column name to be tokenized.\n",
    "    - Each row within the column gets tokenized with use of the predefined RegEx expressions up above.\n",
    "- Calculating the number of tokens to get a feel for how much we'll be dealing with.\n",
    "- Applying a lowercase function to ever list within the list of lists assigned to the output of the tokenization function.\n",
    "- Removing stopwords from the tokens.\n",
    "- Lemmatizing the words, to reach the root of the word (Useful for removing non-English words later).\n",
    "- Removal of Non-English words.\n",
    "- Calculating the amount of tokens, `englishify()` is the last function that removes any tokens, afterwhich the reduction of tokens is calculated.\n",
    "- Printing the calculated numbers.\n",
    "- Returning a tuple of tokenised words and stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning output tuple to \"new\" column of tokens and new for processed\n",
    "txt_dat['processed'] = preprocess(txt_dat, 'comment_text')\n",
    "\n",
    "# Printing to see progress\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Using PyTorch, a transformer will be implemented from scratch to classify comments into toxic or non-toxic.\n",
    "\n",
    "Research Paper used as reference:\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "Website Articles that helped:\n",
    "- [The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "- [Text Classifier with PyTorch Transformer](https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/)\n",
    "- [Basic Transformer With PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "- [Positional Encoding](https://theaisummer.com/positional-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch                                    # PyTorch\n",
    "import torch.nn    as nn                        # PyTorch Neural Networks\n",
    "import torch.optim as optim                     # Optimizers\n",
    "from torch.utils.data import Dataset            # Dataset of words\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
    "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight[input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add ***\"positional encodings\"*** to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed.\n",
    "\n",
    "Thus, we will be working with the following trig functions of different frequencies:\n",
    "$$ PE(pos, 2i) = sin(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "$$ PE(pos, 2i + 1) = cos(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi^{[1]}$. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of\n",
    "$PE_{pos}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "The main star of the show and the key that emphasizes why the transformer is the better architecture.\n",
    "\n",
    "Through remembering the prompts and contextually \"understanding\" the words whilst taking into consideration the meaning and value of the prior words.\n",
    "\n",
    "Hence, the Multi-Head Attetntion formula is described by the following equation:\n",
    "$$ MultiHeadAttention(Q, K, V) = Concat(head_1, \\ldots, head_h)W^O$$\n",
    "\n",
    "Where each $h_i$ is derived from the following formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{model}}})\\cdot V$$\n",
    "\n",
    "In this particular paper, the model has the hyperparameter of $h^{[1]}$ set to $8$, such that $d_k = d_v = d_{model} / h = 64$\n",
    "\n",
    "Softmax:\n",
    "$$ softmax(\\vec{z})_{i} = \\frac{e^{z_{i}}}{\\sum^{n}_{j=1} e^{z_{j}}}$$\n",
    "Where:\n",
    "- $\\vec{z} \\rightarrow$ Input Vector.\n",
    "- $e^{z_i} \\rightarrow$ Standard Exponentiation for Input vector.\n",
    "- $e^{z_j} \\rightarrow$ Standard Exponentiation for Output Vector.\n",
    "- $n \\rightarrow$ number of classes in multi-class classifier.\n",
    "\n",
    "---\n",
    "[1]: h is the number of attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.01):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_per_head = d_model // nhead\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "\n",
    "        query = query.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim_per_head)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        context = torch.matmul(self.dropout(attention_probs), value)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.nhead * self.dim_per_head)\n",
    "\n",
    "        output = self.out_linear(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$ FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is $d_{model} = {512}$, and the inner-layer has dimensionality\n",
    "$d_{ff} = {2048}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.norm_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(self.norm_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.norm_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        output = (x - mean) / (std + self.eps)\n",
    "        output = (output * self.alpha.unsqueeze(0)) + self.beta.unsqueeze(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "Consisting of $6^{[1]}$ layers of the same process. From the original design we can also see that there is a skip forward from:\n",
    "- Before the MHA (Multi-Head Attention)\n",
    "- Before the FFN (Feed Forward Network)\n",
    "\n",
    "As well as consisting of $4$ blocks in total for each Encoder Block:\n",
    "- Multi-Head Attention.\n",
    "- First Normalization.\n",
    "- Feed-Foward Network.\n",
    "- Second Normalization.\n",
    "\n",
    "It would be pointless if we just continuously passed the output from one block without transforming it in any way or shape, hence the introudction of the $LayerNorm(x + SubLayer(x))$ function, where $SubLayer(x)$ is a function produced by the layer itself to facilitate the residual connections, and produces an output of dimension $d_{model} = 512$.\n",
    "\n",
    "---\n",
    "\n",
    "[1]: Included in the original Paper, Section 3.1; Encoder Paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of encoder\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=128, dropout=0.0):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiheadAttention(d_model, nhead)\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "        attn_output = attn_output.view(-1, src.size(1), self.d_model)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        ffn_output = self.feed_forward(src)\n",
    "        src = src + self.dropout2(ffn_output)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "# N Layers of encoders\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedfwd):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(d_model, nhead, dim_feedfwd) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two\n",
    "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
    "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
    "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
    "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position $i$ can depend only on the known outputs at positions less than $i$.\n",
    "\n",
    "#### Note: Due to our implementation for it to be a Text-Classifier version of the Transformer, This will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Transformer\n",
    "For our purpose of this project, we are tasked to classify the given data of Comments into a binary label of `toxic`, as mentioned before the $1$ stands for `toxic` and the $0$ stands for `non-toxic`.\n",
    "\n",
    "This is achieved by only using the Encoder part of the transformer, which means that only the N-Layers of the Encoder will be at work, No need for Cross-Multi-Head Attention or any of the Expected Output to be put into a Decoder, Just the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, num_classes: int = 2, nhead: int = 8, num_encoder_layers: int = 6, vocab_size: int = 1000, feedfwd_dim: int = 1024, max_seq_len: int = 100):\n",
    "        super().__init__()\n",
    "        self.model_dim = d_model\n",
    "        self.num_head = nhead\n",
    "        self.num_enc_layers = num_encoder_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.d_ff = feedfwd_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = Embedding(self.vocab_size, self.model_dim)\n",
    "        self.transformer_enc = TransformerEncoder(self.model_dim, self.num_head, self.num_enc_layers, feedfwd_dim)\n",
    "\n",
    "        self.fc = nn.Linear(self.model_dim, self.num_classes)\n",
    "        self.positional_encoding = PositionalEncoding(self.model_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        src = self.positional_encoding(embedded)\n",
    "        output = self.transformer_enc(src)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        logits = self.fc(output[:, -1, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_data:\n",
    "            logits = model(inputs)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        print(\"Epoch [{}/{}], Loss: {:.6f}\".format(epoch + 1, num_epochs, average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        average_loss = total_loss / len(test_loader)\n",
    "        accuracy = total_correct / total_samples\n",
    "\n",
    "        print(\"Test Loss: {:.4f}, Accuracy: {:.4f}\".format(average_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data, seq_len:int):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Create vocabulary and index mapping here\n",
    "        vocab = Counter([token for tokens in txt_dat['processed'] for token in tokens])\n",
    "        min_word_count = 5\n",
    "        filtered_vocab = [word for word, count in vocab.items() if count >= min_word_count]\n",
    "        word_to_idx = {word: idx + 1 for idx, word in enumerate(filtered_vocab)}\n",
    "        word_to_idx['<PAD>'] = 0\n",
    "        self.w_t_idx = word_to_idx\n",
    "        self.vocab_size = len(word_to_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def voc_size(self):\n",
    "        return len(self.w_t_idx) - 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        src = item.iloc[0]  # Input sequence\n",
    "        label = item.iloc[1]  # Binary label\n",
    "\n",
    "        # Convert tokens to numerical indices and pad sequences\n",
    "        src = [self.w_t_idx.get(token, 0) for token in src]  # Use word_to_idx with a default value of 0\n",
    "        src = src[:self.seq_len] + [0] * (self.seq_len - len(src))  # Padding with 0\n",
    "        src = torch.tensor(src, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return src, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_dataset(dataset, test_size=0.2):\n",
    "    dataset_size = len(dataset)\n",
    "    num_test_samples = int(test_size * dataset_size)\n",
    "    num_train_samples = dataset_size - num_test_samples\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [num_train_samples, num_test_samples])\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    \n",
    "    # Pad inputs with zeros\n",
    "    padded_inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_inputs, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(tokenized_sentences, embedding_dim):\n",
    "    # Create vocabulary and assign each word a unique index\n",
    "    vocab = set([word for sentence in tokenized_sentences for word in sentence])\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    # Create word embedding layer\n",
    "    embedding = Embedding(len(vocab), embedding_dim)\n",
    "\n",
    "    # Convert tokenized sentences to indexes\n",
    "    indexed_sentences = [[word_to_idx[word] for word in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "    # Convert indexed sentences to tensor\n",
    "    tensor_sentences = [torch.tensor(sentence).to(torch.int64) for sentence in indexed_sentences]\n",
    "\n",
    "    # Apply word embedding to tensor sentences\n",
    "    embedded_sentences = [embedding(sentence) for sentence in tensor_sentences]\n",
    "\n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it All Together\n",
    "Dividing Data and turning it into pytorch tensor compatible input.\n",
    "\n",
    "Input into the transformer implemented above and evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the Data\n",
    "whole_data = VocabDataset(txt_dat[['processed', 'toxic']], 100)\n",
    "\n",
    "train_ds, test_ds = train_test_split_dataset(whole_data, 0.2)\n",
    "\n",
    "# Loaders\n",
    "train_load = DataLoader(train_ds, 100, True, collate_fn = collate_fn)\n",
    "test_load = DataLoader(test_ds, 100, True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some params\n",
    "Epochs = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loops\n",
    "model = ClassifierTransformer(d_model = 100, num_classes = 2, nhead = 4, num_encoder_layers = 2, vocab_size = whole_data.voc_size())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_model(model, train_load, criterion, optimizer, Epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "evaluate_model(model, test_load, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
