{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM From Scratch\n",
    "#### (By: Mark Ehab Aziz)\n",
    "##### (Built under Python 3.11.4)\n",
    "\n",
    "Graduation Project for ***Sprints***, to build a LLM (Large Language Model) from scratch without the use of any Transformer related libraries, in order to classify comments scraped from some given website.\n",
    "\n",
    "Notebook will include:\n",
    "- List of imported libraries.\n",
    "- Deep dive into the data.\n",
    "- Markdown cells to explain the detail of every step, along with reason.\n",
    "- Custom defined functions and classes.\n",
    "- Own Transformer from scratch.\n",
    "\n",
    "List of Libraries/Dependencies:\n",
    "- [Pandas](https://pandas.pydata.org/docs/index.html)\n",
    "- [NumPy](https://numpy.org/doc/stable/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - [NLTK Regex Tokenizer](https://www.nltk.org/howto/tokenize.html)\n",
    "    - [NLTK Snowball Stemmer](https://www.nltk.org/howto/stem.html)\n",
    "    - [NLTK WordNet Lemmatizer](https://www.nltk.org/howto/wordnet.html)\n",
    "    - [NLTK English Words (Stopwords too)](https://www.nltk.org/howto/corpus.html)\n",
    "- [PyTorch (`torch` and `torch.nn`)](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Note: NLTK will be used later for tokenization using RegEx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "Importing libraries that will be used in order to implement our own LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                             # Pandas for DataFrame manipulation\n",
    "import numpy as np                              # Linear Algebra and Mathematical Operations\n",
    "import nltk                                     # Downloading word sets\n",
    "from nltk.stem import SnowballStemmer           # Stemming\n",
    "from nltk.stem import WordNetLemmatizer         # Lemmatization (Better word yields)\n",
    "from nltk.corpus import stopwords               # Stopwords\n",
    "from nltk.tokenize import regexp_tokenize       # Tokenization using RegEx (Regular Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading external dependencies to be used\n",
    "# Such as Stopwords, English-only words\n",
    "\n",
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Wordnet for Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setting constants for both normal and stopwords in English\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "ENGLISH_WORDS = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "Loading the data using Panda's `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data with 2 ways\n",
    "# 1 - Reading within my Github Repo\n",
    "txt_dat = pd.read_csv('../dataset/train.csv')\n",
    "\n",
    "# 2 - Reading within the same folder\n",
    "#txt_dat = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Looking at the data from multiple perspectives.\n",
    "\n",
    "Using `head()` and `tail()` methods to look at what columns there are within the dataframe, which may be useful and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\r\\n\\r\\nThat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\r\\n\\r\\nUmm, theres no actual article ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\r\\nAnd ... I really don't think you understa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\r\\n\\r\\nThat...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\r\\n\\r\\nUmm, theres no actual article ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\r\\nAnd ... I really don't think you understa...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining n rows\n",
    "n = 5\n",
    "\n",
    "# Calling and showing first and last n rows\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we can see that we have about $159571$ entries.\n",
    "We can also see that there are columns as such:\n",
    "- `id`: Shows message id code from whatever platform this was scraped from. (Will be dropped)\n",
    "- `comment_text`: main star of the show, text we have to process, it has upper-case letters, escape characters (`\\n`,`\\r`, etc.), and most likely special characters (non-latin-alphabet), URLs, IP addresses; removal needed.\n",
    "- `toxic` (And its derivatives): Labelling the comment if it were a toxic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining What To Clean\n",
    "In the above mentioned description of the data, there exists aspects that need removal, explanation will follow within this cell.\n",
    "\n",
    "- Removal of id: Due to the nature of what this project is about, and usually a general case, IDs are not usually if at all used to gain insights from data, due to them being unique per entry and being nothing but an enumeration of entries.\n",
    "- Cleaning within `comment_text`: Cleaning the comments will be separated into $x$ steps, namely:\n",
    "    - Space and Tab removal: Removing of Newline Characters (`\\n`) and Tabs (`\\t`) will help remove special characters from text and avoiding noise within.\n",
    "    - Uppercase: Due to standarisations within the NLP field, it has been agreed upon to change any uppercase letter into lowercase, to mitigate the face that words can be written using multiple permutations of the same letters but with different cases, so in order to for the machine to recognise the word and not have to account for $n^{52}$ different combinations for the word (Where $n$ is number of characters to represent a word, and $52$ is due to both upper and lower cases of a letter)\n",
    "    - URL: Removal of URLs will prove beneficial, as it doesn't contribute much to the corpus nor is considered a baseline for labelling the comment.\n",
    "    - IP Address: For security reasons.\n",
    "    - Special Characters: Due to the non-existence of any in the Latin Alphabet which English uses, it would be useless to bother with them, although if this was a multi-lingual dataset, some characters from different languages would be needed to keep.\n",
    "- Derivatives of `toxic`: For purposes of simplicity, I have decided to *\"collapse\"* the values that follow after the *`toxic`* column, as in summing the values into said column, then swapping values $>1$ to be just $1$ to indicate toxicity, implying that $0$ would be for non-toxic comments; as a result, this will water down into just a \"Binary Classification\" problem based on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsing the column values onto toxic\n",
    "txt_dat['toxic'] = txt_dat.iloc[:, 2:].sum(axis = 1)\n",
    "\n",
    "# Drop the collapsed columns\n",
    "# Along with the id column\n",
    "txt_dat.drop(columns = ['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\r\\nWhy the edits made under my use...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.219952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.748260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159571.000000\n",
       "mean        0.219952\n",
       "std         0.748260\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         6.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printing the head and describing the toxic column\n",
    "display(txt_dat.head(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, due to collapsing the values onto `toxic`, values over 1 arose hence they will need handling in order to keep the labels as $1$ and $0$. A function doing just so will be implemented later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Patterns\n",
    "Identified Patterns that will be required to be used to capture specific instances of removable slices of text within the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newline, Tab spaces, etc.\n",
    "newline_tabspace = r'[\\r\\n\\t]'\n",
    "\n",
    "# Match words starting with Uppercase letters\n",
    "upper_words = r'([A-Z])\\w+'\n",
    "\n",
    "# Match Words that start with either Upper/lowercase letters\n",
    "upperlower_words = r'[A-Za-z]\\w+'\n",
    "\n",
    "# Sub/Superscript characters\n",
    "# Encountered previously\n",
    "sub_sup_scripts = r'\\w[²³¹⁰ⁱ⁴⁵⁶⁷⁸⁹⁺⁻⁼⁽⁾ⁿ]+'\n",
    "\n",
    "# Punctuation\n",
    "punc_pattern = r'[!\\?.,\\':;\"]'\n",
    "\n",
    "# Single Letters\n",
    "single_letter = r'((?<=^)|(?<= )).((?=$)|(?= ))'\n",
    "\n",
    "# Match URLs\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w+?\\.\\w+])+([a-zA-Z0-9\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)_\\-\\=\\+\\\\\\/\\?\\.\\:\\;\\'\\,]*)?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and such\n",
    "Functions to be used and applied on the dataframe.\n",
    "\n",
    "Each function will be commented, whilst also writing an explanation to a grouped cell of functions indicating their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df: pd.DataFrame, colname: str):\n",
    "    return np.where(df[colname] > 0, 1, 0)\n",
    "\n",
    "def clean_comment(df: pd.DataFrame, colname: str):\n",
    "    # Remove the newlines, tabs, etc.\n",
    "    df[colname].replace(newline_tabspace, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove URLs\n",
    "    df[colname].replace(url_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove superscript, subscript\n",
    "    df[colname].replace(sub_sup_scripts, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove punctuation\n",
    "    df[colname].replace(punc_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove Single Letters\n",
    "    df[colname].replace(single_letter, ' ', regex = True, inplace = True)\n",
    "\n",
    "    return df[colname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A walkthrough the above functions:\n",
    "- `binarize(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "Due to the rise of values larger than 1 after collapsing the other column values, a requirement for binarizing (Chose to make the labels binary, for simplicity's sake) the values, $1$ for Toxic and $0$ for Non-Toxic, this is achieved by assigning any value larger than 0 to 1 otherwise stays as 0.\n",
    "\n",
    "- `clean_comment(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "The nature of the data given, is that it has a lot of *whitespaces*, *tabs*, *special characters*, *urls*, *punctuations*, *single letters*, which are all bound to be removed.\n",
    "The function removes:\n",
    "    - Newlines\n",
    "    - Tabs\n",
    "    - URLs\n",
    "    - Sub/Super Scripts\n",
    "    - Punctuation\n",
    "    - Single Letters (From removal of some punctuation symbols)\n",
    "\n",
    "Both functions return the updated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww  He matches this background colour     s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man      really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More    can   make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation  Why the edits made under my usern...      0\n",
       "1    aww  He matches this background colour     s...      0\n",
       "2  Hey man      really not trying to edit war  It...      0\n",
       "3     More    can   make any real suggestions on ...      0\n",
       "4  You  sir  are my hero  Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>And for the second time of asking  when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself     That is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer     Umm  theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>And       really don   think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "159566        And for the second time of asking  when ...      0\n",
       "159567  You should be ashamed of yourself     That is ...      0\n",
       "159568  Spitzer     Umm  theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570     And       really don   think you understand...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159571.000000\n",
       "mean        0.101679\n",
       "std         0.302226\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_dat['comment_text'] = clean_comment(txt_dat, 'comment_text')\n",
    "\n",
    "txt_dat['toxic'] = binarize(txt_dat, 'toxic')\n",
    "\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no extra characters, like apostrophe's, extra punctuations, etc.\n",
    "\n",
    "As well as having the maximum value to be 1 within the `toxic` column, although the mean is closer to 0 which suggest that the data is really imbalanced, which will require fixing later down the line, ideally it would be better to be closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing\n",
    "Preprocessing the data with ways that are under the NLP PreProcessing Standards, as in:\n",
    "- Tokenizing.\n",
    "- Lowercasing.\n",
    "- Removal of Stopwords.\n",
    "- Removing Non-English words.\n",
    "- Lemmatization/Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting up a sentence or a corpus into plain words (tokens) for variable reasons:\n",
    "- To know which words are present.\n",
    "- Count them.\n",
    "- Et cetera.\n",
    "\n",
    "In my implementation, I will be using the `regexp_tokenizer()` in order to tokenize the text within the `comment_text` column.\n",
    "\n",
    "As well as keeping the tokens within the same dataframe as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a specific column within a passed DataFrame\n",
    "def tokenize(dataframe: pd.DataFrame, colname: str):\n",
    "    return [regexp_tokenize(row, upperlower_words) for row in dataframe[colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww  He matches this background colour     s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, He, matches, this, background, colour, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man      really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Hey, man, really, not, trying, to, edit, war,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More    can   make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[More, can, make, any, real, suggestions, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[You, sir, are, my, hero, Any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  \\\n",
       "0  Explanation  Why the edits made under my usern...      0   \n",
       "1    aww  He matches this background colour     s...      0   \n",
       "2  Hey man      really not trying to edit war  It...      0   \n",
       "3     More    can   make any real suggestions on ...      0   \n",
       "4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Explanation, Why, the, edits, made, under, my...  \n",
       "1  [aww, He, matches, this, background, colour, s...  \n",
       "2  [Hey, man, really, not, trying, to, edit, war,...  \n",
       "3  [More, can, make, any, real, suggestions, on, ...  \n",
       "4  [You, sir, are, my, hero, Any, chance, you, re...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the tokenize() function on a new column within the main DataFrame\n",
    "txt_dat['tokens'] = tokenize(txt_dat, 'comment_text')\n",
    "\n",
    "# Checking if the function worked\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the sentences have been tokenised, each word is its own string, within a list of strings under the new `tokens` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case Words\n",
    "By conventional standards, changing the case of the words into lowercase has been agreed upon to negate the need to account for case sensitivity with the operations that follow to preprocess text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lowercase text\n",
    "def token_lower(token_list: list):\n",
    "    return [token.lower() for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww  He matches this background colour     s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, he, matches, this, background, colour, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man      really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, not, trying, to, edit, war,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More    can   make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[more, can, make, any, real, suggestions, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  \\\n",
       "0  Explanation  Why the edits made under my usern...      0   \n",
       "1    aww  He matches this background colour     s...      0   \n",
       "2  Hey man      really not trying to edit war  It...      0   \n",
       "3     More    can   make any real suggestions on ...      0   \n",
       "4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [aww, he, matches, this, background, colour, s...  \n",
       "2  [hey, man, really, not, trying, to, edit, war,...  \n",
       "3  [more, can, make, any, real, suggestions, on, ...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterating over the column\n",
    "# Iterable is of type \"list\"\n",
    "txt_dat['tokens'] = [token_lower(row) for row in txt_dat['tokens']]\n",
    "\n",
    "# Displaying results of applying the above function\n",
    "txt_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining how the function works first before observation.\n",
    "\n",
    "Take the column `tokens`, it contains lists of tokens for each comment like: `[Explanation, Why, the, edits, made, under, my...]`.\n",
    "\n",
    "Said list is what is being iterated on, which gets passed to the function, so for every iteration a new list of words gets passed to the function; but within the function, we iterate over each item within said list, so `'Explanantion` and `'Why'`, etc..\n",
    "\n",
    "Therefore, passing over each token, we apply the `.lower()` function for the `str` data type; thus we get a lowercase version of the string.\n",
    "\n",
    "After every iteration of lists, they get returned and reassigned where the original list used to be.\n",
    "\n",
    "As for observation, we can see that the words did indeed become lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "Removing of stopwords drastically decreases the amount of words that need to be taken into consideration by the model, and contribute little to nothing regarding meaning.\n",
    "\n",
    "Removing them would shave off redundant computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stopwords\n",
    "def stopword_remover(wordlist: list):\n",
    "    return [word for word in wordlist if word not in ENGLISH_STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 13, 41, 103, 13]\n",
      "[23, 10, 21, 50, 5]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww  He matches this background colour     s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, matches, background, colour, seemingly, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man      really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More    can   make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  \\\n",
       "0  Explanation  Why the edits made under my usern...      0   \n",
       "1    aww  He matches this background colour     s...      0   \n",
       "2  Hey man      really not trying to edit war  It...      0   \n",
       "3     More    can   make any real suggestions on ...      0   \n",
       "4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [aww, matches, background, colour, seemingly, ...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [make, real, suggestions, improvement, wondere...  \n",
       "4                [sir, hero, chance, remember, page]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying word count for first few lists\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "# Iterating over column\n",
    "# Iterable is the list of\n",
    "# Lowercase english words\n",
    "txt_dat['tokens'] = [stopword_remover(row) for row in txt_dat['tokens']]\n",
    "\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by the decrease in the word count per token list, there was indeed a removal of redundant words.\n",
    "\n",
    "As for how the function works, it is similar to how the previous ones work, passing the iterated list to iterate over the tokens within and apply a function; in our case it is a conditional to just include words that are not within the collection of words which are considered stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization & Stemming\n",
    "Lemmatising/Stemming both have the same target in mind, to reach the root of the word but their difference is the algorithm used.\n",
    "\n",
    "Stemming utilises a \"Porter\" algorithm which essentially just chops off common word endings, due to Stemmer using a crude old method, which is aimed for speed and efficiency, unlike lemmatizaton which morphologically analyses lexical changes in words to revert them back to their roots, unlike the chopping of \"commonly found prefixes/suffixes\" which stemming does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(wordlist: list):\n",
    "    return [WordNetLemmatizer().lemmatize(token) for token in wordlist]\n",
    "\n",
    "def stem(wordlist):\n",
    "    return [SnowballStemmer(\"english\").stem(token) for token in wordlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, after some look up, lemmatization is beneficial to reduce the word to its base, in a more human context and understanding, mainly used in chatbots, etc.\n",
    "\n",
    "Whilest the stemmer is used to purely chop off the words' endings to get to the base, used more in sentiment analysis, which is our current case with the binary classification, hence it will be used after lemmatizer and english word remover are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up NLP Preprocessing\n",
    "In this section, a whole function will take in a column in order to apply ***all*** the above functions in one go.\n",
    "\n",
    "Description and explanation for function order will be explained in either comments or a markdown cell following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, colname: str):\n",
    "    # List to hold the lists of tokens\n",
    "    tokenlist = tokenize(df, colname)\n",
    "\n",
    "    # Get number of tokens originally\n",
    "    token_num = sum([len(t_list) for t_list in tokenlist])\n",
    "\n",
    "    # Lowercase list of token lists\n",
    "    # t_list -> (t)oken_list\n",
    "    lower_tokens = [token_lower(t_list) for t_list in tokenlist]\n",
    "\n",
    "    # Stopword-free list\n",
    "    # tl_list -> (t)oken(l)lowercase_list\n",
    "    stopwordless = [stopword_remover(tl_list) for tl_list in lower_tokens]\n",
    "\n",
    "    # List of token lemmas\n",
    "    # tls_list -> (t)oken(l)owercase(s)topwordless_list\n",
    "    lemmas = [lemmatize(tls_list) for tls_list in stopwordless]\n",
    "\n",
    "    # Get number of tokens after processing\n",
    "    # (Englishify is the last function to remove)\n",
    "    proc_num = sum([len(tlsl_list) for tlsl_list in lemmas])\n",
    "\n",
    "    # Reduction\n",
    "    reduced_by = 1 - proc_num / token_num\n",
    "\n",
    "    # Print counts and percentage\n",
    "    print(\"Original Token Count : {}\\nProcessed Token Count: {}\\nReduction Percentage : {:.2%}\".format(token_num, proc_num, reduced_by))\n",
    "\n",
    "    # Return tuple of token list and processed tokens\n",
    "    return [stem(word) for word in lemmas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation for how the wrap-up function works:\n",
    "- Passing the DataFrame to be processed, along with the column name to be tokenized.\n",
    "    - Each row within the column gets tokenized with use of the predefined RegEx expressions up above.\n",
    "- Calculating the number of tokens to get a feel for how much we'll be dealing with.\n",
    "- Applying a lowercase function to ever list within the list of lists assigned to the output of the tokenization function.\n",
    "- Removing stopwords from the tokens.\n",
    "- Lemmatizing the words, to reach the root of the word (Useful for removing non-English words later).\n",
    "- Removal of Non-English words.\n",
    "- Calculating the amount of tokens.\n",
    "- Printing the calculated numbers.\n",
    "- Returning a tuple of tokenised words and stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token Count : 10130199\n",
      "Processed Token Count: 5425798\n",
      "Reduction Percentage : 46.44%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>[explan, edit, made, usernam, hardcor, metalli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww  He matches this background colour     s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, matches, background, colour, seemingly, ...</td>\n",
       "      <td>[aww, match, background, colour, seem, stuck, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man      really not trying to edit war  It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>[hey, man, realli, tri, edit, war, guy, consta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More    can   make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>[make, real, suggest, improv, wonder, section,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>[sir, hero, chanc, rememb, page]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  \\\n",
       "0  Explanation  Why the edits made under my usern...      0   \n",
       "1    aww  He matches this background colour     s...      0   \n",
       "2  Hey man      really not trying to edit war  It...      0   \n",
       "3     More    can   make any real suggestions on ...      0   \n",
       "4  You  sir  are my hero  Any chance you remember...      0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [aww, matches, background, colour, seemingly, ...   \n",
       "2  [hey, man, really, trying, edit, war, guy, con...   \n",
       "3  [make, real, suggestions, improvement, wondere...   \n",
       "4                [sir, hero, chance, remember, page]   \n",
       "\n",
       "                                           processed  \n",
       "0  [explan, edit, made, usernam, hardcor, metalli...  \n",
       "1  [aww, match, background, colour, seem, stuck, ...  \n",
       "2  [hey, man, realli, tri, edit, war, guy, consta...  \n",
       "3  [make, real, suggest, improv, wonder, section,...  \n",
       "4                   [sir, hero, chanc, rememb, page]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning output tuple to \"new\" column of tokens and new for processed\n",
    "txt_dat['processed'] = preprocess(txt_dat, 'comment_text')\n",
    "\n",
    "# Printing to see progress\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there as been a reduction in token count by $46.44%$, which mainly consists of Stopwords.\n",
    "\n",
    "All in all, less tokens are beneficial for less computation; All whilst maintaining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more toxic comments\n",
    "def inject_toxic(df: pd.DataFrame, toxic_val: int, toxic_colname: str, iters: int):\n",
    "    # Extracting toxic comments\n",
    "    toxic_df = df[df[toxic_colname] == toxic_val]\n",
    "\n",
    "    # Injecting toxic comments however many times\n",
    "    for _ in range(0, iters):\n",
    "        df = pd.concat([df, toxic_df])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>192021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.253488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.435009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  192021.000000\n",
       "mean        0.253488\n",
       "std         0.435009\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_dat = inject_toxic(txt_dat, 1, 'toxic', 2)\n",
    "\n",
    "txt_dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Using PyTorch, a transformer will be implemented from scratch to classify comments into toxic or non-toxic.\n",
    "\n",
    "Research Paper used as reference:\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "Website Articles that helped:\n",
    "- [The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "- [Text Classifier with PyTorch Transformer](https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/)\n",
    "- [Basic Transformer With PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "- [Positional Encoding](https://theaisummer.com/positional-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch                                    # PyTorch\n",
    "import torch.nn    as nn                        # PyTorch Neural Networks\n",
    "import torch.optim as optim                     # Optimizers\n",
    "import torch.nn.functional as F                    # Functions\n",
    "from torch.utils.data import Dataset            # Dataset of words\n",
    "from torch.utils.data import DataLoader         # Loading and Batching Dataset\n",
    "from torch.utils.data import random_split       # Splitting dataset\n",
    "from collections import Counter                 # Enumeration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
    "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight[input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add ***\"positional encodings\"*** to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed.\n",
    "\n",
    "Thus, we will be working with the following trig functions of different frequencies:\n",
    "$$ PE(pos, 2i) = sin(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "$$ PE(pos, 2i + 1) = cos(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi^{[1]}$. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of\n",
    "$PE_{pos}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Initalise a zero tensor to fill for padding\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # Initialising a vector of length max_seq_len\n",
    "        # to map words to their positions over a function\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Using log for better numerical stability\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Using the above 2 equations, apply the sine and cosine functions\n",
    "        # to the respective parts\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "The main star of the show and the key that emphasizes why the transformer is the better architecture.\n",
    "\n",
    "Through remembering the prompts and contextually \"understanding\" the words whilst taking into consideration the meaning and value of the prior words.\n",
    "\n",
    "Hence, the Multi-Head Attetntion formula is described by the following equation:\n",
    "$$ MultiHeadAttention(Q, K, V) = Concat(head_1, \\ldots, head_h)W^O$$\n",
    "\n",
    "Where each $h_i$ is derived from the following formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{model}}})\\cdot V$$\n",
    "\n",
    "In this particular paper, the model has the hyperparameter of $h^{[1]}$ set to $8$, such that $d_k = d_v = d_{model} / h = 64$\n",
    "\n",
    "Softmax:\n",
    "$$ softmax(\\vec{z})_{i} = \\frac{e^{z_{i}}}{\\sum^{n}_{j=1} e^{z_{j}}}$$\n",
    "Where:\n",
    "- $\\vec{z} \\rightarrow$ Input Vector.\n",
    "- $e^{z_i} \\rightarrow$ Standard Exponentiation for Input vector.\n",
    "- $e^{z_j} \\rightarrow$ Standard Exponentiation for Output Vector.\n",
    "- $n \\rightarrow$ number of classes in multi-class classifier.\n",
    "\n",
    "---\n",
    "[1]: h is the number of attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.01):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        # Number of Heads\n",
    "        self.nhead = nhead\n",
    "        # Model Dimension\n",
    "        self.d_model = d_model\n",
    "        # Head dimension\n",
    "        self.dim_per_head = d_model // nhead\n",
    "\n",
    "        # Q, K, V\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Linearising\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout to lessen overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "\n",
    "        query = query.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.nhead, self.dim_per_head).transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim_per_head)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        context = torch.matmul(self.dropout(attention_probs), value)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.nhead * self.dim_per_head)\n",
    "\n",
    "        output = self.out_linear(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$ FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is $d_{model} = {512}$, and the inner-layer has dimensionality\n",
    "$d_{ff} = {2048}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.norm_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(self.norm_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.norm_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Standarising the values to be within\n",
    "        # normal distr.\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        output = (x - mean) / (std + self.eps)\n",
    "        output = (output * self.alpha.unsqueeze(0)) + self.beta.unsqueeze(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "Consisting of $6^{[1]}$ layers of the same process. From the original design we can also see that there is a skip forward from:\n",
    "- Before the MHA (Multi-Head Attention)\n",
    "- Before the FFN (Feed Forward Network)\n",
    "\n",
    "As well as consisting of $4$ blocks in total for each Encoder Block:\n",
    "- Multi-Head Attention.\n",
    "- First Normalization.\n",
    "- Feed-Foward Network.\n",
    "- Second Normalization.\n",
    "\n",
    "It would be pointless if we just continuously passed the output from one block without transforming it in any way or shape, hence the introudction of the $LayerNorm(x + SubLayer(x))$ function, where $SubLayer(x)$ is a function produced by the layer itself to facilitate the residual connections, and produces an output of dimension $d_{model} = 512$.\n",
    "\n",
    "---\n",
    "\n",
    "[1]: Included in the original Paper, Section 3.1; Encoder Paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of encoder\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=128, dropout=0.0):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multihead Attention block instance\n",
    "        self.self_attention = MultiheadAttention(d_model, nhead)\n",
    "        \n",
    "        # Normalization block instance\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Model Dimension\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Feed Forward block instance\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Passing values to attention\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "\n",
    "        # Changing shape and storing\n",
    "        attn_output = attn_output.view(-1, src.size(1), self.d_model)\n",
    "\n",
    "        # Applying dropout (Lesser overfitting)\n",
    "        # and adding to original\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        # Normalising\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Feeding to FFN\n",
    "        ffn_output = self.feed_forward(src)\n",
    "\n",
    "        # Dropout and normalise\n",
    "        src = src + self.dropout2(ffn_output)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "# N Layers of encoders\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedfwd):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            # Instantiating layers of encoders\n",
    "            [TransformerEncoderLayer(d_model, nhead, dim_feedfwd) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        # So that the output of each layer\n",
    "        # is passed to input of the next\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two\n",
    "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
    "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
    "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
    "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position $i$ can depend only on the known outputs at positions less than $i$.\n",
    "\n",
    "#### Note: Due to our implementation for it to be a Text-Classifier version of the Transformer, This will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # masked multi-head attention\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, n_heads)\n",
    "\n",
    "        # normalization layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # cross multi-head attention\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, n_heads)\n",
    "\n",
    "        # second normalization layer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # linear projection layer\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # final normalization layer\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input, encoder_output, src_mask, tgt_mask):\n",
    "        # apply masked multi-head attention\n",
    "        attn_output, _ = self.self_attention(input, input, input, attn_mask=tgt_mask)\n",
    "\n",
    "        # add and normalize\n",
    "        output1 = self.norm1(input + attn_output)\n",
    "\n",
    "        # apply cross multi-head attention\n",
    "        cross_attn_output, _ = self.cross_attention(output1, encoder_output, encoder_output, attn_mask=src_mask)\n",
    "\n",
    "        # add and normalize\n",
    "        output2 = self.norm2(output1 + cross_attn_output)\n",
    "\n",
    "        # linear projection\n",
    "        projected_output = self.linear(output2)\n",
    "        # add and normalize\n",
    "        output3 = self.norm3(output2 + projected_output)\n",
    "        # apply softmax\n",
    "        output = F.softmax(output3, dim=-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Transformer\n",
    "For our purpose of this project, we are tasked to classify the given data of Comments into a binary label of `toxic`, as mentioned before the $1$ stands for `toxic` and the $0$ stands for `non-toxic`.\n",
    "\n",
    "This is achieved by only using the Encoder part of the transformer, which means that only the N-Layers of the Encoder will be at work, No need for Cross-Multi-Head Attention or any of the Expected Output to be put into a Decoder, Just the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int = 512,            # Dimensions of the model, also vector length for sentences\n",
    "                 num_classes: int = 2,          # Number of Classification Classes (1, 0 -> Binary -> 2)\n",
    "                 nhead: int = 8,                # Number of Attention Heads\n",
    "                 num_encoder_layers: int = 6,   # Number of Layers to be within the Encoder\n",
    "                 vocab_size: int = 1000,        # Size of Vocabulary\n",
    "                 feedfwd_dim: int = 1024,       # SIze of Feed Forward NN\n",
    "                 max_seq_len: int = 512         # Maximum length of Sequence\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # Model dimension assignment to self store\n",
    "        self.model_dim = d_model\n",
    "\n",
    "        # Head amount\n",
    "        self.num_head = nhead\n",
    "\n",
    "        # Number of Encoder Layers\n",
    "        self.num_enc_layers = num_encoder_layers\n",
    "\n",
    "        # Number of Classes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Dimensions of Feed Forward NN\n",
    "        self.d_ff = feedfwd_dim\n",
    "        \n",
    "        # Size of Vocabulary\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.model_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.transformer_enc = TransformerEncoder(self.model_dim, self.num_head, self.num_enc_layers, feedfwd_dim)\n",
    "\n",
    "        # Feed Forward\n",
    "        self.fc = nn.Linear(self.model_dim, self.num_classes)\n",
    "        \n",
    "        # Encoding\n",
    "        self.positional_encoding = PositionalEncoding(self.model_dim, max_seq_len)\n",
    "\n",
    "    # Defining the forward method to be called on passing outputs to later layers/blocks\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        encoded = self.positional_encoding(embedded)\n",
    "        output = self.transformer_enc(encoded)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        logits = self.fc(output[:, -1, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "A vital part of checking whether or not the model is doing what it's supposed to do, is to train it then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    # Passing model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Enabling Training mode\n",
    "    model.train()\n",
    "\n",
    "    # Storing loss over epochs\n",
    "    loss_over_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Cumulative loss over epochs\n",
    "        # (Used in calculating average loss over epochs)\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Unpacking data to inputs and labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Passing each to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Resetting the optimizer's gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Storing model output\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Niche work around to error I used to get\n",
    "            # Dimensionality problem between model output\n",
    "            # and label shape\n",
    "            # Conditional just checks\n",
    "            # and prevents errors\n",
    "            if (outputs.shape[0] == labels.shape[0]):\n",
    "                # Calculating loss based on criteria\n",
    "                # (Binary Cross Entropy in our case)\n",
    "                # and updating weights\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                # Taking a step with the optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Adding loss to cumulative\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Printing results and values\n",
    "                print(\"Epoch [{}/{}]\\n\\tBatch [{}]:\\n\\t Loss: {:.4f}\".format(epoch + 1, num_epochs, i, loss))\n",
    "                \n",
    "                # Appending to list of loss\n",
    "                loss_over_epoch.append(loss.item())\n",
    "\n",
    "        # Calculating average loss over epochs\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        # Printing results\n",
    "        print(\"Epoch [{}/{}], Average Loss: {:.6f}\".format(epoch + 1, num_epochs, average_loss))\n",
    "\n",
    "    return loss_over_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.to(device)\n",
    "\n",
    "    acc_list = []\n",
    "    loss_list= []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Instantiating the evaluation parameters\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if (outputs.shape[0] == labels.shape[0]):\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                loss_list.append(loss.item())\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                accuracy = total_correct / total_samples\n",
    "                print(\"Test Batch [{}]:\\n\\tLoss: {:.4f}, Accuracy {:.2f}%\\n\".format(i + 1, loss, accuracy * 100))\n",
    "                acc_list.append(accuracy)\n",
    "\n",
    "        average_loss = total_loss / len(test_loader)\n",
    "        print(\"After [{}] Batches:\\n\\tAverage Test Loss: {:.4f}, Accuracy: {:.2f}%\\n  Over {} samples, {} were correct\"\\\n",
    "            .format(i + 1, average_loss, accuracy * 100, total_samples, total_correct))\n",
    "    return acc_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data, seq_len:int):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Create vocabulary and index mapping here\n",
    "        vocab = Counter([token for tokens in txt_dat['processed'] for token in tokens])\n",
    "\n",
    "        word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "\n",
    "        # Defining padding\n",
    "        # fills up the rest of the vector\n",
    "        # should sentence not take it up whole\n",
    "        word_to_idx['<PAD>'] = 0\n",
    "        self.w_t_idx = word_to_idx\n",
    "    \n",
    "    # Method to return number of entries\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Method to return length of vocab\n",
    "    def voc_size(self):\n",
    "        return len(self.w_t_idx)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item  = self.data.iloc[index]\n",
    "        src   = item.iloc[0]  # Input sequence\n",
    "        label = item.iloc[1]  # Binary label\n",
    "\n",
    "        # Convert tokens to numerical indices and pad sequences\n",
    "        src = [self.w_t_idx.get(token, 0) for token in src]  # Use word_to_idx with a default value of 0\n",
    "        src = src[:self.seq_len] + [0] * (self.seq_len - len(src))  # Padding with 0\n",
    "        src = torch.tensor(src, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return src, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_dataset(dataset, test_size=0.2):\n",
    "    # Method to split the Torch Dataset\n",
    "    # Into training and testing sets\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    # Calculating number of entries to be within each set\n",
    "    num_test_samples = int(test_size * dataset_size)\n",
    "    num_train_samples = dataset_size - num_test_samples\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [num_train_samples, num_test_samples])\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it All Together\n",
    "Dividing Data and turning it into pytorch tensor compatible input.\n",
    "\n",
    "Input into the transformer implemented above and evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Defining some parameters\n",
    "\n",
    "# Number of Epochs to run for\n",
    "Epochs = 1\n",
    "\n",
    "test_size = 0.4\n",
    "# Model and Batch dimensions\n",
    "d_b_size = 100\n",
    "max_seq_length = 100\n",
    "class_number = 2\n",
    "head_number = 4\n",
    "encoder_layers = 6\n",
    "ffn_dim = 2048\n",
    "\n",
    "# Criteria to evaluate loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.0001\n",
    "\n",
    "# Which device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the Data\n",
    "whole_data = VocabDataset(txt_dat[['processed', 'toxic']], max_seq_length)\n",
    "\n",
    "# Split\n",
    "train_ds, test_ds = train_test_split_dataset(whole_data, test_size)\n",
    "\n",
    "# Loaders\n",
    "train_load = DataLoader(train_ds, 100, True)\n",
    "test_load = DataLoader(test_ds, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 100, 5159,   46,  150,  263, 5160, 1866,  536,  211,  430, 3356,  553,\n",
       "         5161, 5162,   73,  211, 2786, 5163,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an entry\n",
    "whole_data[700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the entry per row is considered as a tuple of tensors, with word embedding in one (Numbers) and label in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n",
      "\tBatch [0]:\n",
      "\t Loss: 0.6392\n",
      "Epoch [1/1]\n",
      "\tBatch [1]:\n",
      "\t Loss: 0.6327\n",
      "Epoch [1/1]\n",
      "\tBatch [2]:\n",
      "\t Loss: 0.6579\n",
      "Epoch [1/1]\n",
      "\tBatch [3]:\n",
      "\t Loss: 0.6261\n",
      "Epoch [1/1]\n",
      "\tBatch [4]:\n",
      "\t Loss: 0.6306\n",
      "Epoch [1/1]\n",
      "\tBatch [5]:\n",
      "\t Loss: 0.6743\n",
      "Epoch [1/1]\n",
      "\tBatch [6]:\n",
      "\t Loss: 0.6496\n",
      "Epoch [1/1]\n",
      "\tBatch [7]:\n",
      "\t Loss: 0.5579\n",
      "Epoch [1/1]\n",
      "\tBatch [8]:\n",
      "\t Loss: 0.5887\n",
      "Epoch [1/1]\n",
      "\tBatch [9]:\n",
      "\t Loss: 0.6439\n",
      "Epoch [1/1]\n",
      "\tBatch [10]:\n",
      "\t Loss: 0.5766\n",
      "Epoch [1/1]\n",
      "\tBatch [11]:\n",
      "\t Loss: 0.5662\n",
      "Epoch [1/1]\n",
      "\tBatch [12]:\n",
      "\t Loss: 0.6245\n",
      "Epoch [1/1]\n",
      "\tBatch [13]:\n",
      "\t Loss: 0.5566\n",
      "Epoch [1/1]\n",
      "\tBatch [14]:\n",
      "\t Loss: 0.6428\n",
      "Epoch [1/1]\n",
      "\tBatch [15]:\n",
      "\t Loss: 0.6657\n",
      "Epoch [1/1]\n",
      "\tBatch [16]:\n",
      "\t Loss: 0.6485\n",
      "Epoch [1/1]\n",
      "\tBatch [17]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [18]:\n",
      "\t Loss: 0.6555\n",
      "Epoch [1/1]\n",
      "\tBatch [19]:\n",
      "\t Loss: 0.6214\n",
      "Epoch [1/1]\n",
      "\tBatch [20]:\n",
      "\t Loss: 0.5673\n",
      "Epoch [1/1]\n",
      "\tBatch [21]:\n",
      "\t Loss: 0.5428\n",
      "Epoch [1/1]\n",
      "\tBatch [22]:\n",
      "\t Loss: 0.5764\n",
      "Epoch [1/1]\n",
      "\tBatch [23]:\n",
      "\t Loss: 0.5839\n",
      "Epoch [1/1]\n",
      "\tBatch [24]:\n",
      "\t Loss: 0.5177\n",
      "Epoch [1/1]\n",
      "\tBatch [25]:\n",
      "\t Loss: 0.5870\n",
      "Epoch [1/1]\n",
      "\tBatch [26]:\n",
      "\t Loss: 0.5589\n",
      "Epoch [1/1]\n",
      "\tBatch [27]:\n",
      "\t Loss: 0.6233\n",
      "Epoch [1/1]\n",
      "\tBatch [28]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [29]:\n",
      "\t Loss: 0.6605\n",
      "Epoch [1/1]\n",
      "\tBatch [30]:\n",
      "\t Loss: 0.5751\n",
      "Epoch [1/1]\n",
      "\tBatch [31]:\n",
      "\t Loss: 0.5295\n",
      "Epoch [1/1]\n",
      "\tBatch [32]:\n",
      "\t Loss: 0.5516\n",
      "Epoch [1/1]\n",
      "\tBatch [33]:\n",
      "\t Loss: 0.4761\n",
      "Epoch [1/1]\n",
      "\tBatch [34]:\n",
      "\t Loss: 0.6075\n",
      "Epoch [1/1]\n",
      "\tBatch [35]:\n",
      "\t Loss: 0.5196\n",
      "Epoch [1/1]\n",
      "\tBatch [36]:\n",
      "\t Loss: 0.6431\n",
      "Epoch [1/1]\n",
      "\tBatch [37]:\n",
      "\t Loss: 0.5485\n",
      "Epoch [1/1]\n",
      "\tBatch [38]:\n",
      "\t Loss: 0.5543\n",
      "Epoch [1/1]\n",
      "\tBatch [39]:\n",
      "\t Loss: 0.6029\n",
      "Epoch [1/1]\n",
      "\tBatch [40]:\n",
      "\t Loss: 0.6020\n",
      "Epoch [1/1]\n",
      "\tBatch [41]:\n",
      "\t Loss: 0.5574\n",
      "Epoch [1/1]\n",
      "\tBatch [42]:\n",
      "\t Loss: 0.4988\n",
      "Epoch [1/1]\n",
      "\tBatch [43]:\n",
      "\t Loss: 0.5897\n",
      "Epoch [1/1]\n",
      "\tBatch [44]:\n",
      "\t Loss: 0.5066\n",
      "Epoch [1/1]\n",
      "\tBatch [45]:\n",
      "\t Loss: 0.5858\n",
      "Epoch [1/1]\n",
      "\tBatch [46]:\n",
      "\t Loss: 0.6637\n",
      "Epoch [1/1]\n",
      "\tBatch [47]:\n",
      "\t Loss: 0.5314\n",
      "Epoch [1/1]\n",
      "\tBatch [48]:\n",
      "\t Loss: 0.5661\n",
      "Epoch [1/1]\n",
      "\tBatch [49]:\n",
      "\t Loss: 0.5595\n",
      "Epoch [1/1]\n",
      "\tBatch [50]:\n",
      "\t Loss: 0.5234\n",
      "Epoch [1/1]\n",
      "\tBatch [51]:\n",
      "\t Loss: 0.5065\n",
      "Epoch [1/1]\n",
      "\tBatch [52]:\n",
      "\t Loss: 0.5738\n",
      "Epoch [1/1]\n",
      "\tBatch [53]:\n",
      "\t Loss: 0.7303\n",
      "Epoch [1/1]\n",
      "\tBatch [54]:\n",
      "\t Loss: 0.6278\n",
      "Epoch [1/1]\n",
      "\tBatch [55]:\n",
      "\t Loss: 0.5477\n",
      "Epoch [1/1]\n",
      "\tBatch [56]:\n",
      "\t Loss: 0.5474\n",
      "Epoch [1/1]\n",
      "\tBatch [57]:\n",
      "\t Loss: 0.6399\n",
      "Epoch [1/1]\n",
      "\tBatch [58]:\n",
      "\t Loss: 0.6377\n",
      "Epoch [1/1]\n",
      "\tBatch [59]:\n",
      "\t Loss: 0.5418\n",
      "Epoch [1/1]\n",
      "\tBatch [60]:\n",
      "\t Loss: 0.5341\n",
      "Epoch [1/1]\n",
      "\tBatch [61]:\n",
      "\t Loss: 0.5860\n",
      "Epoch [1/1]\n",
      "\tBatch [62]:\n",
      "\t Loss: 0.5555\n",
      "Epoch [1/1]\n",
      "\tBatch [63]:\n",
      "\t Loss: 0.6213\n",
      "Epoch [1/1]\n",
      "\tBatch [64]:\n",
      "\t Loss: 0.5528\n",
      "Epoch [1/1]\n",
      "\tBatch [65]:\n",
      "\t Loss: 0.5704\n",
      "Epoch [1/1]\n",
      "\tBatch [66]:\n",
      "\t Loss: 0.5627\n",
      "Epoch [1/1]\n",
      "\tBatch [67]:\n",
      "\t Loss: 0.5179\n",
      "Epoch [1/1]\n",
      "\tBatch [68]:\n",
      "\t Loss: 0.5750\n",
      "Epoch [1/1]\n",
      "\tBatch [69]:\n",
      "\t Loss: 0.5138\n",
      "Epoch [1/1]\n",
      "\tBatch [70]:\n",
      "\t Loss: 0.5762\n",
      "Epoch [1/1]\n",
      "\tBatch [71]:\n",
      "\t Loss: 0.6153\n",
      "Epoch [1/1]\n",
      "\tBatch [72]:\n",
      "\t Loss: 0.6394\n",
      "Epoch [1/1]\n",
      "\tBatch [73]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [74]:\n",
      "\t Loss: 0.6192\n",
      "Epoch [1/1]\n",
      "\tBatch [75]:\n",
      "\t Loss: 0.5928\n",
      "Epoch [1/1]\n",
      "\tBatch [76]:\n",
      "\t Loss: 0.4937\n",
      "Epoch [1/1]\n",
      "\tBatch [77]:\n",
      "\t Loss: 0.5121\n",
      "Epoch [1/1]\n",
      "\tBatch [78]:\n",
      "\t Loss: 0.5109\n",
      "Epoch [1/1]\n",
      "\tBatch [79]:\n",
      "\t Loss: 0.6001\n",
      "Epoch [1/1]\n",
      "\tBatch [80]:\n",
      "\t Loss: 0.6747\n",
      "Epoch [1/1]\n",
      "\tBatch [81]:\n",
      "\t Loss: 0.5324\n",
      "Epoch [1/1]\n",
      "\tBatch [82]:\n",
      "\t Loss: 0.6180\n",
      "Epoch [1/1]\n",
      "\tBatch [83]:\n",
      "\t Loss: 0.5327\n",
      "Epoch [1/1]\n",
      "\tBatch [84]:\n",
      "\t Loss: 0.5944\n",
      "Epoch [1/1]\n",
      "\tBatch [85]:\n",
      "\t Loss: 0.5957\n",
      "Epoch [1/1]\n",
      "\tBatch [86]:\n",
      "\t Loss: 0.5698\n",
      "Epoch [1/1]\n",
      "\tBatch [87]:\n",
      "\t Loss: 0.5928\n",
      "Epoch [1/1]\n",
      "\tBatch [88]:\n",
      "\t Loss: 0.5643\n",
      "Epoch [1/1]\n",
      "\tBatch [89]:\n",
      "\t Loss: 0.5845\n",
      "Epoch [1/1]\n",
      "\tBatch [90]:\n",
      "\t Loss: 0.6455\n",
      "Epoch [1/1]\n",
      "\tBatch [91]:\n",
      "\t Loss: 0.6122\n",
      "Epoch [1/1]\n",
      "\tBatch [92]:\n",
      "\t Loss: 0.6580\n",
      "Epoch [1/1]\n",
      "\tBatch [93]:\n",
      "\t Loss: 0.6118\n",
      "Epoch [1/1]\n",
      "\tBatch [94]:\n",
      "\t Loss: 0.6194\n",
      "Epoch [1/1]\n",
      "\tBatch [95]:\n",
      "\t Loss: 0.6270\n",
      "Epoch [1/1]\n",
      "\tBatch [96]:\n",
      "\t Loss: 0.5753\n",
      "Epoch [1/1]\n",
      "\tBatch [97]:\n",
      "\t Loss: 0.5720\n",
      "Epoch [1/1]\n",
      "\tBatch [98]:\n",
      "\t Loss: 0.5255\n",
      "Epoch [1/1]\n",
      "\tBatch [99]:\n",
      "\t Loss: 0.5011\n",
      "Epoch [1/1]\n",
      "\tBatch [100]:\n",
      "\t Loss: 0.6594\n",
      "Epoch [1/1]\n",
      "\tBatch [101]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [102]:\n",
      "\t Loss: 0.5146\n",
      "Epoch [1/1]\n",
      "\tBatch [103]:\n",
      "\t Loss: 0.5801\n",
      "Epoch [1/1]\n",
      "\tBatch [104]:\n",
      "\t Loss: 0.5028\n",
      "Epoch [1/1]\n",
      "\tBatch [105]:\n",
      "\t Loss: 0.4878\n",
      "Epoch [1/1]\n",
      "\tBatch [106]:\n",
      "\t Loss: 0.6340\n",
      "Epoch [1/1]\n",
      "\tBatch [107]:\n",
      "\t Loss: 0.6043\n",
      "Epoch [1/1]\n",
      "\tBatch [108]:\n",
      "\t Loss: 0.4859\n",
      "Epoch [1/1]\n",
      "\tBatch [109]:\n",
      "\t Loss: 0.4585\n",
      "Epoch [1/1]\n",
      "\tBatch [110]:\n",
      "\t Loss: 0.6876\n",
      "Epoch [1/1]\n",
      "\tBatch [111]:\n",
      "\t Loss: 0.6154\n",
      "Epoch [1/1]\n",
      "\tBatch [112]:\n",
      "\t Loss: 0.5282\n",
      "Epoch [1/1]\n",
      "\tBatch [113]:\n",
      "\t Loss: 0.5618\n",
      "Epoch [1/1]\n",
      "\tBatch [114]:\n",
      "\t Loss: 0.5698\n",
      "Epoch [1/1]\n",
      "\tBatch [115]:\n",
      "\t Loss: 0.4995\n",
      "Epoch [1/1]\n",
      "\tBatch [116]:\n",
      "\t Loss: 0.5461\n",
      "Epoch [1/1]\n",
      "\tBatch [117]:\n",
      "\t Loss: 0.4883\n",
      "Epoch [1/1]\n",
      "\tBatch [118]:\n",
      "\t Loss: 0.5351\n",
      "Epoch [1/1]\n",
      "\tBatch [119]:\n",
      "\t Loss: 0.5297\n",
      "Epoch [1/1]\n",
      "\tBatch [120]:\n",
      "\t Loss: 0.5270\n",
      "Epoch [1/1]\n",
      "\tBatch [121]:\n",
      "\t Loss: 0.6254\n",
      "Epoch [1/1]\n",
      "\tBatch [122]:\n",
      "\t Loss: 0.5775\n",
      "Epoch [1/1]\n",
      "\tBatch [123]:\n",
      "\t Loss: 0.5273\n",
      "Epoch [1/1]\n",
      "\tBatch [124]:\n",
      "\t Loss: 0.5926\n",
      "Epoch [1/1]\n",
      "\tBatch [125]:\n",
      "\t Loss: 0.5510\n",
      "Epoch [1/1]\n",
      "\tBatch [126]:\n",
      "\t Loss: 0.4871\n",
      "Epoch [1/1]\n",
      "\tBatch [127]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [128]:\n",
      "\t Loss: 0.7145\n",
      "Epoch [1/1]\n",
      "\tBatch [129]:\n",
      "\t Loss: 0.5153\n",
      "Epoch [1/1]\n",
      "\tBatch [130]:\n",
      "\t Loss: 0.6058\n",
      "Epoch [1/1]\n",
      "\tBatch [131]:\n",
      "\t Loss: 0.5963\n",
      "Epoch [1/1]\n",
      "\tBatch [132]:\n",
      "\t Loss: 0.5542\n",
      "Epoch [1/1]\n",
      "\tBatch [133]:\n",
      "\t Loss: 0.6021\n",
      "Epoch [1/1]\n",
      "\tBatch [134]:\n",
      "\t Loss: 0.5235\n",
      "Epoch [1/1]\n",
      "\tBatch [135]:\n",
      "\t Loss: 0.5149\n",
      "Epoch [1/1]\n",
      "\tBatch [136]:\n",
      "\t Loss: 0.5072\n",
      "Epoch [1/1]\n",
      "\tBatch [137]:\n",
      "\t Loss: 0.5744\n",
      "Epoch [1/1]\n",
      "\tBatch [138]:\n",
      "\t Loss: 0.6078\n",
      "Epoch [1/1]\n",
      "\tBatch [139]:\n",
      "\t Loss: 0.6205\n",
      "Epoch [1/1]\n",
      "\tBatch [140]:\n",
      "\t Loss: 0.5392\n",
      "Epoch [1/1]\n",
      "\tBatch [141]:\n",
      "\t Loss: 0.6313\n",
      "Epoch [1/1]\n",
      "\tBatch [142]:\n",
      "\t Loss: 0.5198\n",
      "Epoch [1/1]\n",
      "\tBatch [143]:\n",
      "\t Loss: 0.5628\n",
      "Epoch [1/1]\n",
      "\tBatch [144]:\n",
      "\t Loss: 0.5858\n",
      "Epoch [1/1]\n",
      "\tBatch [145]:\n",
      "\t Loss: 0.5759\n",
      "Epoch [1/1]\n",
      "\tBatch [146]:\n",
      "\t Loss: 0.5281\n",
      "Epoch [1/1]\n",
      "\tBatch [147]:\n",
      "\t Loss: 0.5408\n",
      "Epoch [1/1]\n",
      "\tBatch [148]:\n",
      "\t Loss: 0.5847\n",
      "Epoch [1/1]\n",
      "\tBatch [149]:\n",
      "\t Loss: 0.6063\n",
      "Epoch [1/1]\n",
      "\tBatch [150]:\n",
      "\t Loss: 0.5073\n",
      "Epoch [1/1]\n",
      "\tBatch [151]:\n",
      "\t Loss: 0.6070\n",
      "Epoch [1/1]\n",
      "\tBatch [152]:\n",
      "\t Loss: 0.5755\n",
      "Epoch [1/1]\n",
      "\tBatch [153]:\n",
      "\t Loss: 0.5544\n",
      "Epoch [1/1]\n",
      "\tBatch [154]:\n",
      "\t Loss: 0.5653\n",
      "Epoch [1/1]\n",
      "\tBatch [155]:\n",
      "\t Loss: 0.5087\n",
      "Epoch [1/1]\n",
      "\tBatch [156]:\n",
      "\t Loss: 0.5625\n",
      "Epoch [1/1]\n",
      "\tBatch [157]:\n",
      "\t Loss: 0.6070\n",
      "Epoch [1/1]\n",
      "\tBatch [158]:\n",
      "\t Loss: 0.5623\n",
      "Epoch [1/1]\n",
      "\tBatch [159]:\n",
      "\t Loss: 0.5400\n",
      "Epoch [1/1]\n",
      "\tBatch [160]:\n",
      "\t Loss: 0.5611\n",
      "Epoch [1/1]\n",
      "\tBatch [161]:\n",
      "\t Loss: 0.6508\n",
      "Epoch [1/1]\n",
      "\tBatch [162]:\n",
      "\t Loss: 0.5492\n",
      "Epoch [1/1]\n",
      "\tBatch [163]:\n",
      "\t Loss: 0.5614\n",
      "Epoch [1/1]\n",
      "\tBatch [164]:\n",
      "\t Loss: 0.5943\n",
      "Epoch [1/1]\n",
      "\tBatch [165]:\n",
      "\t Loss: 0.6455\n",
      "Epoch [1/1]\n",
      "\tBatch [166]:\n",
      "\t Loss: 0.5246\n",
      "Epoch [1/1]\n",
      "\tBatch [167]:\n",
      "\t Loss: 0.5724\n",
      "Epoch [1/1]\n",
      "\tBatch [168]:\n",
      "\t Loss: 0.6116\n",
      "Epoch [1/1]\n",
      "\tBatch [169]:\n",
      "\t Loss: 0.5574\n",
      "Epoch [1/1]\n",
      "\tBatch [170]:\n",
      "\t Loss: 0.5465\n",
      "Epoch [1/1]\n",
      "\tBatch [171]:\n",
      "\t Loss: 0.6039\n",
      "Epoch [1/1]\n",
      "\tBatch [172]:\n",
      "\t Loss: 0.5932\n",
      "Epoch [1/1]\n",
      "\tBatch [173]:\n",
      "\t Loss: 0.6022\n",
      "Epoch [1/1]\n",
      "\tBatch [174]:\n",
      "\t Loss: 0.5540\n",
      "Epoch [1/1]\n",
      "\tBatch [175]:\n",
      "\t Loss: 0.5721\n",
      "Epoch [1/1]\n",
      "\tBatch [176]:\n",
      "\t Loss: 0.5233\n",
      "Epoch [1/1]\n",
      "\tBatch [177]:\n",
      "\t Loss: 0.5390\n",
      "Epoch [1/1]\n",
      "\tBatch [178]:\n",
      "\t Loss: 0.5525\n",
      "Epoch [1/1]\n",
      "\tBatch [179]:\n",
      "\t Loss: 0.6296\n",
      "Epoch [1/1]\n",
      "\tBatch [180]:\n",
      "\t Loss: 0.5972\n",
      "Epoch [1/1]\n",
      "\tBatch [181]:\n",
      "\t Loss: 0.5520\n",
      "Epoch [1/1]\n",
      "\tBatch [182]:\n",
      "\t Loss: 0.6322\n",
      "Epoch [1/1]\n",
      "\tBatch [183]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [184]:\n",
      "\t Loss: 0.5626\n",
      "Epoch [1/1]\n",
      "\tBatch [185]:\n",
      "\t Loss: 0.6350\n",
      "Epoch [1/1]\n",
      "\tBatch [186]:\n",
      "\t Loss: 0.5641\n",
      "Epoch [1/1]\n",
      "\tBatch [187]:\n",
      "\t Loss: 0.6342\n",
      "Epoch [1/1]\n",
      "\tBatch [188]:\n",
      "\t Loss: 0.5724\n",
      "Epoch [1/1]\n",
      "\tBatch [189]:\n",
      "\t Loss: 0.5417\n",
      "Epoch [1/1]\n",
      "\tBatch [190]:\n",
      "\t Loss: 0.5386\n",
      "Epoch [1/1]\n",
      "\tBatch [191]:\n",
      "\t Loss: 0.5839\n",
      "Epoch [1/1]\n",
      "\tBatch [192]:\n",
      "\t Loss: 0.5718\n",
      "Epoch [1/1]\n",
      "\tBatch [193]:\n",
      "\t Loss: 0.5352\n",
      "Epoch [1/1]\n",
      "\tBatch [194]:\n",
      "\t Loss: 0.5536\n",
      "Epoch [1/1]\n",
      "\tBatch [195]:\n",
      "\t Loss: 0.6068\n",
      "Epoch [1/1]\n",
      "\tBatch [196]:\n",
      "\t Loss: 0.5970\n",
      "Epoch [1/1]\n",
      "\tBatch [197]:\n",
      "\t Loss: 0.5658\n",
      "Epoch [1/1]\n",
      "\tBatch [198]:\n",
      "\t Loss: 0.6426\n",
      "Epoch [1/1]\n",
      "\tBatch [199]:\n",
      "\t Loss: 0.5276\n",
      "Epoch [1/1]\n",
      "\tBatch [200]:\n",
      "\t Loss: 0.5849\n",
      "Epoch [1/1]\n",
      "\tBatch [201]:\n",
      "\t Loss: 0.6202\n",
      "Epoch [1/1]\n",
      "\tBatch [202]:\n",
      "\t Loss: 0.5748\n",
      "Epoch [1/1]\n",
      "\tBatch [203]:\n",
      "\t Loss: 0.6049\n",
      "Epoch [1/1]\n",
      "\tBatch [204]:\n",
      "\t Loss: 0.5992\n",
      "Epoch [1/1]\n",
      "\tBatch [205]:\n",
      "\t Loss: 0.5555\n",
      "Epoch [1/1]\n",
      "\tBatch [206]:\n",
      "\t Loss: 0.5465\n",
      "Epoch [1/1]\n",
      "\tBatch [207]:\n",
      "\t Loss: 0.5391\n",
      "Epoch [1/1]\n",
      "\tBatch [208]:\n",
      "\t Loss: 0.5464\n",
      "Epoch [1/1]\n",
      "\tBatch [209]:\n",
      "\t Loss: 0.5647\n",
      "Epoch [1/1]\n",
      "\tBatch [210]:\n",
      "\t Loss: 0.5610\n",
      "Epoch [1/1]\n",
      "\tBatch [211]:\n",
      "\t Loss: 0.5111\n",
      "Epoch [1/1]\n",
      "\tBatch [212]:\n",
      "\t Loss: 0.5629\n",
      "Epoch [1/1]\n",
      "\tBatch [213]:\n",
      "\t Loss: 0.5991\n",
      "Epoch [1/1]\n",
      "\tBatch [214]:\n",
      "\t Loss: 0.4746\n",
      "Epoch [1/1]\n",
      "\tBatch [215]:\n",
      "\t Loss: 0.4850\n",
      "Epoch [1/1]\n",
      "\tBatch [216]:\n",
      "\t Loss: 0.6242\n",
      "Epoch [1/1]\n",
      "\tBatch [217]:\n",
      "\t Loss: 0.5388\n",
      "Epoch [1/1]\n",
      "\tBatch [218]:\n",
      "\t Loss: 0.5596\n",
      "Epoch [1/1]\n",
      "\tBatch [219]:\n",
      "\t Loss: 0.5527\n",
      "Epoch [1/1]\n",
      "\tBatch [220]:\n",
      "\t Loss: 0.5165\n",
      "Epoch [1/1]\n",
      "\tBatch [221]:\n",
      "\t Loss: 0.6313\n",
      "Epoch [1/1]\n",
      "\tBatch [222]:\n",
      "\t Loss: 0.5244\n",
      "Epoch [1/1]\n",
      "\tBatch [223]:\n",
      "\t Loss: 0.5529\n",
      "Epoch [1/1]\n",
      "\tBatch [224]:\n",
      "\t Loss: 0.6319\n",
      "Epoch [1/1]\n",
      "\tBatch [225]:\n",
      "\t Loss: 0.5901\n",
      "Epoch [1/1]\n",
      "\tBatch [226]:\n",
      "\t Loss: 0.5528\n",
      "Epoch [1/1]\n",
      "\tBatch [227]:\n",
      "\t Loss: 0.6226\n",
      "Epoch [1/1]\n",
      "\tBatch [228]:\n",
      "\t Loss: 0.5563\n",
      "Epoch [1/1]\n",
      "\tBatch [229]:\n",
      "\t Loss: 0.5418\n",
      "Epoch [1/1]\n",
      "\tBatch [230]:\n",
      "\t Loss: 0.6096\n",
      "Epoch [1/1]\n",
      "\tBatch [231]:\n",
      "\t Loss: 0.5769\n",
      "Epoch [1/1]\n",
      "\tBatch [232]:\n",
      "\t Loss: 0.4913\n",
      "Epoch [1/1]\n",
      "\tBatch [233]:\n",
      "\t Loss: 0.5490\n",
      "Epoch [1/1]\n",
      "\tBatch [234]:\n",
      "\t Loss: 0.5899\n",
      "Epoch [1/1]\n",
      "\tBatch [235]:\n",
      "\t Loss: 0.5446\n",
      "Epoch [1/1]\n",
      "\tBatch [236]:\n",
      "\t Loss: 0.5382\n",
      "Epoch [1/1]\n",
      "\tBatch [237]:\n",
      "\t Loss: 0.5859\n",
      "Epoch [1/1]\n",
      "\tBatch [238]:\n",
      "\t Loss: 0.5289\n",
      "Epoch [1/1]\n",
      "\tBatch [239]:\n",
      "\t Loss: 0.5784\n",
      "Epoch [1/1]\n",
      "\tBatch [240]:\n",
      "\t Loss: 0.6069\n",
      "Epoch [1/1]\n",
      "\tBatch [241]:\n",
      "\t Loss: 0.6202\n",
      "Epoch [1/1]\n",
      "\tBatch [242]:\n",
      "\t Loss: 0.5519\n",
      "Epoch [1/1]\n",
      "\tBatch [243]:\n",
      "\t Loss: 0.4654\n",
      "Epoch [1/1]\n",
      "\tBatch [244]:\n",
      "\t Loss: 0.5391\n",
      "Epoch [1/1]\n",
      "\tBatch [245]:\n",
      "\t Loss: 0.5266\n",
      "Epoch [1/1]\n",
      "\tBatch [246]:\n",
      "\t Loss: 0.5148\n",
      "Epoch [1/1]\n",
      "\tBatch [247]:\n",
      "\t Loss: 0.5865\n",
      "Epoch [1/1]\n",
      "\tBatch [248]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [249]:\n",
      "\t Loss: 0.4931\n",
      "Epoch [1/1]\n",
      "\tBatch [250]:\n",
      "\t Loss: 0.6222\n",
      "Epoch [1/1]\n",
      "\tBatch [251]:\n",
      "\t Loss: 0.5498\n",
      "Epoch [1/1]\n",
      "\tBatch [252]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [253]:\n",
      "\t Loss: 0.5844\n",
      "Epoch [1/1]\n",
      "\tBatch [254]:\n",
      "\t Loss: 0.5069\n",
      "Epoch [1/1]\n",
      "\tBatch [255]:\n",
      "\t Loss: 0.5944\n",
      "Epoch [1/1]\n",
      "\tBatch [256]:\n",
      "\t Loss: 0.5724\n",
      "Epoch [1/1]\n",
      "\tBatch [257]:\n",
      "\t Loss: 0.5411\n",
      "Epoch [1/1]\n",
      "\tBatch [258]:\n",
      "\t Loss: 0.6587\n",
      "Epoch [1/1]\n",
      "\tBatch [259]:\n",
      "\t Loss: 0.5633\n",
      "Epoch [1/1]\n",
      "\tBatch [260]:\n",
      "\t Loss: 0.5010\n",
      "Epoch [1/1]\n",
      "\tBatch [261]:\n",
      "\t Loss: 0.5196\n",
      "Epoch [1/1]\n",
      "\tBatch [262]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [263]:\n",
      "\t Loss: 0.4879\n",
      "Epoch [1/1]\n",
      "\tBatch [264]:\n",
      "\t Loss: 0.5944\n",
      "Epoch [1/1]\n",
      "\tBatch [265]:\n",
      "\t Loss: 0.4489\n",
      "Epoch [1/1]\n",
      "\tBatch [266]:\n",
      "\t Loss: 0.5310\n",
      "Epoch [1/1]\n",
      "\tBatch [267]:\n",
      "\t Loss: 0.5284\n",
      "Epoch [1/1]\n",
      "\tBatch [268]:\n",
      "\t Loss: 0.6205\n",
      "Epoch [1/1]\n",
      "\tBatch [269]:\n",
      "\t Loss: 0.5006\n",
      "Epoch [1/1]\n",
      "\tBatch [270]:\n",
      "\t Loss: 0.6071\n",
      "Epoch [1/1]\n",
      "\tBatch [271]:\n",
      "\t Loss: 0.5150\n",
      "Epoch [1/1]\n",
      "\tBatch [272]:\n",
      "\t Loss: 0.5131\n",
      "Epoch [1/1]\n",
      "\tBatch [273]:\n",
      "\t Loss: 0.5800\n",
      "Epoch [1/1]\n",
      "\tBatch [274]:\n",
      "\t Loss: 0.5141\n",
      "Epoch [1/1]\n",
      "\tBatch [275]:\n",
      "\t Loss: 0.4878\n",
      "Epoch [1/1]\n",
      "\tBatch [276]:\n",
      "\t Loss: 0.5760\n",
      "Epoch [1/1]\n",
      "\tBatch [277]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [278]:\n",
      "\t Loss: 0.5182\n",
      "Epoch [1/1]\n",
      "\tBatch [279]:\n",
      "\t Loss: 0.6558\n",
      "Epoch [1/1]\n",
      "\tBatch [280]:\n",
      "\t Loss: 0.4843\n",
      "Epoch [1/1]\n",
      "\tBatch [281]:\n",
      "\t Loss: 0.5286\n",
      "Epoch [1/1]\n",
      "\tBatch [282]:\n",
      "\t Loss: 0.5743\n",
      "Epoch [1/1]\n",
      "\tBatch [283]:\n",
      "\t Loss: 0.5840\n",
      "Epoch [1/1]\n",
      "\tBatch [284]:\n",
      "\t Loss: 0.5649\n",
      "Epoch [1/1]\n",
      "\tBatch [285]:\n",
      "\t Loss: 0.5733\n",
      "Epoch [1/1]\n",
      "\tBatch [286]:\n",
      "\t Loss: 0.5961\n",
      "Epoch [1/1]\n",
      "\tBatch [287]:\n",
      "\t Loss: 0.6350\n",
      "Epoch [1/1]\n",
      "\tBatch [288]:\n",
      "\t Loss: 0.5858\n",
      "Epoch [1/1]\n",
      "\tBatch [289]:\n",
      "\t Loss: 0.5076\n",
      "Epoch [1/1]\n",
      "\tBatch [290]:\n",
      "\t Loss: 0.6031\n",
      "Epoch [1/1]\n",
      "\tBatch [291]:\n",
      "\t Loss: 0.4966\n",
      "Epoch [1/1]\n",
      "\tBatch [292]:\n",
      "\t Loss: 0.6147\n",
      "Epoch [1/1]\n",
      "\tBatch [293]:\n",
      "\t Loss: 0.5534\n",
      "Epoch [1/1]\n",
      "\tBatch [294]:\n",
      "\t Loss: 0.5218\n",
      "Epoch [1/1]\n",
      "\tBatch [295]:\n",
      "\t Loss: 0.5511\n",
      "Epoch [1/1]\n",
      "\tBatch [296]:\n",
      "\t Loss: 0.5623\n",
      "Epoch [1/1]\n",
      "\tBatch [297]:\n",
      "\t Loss: 0.5051\n",
      "Epoch [1/1]\n",
      "\tBatch [298]:\n",
      "\t Loss: 0.4899\n",
      "Epoch [1/1]\n",
      "\tBatch [299]:\n",
      "\t Loss: 0.5881\n",
      "Epoch [1/1]\n",
      "\tBatch [300]:\n",
      "\t Loss: 0.6076\n",
      "Epoch [1/1]\n",
      "\tBatch [301]:\n",
      "\t Loss: 0.6203\n",
      "Epoch [1/1]\n",
      "\tBatch [302]:\n",
      "\t Loss: 0.5776\n",
      "Epoch [1/1]\n",
      "\tBatch [303]:\n",
      "\t Loss: 0.5133\n",
      "Epoch [1/1]\n",
      "\tBatch [304]:\n",
      "\t Loss: 0.5657\n",
      "Epoch [1/1]\n",
      "\tBatch [305]:\n",
      "\t Loss: 0.5393\n",
      "Epoch [1/1]\n",
      "\tBatch [306]:\n",
      "\t Loss: 0.6551\n",
      "Epoch [1/1]\n",
      "\tBatch [307]:\n",
      "\t Loss: 0.5839\n",
      "Epoch [1/1]\n",
      "\tBatch [308]:\n",
      "\t Loss: 0.6649\n",
      "Epoch [1/1]\n",
      "\tBatch [309]:\n",
      "\t Loss: 0.5564\n",
      "Epoch [1/1]\n",
      "\tBatch [310]:\n",
      "\t Loss: 0.5318\n",
      "Epoch [1/1]\n",
      "\tBatch [311]:\n",
      "\t Loss: 0.5176\n",
      "Epoch [1/1]\n",
      "\tBatch [312]:\n",
      "\t Loss: 0.5845\n",
      "Epoch [1/1]\n",
      "\tBatch [313]:\n",
      "\t Loss: 0.5309\n",
      "Epoch [1/1]\n",
      "\tBatch [314]:\n",
      "\t Loss: 0.6126\n",
      "Epoch [1/1]\n",
      "\tBatch [315]:\n",
      "\t Loss: 0.5939\n",
      "Epoch [1/1]\n",
      "\tBatch [316]:\n",
      "\t Loss: 0.6351\n",
      "Epoch [1/1]\n",
      "\tBatch [317]:\n",
      "\t Loss: 0.5429\n",
      "Epoch [1/1]\n",
      "\tBatch [318]:\n",
      "\t Loss: 0.4779\n",
      "Epoch [1/1]\n",
      "\tBatch [319]:\n",
      "\t Loss: 0.5167\n",
      "Epoch [1/1]\n",
      "\tBatch [320]:\n",
      "\t Loss: 0.5871\n",
      "Epoch [1/1]\n",
      "\tBatch [321]:\n",
      "\t Loss: 0.5270\n",
      "Epoch [1/1]\n",
      "\tBatch [322]:\n",
      "\t Loss: 0.5509\n",
      "Epoch [1/1]\n",
      "\tBatch [323]:\n",
      "\t Loss: 0.5908\n",
      "Epoch [1/1]\n",
      "\tBatch [324]:\n",
      "\t Loss: 0.4887\n",
      "Epoch [1/1]\n",
      "\tBatch [325]:\n",
      "\t Loss: 0.5659\n",
      "Epoch [1/1]\n",
      "\tBatch [326]:\n",
      "\t Loss: 0.6056\n",
      "Epoch [1/1]\n",
      "\tBatch [327]:\n",
      "\t Loss: 0.5659\n",
      "Epoch [1/1]\n",
      "\tBatch [328]:\n",
      "\t Loss: 0.5897\n",
      "Epoch [1/1]\n",
      "\tBatch [329]:\n",
      "\t Loss: 0.6475\n",
      "Epoch [1/1]\n",
      "\tBatch [330]:\n",
      "\t Loss: 0.5947\n",
      "Epoch [1/1]\n",
      "\tBatch [331]:\n",
      "\t Loss: 0.5734\n",
      "Epoch [1/1]\n",
      "\tBatch [332]:\n",
      "\t Loss: 0.5334\n",
      "Epoch [1/1]\n",
      "\tBatch [333]:\n",
      "\t Loss: 0.5926\n",
      "Epoch [1/1]\n",
      "\tBatch [334]:\n",
      "\t Loss: 0.5491\n",
      "Epoch [1/1]\n",
      "\tBatch [335]:\n",
      "\t Loss: 0.5236\n",
      "Epoch [1/1]\n",
      "\tBatch [336]:\n",
      "\t Loss: 0.5133\n",
      "Epoch [1/1]\n",
      "\tBatch [337]:\n",
      "\t Loss: 0.6113\n",
      "Epoch [1/1]\n",
      "\tBatch [338]:\n",
      "\t Loss: 0.5535\n",
      "Epoch [1/1]\n",
      "\tBatch [339]:\n",
      "\t Loss: 0.5419\n",
      "Epoch [1/1]\n",
      "\tBatch [340]:\n",
      "\t Loss: 0.5296\n",
      "Epoch [1/1]\n",
      "\tBatch [341]:\n",
      "\t Loss: 0.5167\n",
      "Epoch [1/1]\n",
      "\tBatch [342]:\n",
      "\t Loss: 0.6112\n",
      "Epoch [1/1]\n",
      "\tBatch [343]:\n",
      "\t Loss: 0.6404\n",
      "Epoch [1/1]\n",
      "\tBatch [344]:\n",
      "\t Loss: 0.5878\n",
      "Epoch [1/1]\n",
      "\tBatch [345]:\n",
      "\t Loss: 0.5899\n",
      "Epoch [1/1]\n",
      "\tBatch [346]:\n",
      "\t Loss: 0.5855\n",
      "Epoch [1/1]\n",
      "\tBatch [347]:\n",
      "\t Loss: 0.5292\n",
      "Epoch [1/1]\n",
      "\tBatch [348]:\n",
      "\t Loss: 0.5295\n",
      "Epoch [1/1]\n",
      "\tBatch [349]:\n",
      "\t Loss: 0.4949\n",
      "Epoch [1/1]\n",
      "\tBatch [350]:\n",
      "\t Loss: 0.5963\n",
      "Epoch [1/1]\n",
      "\tBatch [351]:\n",
      "\t Loss: 0.5722\n",
      "Epoch [1/1]\n",
      "\tBatch [352]:\n",
      "\t Loss: 0.5841\n",
      "Epoch [1/1]\n",
      "\tBatch [353]:\n",
      "\t Loss: 0.5745\n",
      "Epoch [1/1]\n",
      "\tBatch [354]:\n",
      "\t Loss: 0.5087\n",
      "Epoch [1/1]\n",
      "\tBatch [355]:\n",
      "\t Loss: 0.6700\n",
      "Epoch [1/1]\n",
      "\tBatch [356]:\n",
      "\t Loss: 0.5606\n",
      "Epoch [1/1]\n",
      "\tBatch [357]:\n",
      "\t Loss: 0.5426\n",
      "Epoch [1/1]\n",
      "\tBatch [358]:\n",
      "\t Loss: 0.6646\n",
      "Epoch [1/1]\n",
      "\tBatch [359]:\n",
      "\t Loss: 0.4469\n",
      "Epoch [1/1]\n",
      "\tBatch [360]:\n",
      "\t Loss: 0.5150\n",
      "Epoch [1/1]\n",
      "\tBatch [361]:\n",
      "\t Loss: 0.5826\n",
      "Epoch [1/1]\n",
      "\tBatch [362]:\n",
      "\t Loss: 0.5536\n",
      "Epoch [1/1]\n",
      "\tBatch [363]:\n",
      "\t Loss: 0.5184\n",
      "Epoch [1/1]\n",
      "\tBatch [364]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [365]:\n",
      "\t Loss: 0.6567\n",
      "Epoch [1/1]\n",
      "\tBatch [366]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [367]:\n",
      "\t Loss: 0.5397\n",
      "Epoch [1/1]\n",
      "\tBatch [368]:\n",
      "\t Loss: 0.5389\n",
      "Epoch [1/1]\n",
      "\tBatch [369]:\n",
      "\t Loss: 0.4789\n",
      "Epoch [1/1]\n",
      "\tBatch [370]:\n",
      "\t Loss: 0.6373\n",
      "Epoch [1/1]\n",
      "\tBatch [371]:\n",
      "\t Loss: 0.5271\n",
      "Epoch [1/1]\n",
      "\tBatch [372]:\n",
      "\t Loss: 0.5274\n",
      "Epoch [1/1]\n",
      "\tBatch [373]:\n",
      "\t Loss: 0.5622\n",
      "Epoch [1/1]\n",
      "\tBatch [374]:\n",
      "\t Loss: 0.6467\n",
      "Epoch [1/1]\n",
      "\tBatch [375]:\n",
      "\t Loss: 0.5626\n",
      "Epoch [1/1]\n",
      "\tBatch [376]:\n",
      "\t Loss: 0.5176\n",
      "Epoch [1/1]\n",
      "\tBatch [377]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [378]:\n",
      "\t Loss: 0.6067\n",
      "Epoch [1/1]\n",
      "\tBatch [379]:\n",
      "\t Loss: 0.5307\n",
      "Epoch [1/1]\n",
      "\tBatch [380]:\n",
      "\t Loss: 0.5432\n",
      "Epoch [1/1]\n",
      "\tBatch [381]:\n",
      "\t Loss: 0.5731\n",
      "Epoch [1/1]\n",
      "\tBatch [382]:\n",
      "\t Loss: 0.4786\n",
      "Epoch [1/1]\n",
      "\tBatch [383]:\n",
      "\t Loss: 0.5952\n",
      "Epoch [1/1]\n",
      "\tBatch [384]:\n",
      "\t Loss: 0.5626\n",
      "Epoch [1/1]\n",
      "\tBatch [385]:\n",
      "\t Loss: 0.4966\n",
      "Epoch [1/1]\n",
      "\tBatch [386]:\n",
      "\t Loss: 0.5533\n",
      "Epoch [1/1]\n",
      "\tBatch [387]:\n",
      "\t Loss: 0.6003\n",
      "Epoch [1/1]\n",
      "\tBatch [388]:\n",
      "\t Loss: 0.5996\n",
      "Epoch [1/1]\n",
      "\tBatch [389]:\n",
      "\t Loss: 0.5636\n",
      "Epoch [1/1]\n",
      "\tBatch [390]:\n",
      "\t Loss: 0.5985\n",
      "Epoch [1/1]\n",
      "\tBatch [391]:\n",
      "\t Loss: 0.5972\n",
      "Epoch [1/1]\n",
      "\tBatch [392]:\n",
      "\t Loss: 0.5274\n",
      "Epoch [1/1]\n",
      "\tBatch [393]:\n",
      "\t Loss: 0.5520\n",
      "Epoch [1/1]\n",
      "\tBatch [394]:\n",
      "\t Loss: 0.5403\n",
      "Epoch [1/1]\n",
      "\tBatch [395]:\n",
      "\t Loss: 0.6053\n",
      "Epoch [1/1]\n",
      "\tBatch [396]:\n",
      "\t Loss: 0.6253\n",
      "Epoch [1/1]\n",
      "\tBatch [397]:\n",
      "\t Loss: 0.5517\n",
      "Epoch [1/1]\n",
      "\tBatch [398]:\n",
      "\t Loss: 0.5333\n",
      "Epoch [1/1]\n",
      "\tBatch [399]:\n",
      "\t Loss: 0.5537\n",
      "Epoch [1/1]\n",
      "\tBatch [400]:\n",
      "\t Loss: 0.5928\n",
      "Epoch [1/1]\n",
      "\tBatch [401]:\n",
      "\t Loss: 0.6038\n",
      "Epoch [1/1]\n",
      "\tBatch [402]:\n",
      "\t Loss: 0.5334\n",
      "Epoch [1/1]\n",
      "\tBatch [403]:\n",
      "\t Loss: 0.5329\n",
      "Epoch [1/1]\n",
      "\tBatch [404]:\n",
      "\t Loss: 0.5833\n",
      "Epoch [1/1]\n",
      "\tBatch [405]:\n",
      "\t Loss: 0.5839\n",
      "Epoch [1/1]\n",
      "\tBatch [406]:\n",
      "\t Loss: 0.5834\n",
      "Epoch [1/1]\n",
      "\tBatch [407]:\n",
      "\t Loss: 0.5520\n",
      "Epoch [1/1]\n",
      "\tBatch [408]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [409]:\n",
      "\t Loss: 0.5846\n",
      "Epoch [1/1]\n",
      "\tBatch [410]:\n",
      "\t Loss: 0.5951\n",
      "Epoch [1/1]\n",
      "\tBatch [411]:\n",
      "\t Loss: 0.5841\n",
      "Epoch [1/1]\n",
      "\tBatch [412]:\n",
      "\t Loss: 0.5526\n",
      "Epoch [1/1]\n",
      "\tBatch [413]:\n",
      "\t Loss: 0.5417\n",
      "Epoch [1/1]\n",
      "\tBatch [414]:\n",
      "\t Loss: 0.5847\n",
      "Epoch [1/1]\n",
      "\tBatch [415]:\n",
      "\t Loss: 0.4982\n",
      "Epoch [1/1]\n",
      "\tBatch [416]:\n",
      "\t Loss: 0.5192\n",
      "Epoch [1/1]\n",
      "\tBatch [417]:\n",
      "\t Loss: 0.5175\n",
      "Epoch [1/1]\n",
      "\tBatch [418]:\n",
      "\t Loss: 0.6324\n",
      "Epoch [1/1]\n",
      "\tBatch [419]:\n",
      "\t Loss: 0.5636\n",
      "Epoch [1/1]\n",
      "\tBatch [420]:\n",
      "\t Loss: 0.6087\n",
      "Epoch [1/1]\n",
      "\tBatch [421]:\n",
      "\t Loss: 0.4928\n",
      "Epoch [1/1]\n",
      "\tBatch [422]:\n",
      "\t Loss: 0.5868\n",
      "Epoch [1/1]\n",
      "\tBatch [423]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [424]:\n",
      "\t Loss: 0.6537\n",
      "Epoch [1/1]\n",
      "\tBatch [425]:\n",
      "\t Loss: 0.4640\n",
      "Epoch [1/1]\n",
      "\tBatch [426]:\n",
      "\t Loss: 0.5730\n",
      "Epoch [1/1]\n",
      "\tBatch [427]:\n",
      "\t Loss: 0.5079\n",
      "Epoch [1/1]\n",
      "\tBatch [428]:\n",
      "\t Loss: 0.5187\n",
      "Epoch [1/1]\n",
      "\tBatch [429]:\n",
      "\t Loss: 0.6516\n",
      "Epoch [1/1]\n",
      "\tBatch [430]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [431]:\n",
      "\t Loss: 0.5960\n",
      "Epoch [1/1]\n",
      "\tBatch [432]:\n",
      "\t Loss: 0.5945\n",
      "Epoch [1/1]\n",
      "\tBatch [433]:\n",
      "\t Loss: 0.5833\n",
      "Epoch [1/1]\n",
      "\tBatch [434]:\n",
      "\t Loss: 0.5620\n",
      "Epoch [1/1]\n",
      "\tBatch [435]:\n",
      "\t Loss: 0.5831\n",
      "Epoch [1/1]\n",
      "\tBatch [436]:\n",
      "\t Loss: 0.4629\n",
      "Epoch [1/1]\n",
      "\tBatch [437]:\n",
      "\t Loss: 0.5420\n",
      "Epoch [1/1]\n",
      "\tBatch [438]:\n",
      "\t Loss: 0.6046\n",
      "Epoch [1/1]\n",
      "\tBatch [439]:\n",
      "\t Loss: 0.5518\n",
      "Epoch [1/1]\n",
      "\tBatch [440]:\n",
      "\t Loss: 0.5618\n",
      "Epoch [1/1]\n",
      "\tBatch [441]:\n",
      "\t Loss: 0.6511\n",
      "Epoch [1/1]\n",
      "\tBatch [442]:\n",
      "\t Loss: 0.5514\n",
      "Epoch [1/1]\n",
      "\tBatch [443]:\n",
      "\t Loss: 0.4961\n",
      "Epoch [1/1]\n",
      "\tBatch [444]:\n",
      "\t Loss: 0.5950\n",
      "Epoch [1/1]\n",
      "\tBatch [445]:\n",
      "\t Loss: 0.5840\n",
      "Epoch [1/1]\n",
      "\tBatch [446]:\n",
      "\t Loss: 0.5406\n",
      "Epoch [1/1]\n",
      "\tBatch [447]:\n",
      "\t Loss: 0.5188\n",
      "Epoch [1/1]\n",
      "\tBatch [448]:\n",
      "\t Loss: 0.5289\n",
      "Epoch [1/1]\n",
      "\tBatch [449]:\n",
      "\t Loss: 0.5743\n",
      "Epoch [1/1]\n",
      "\tBatch [450]:\n",
      "\t Loss: 0.5972\n",
      "Epoch [1/1]\n",
      "\tBatch [451]:\n",
      "\t Loss: 0.4698\n",
      "Epoch [1/1]\n",
      "\tBatch [452]:\n",
      "\t Loss: 0.6569\n",
      "Epoch [1/1]\n",
      "\tBatch [453]:\n",
      "\t Loss: 0.5157\n",
      "Epoch [1/1]\n",
      "\tBatch [454]:\n",
      "\t Loss: 0.5398\n",
      "Epoch [1/1]\n",
      "\tBatch [455]:\n",
      "\t Loss: 0.4821\n",
      "Epoch [1/1]\n",
      "\tBatch [456]:\n",
      "\t Loss: 0.6466\n",
      "Epoch [1/1]\n",
      "\tBatch [457]:\n",
      "\t Loss: 0.6096\n",
      "Epoch [1/1]\n",
      "\tBatch [458]:\n",
      "\t Loss: 0.5280\n",
      "Epoch [1/1]\n",
      "\tBatch [459]:\n",
      "\t Loss: 0.5524\n",
      "Epoch [1/1]\n",
      "\tBatch [460]:\n",
      "\t Loss: 0.5514\n",
      "Epoch [1/1]\n",
      "\tBatch [461]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [462]:\n",
      "\t Loss: 0.5836\n",
      "Epoch [1/1]\n",
      "\tBatch [463]:\n",
      "\t Loss: 0.6282\n",
      "Epoch [1/1]\n",
      "\tBatch [464]:\n",
      "\t Loss: 0.6348\n",
      "Epoch [1/1]\n",
      "\tBatch [465]:\n",
      "\t Loss: 0.6323\n",
      "Epoch [1/1]\n",
      "\tBatch [466]:\n",
      "\t Loss: 0.5837\n",
      "Epoch [1/1]\n",
      "\tBatch [467]:\n",
      "\t Loss: 0.5397\n",
      "Epoch [1/1]\n",
      "\tBatch [468]:\n",
      "\t Loss: 0.5859\n",
      "Epoch [1/1]\n",
      "\tBatch [469]:\n",
      "\t Loss: 0.5857\n",
      "Epoch [1/1]\n",
      "\tBatch [470]:\n",
      "\t Loss: 0.5689\n",
      "Epoch [1/1]\n",
      "\tBatch [471]:\n",
      "\t Loss: 0.5842\n",
      "Epoch [1/1]\n",
      "\tBatch [472]:\n",
      "\t Loss: 0.4833\n",
      "Epoch [1/1]\n",
      "\tBatch [473]:\n",
      "\t Loss: 0.5932\n",
      "Epoch [1/1]\n",
      "\tBatch [474]:\n",
      "\t Loss: 0.4927\n",
      "Epoch [1/1]\n",
      "\tBatch [475]:\n",
      "\t Loss: 0.5293\n",
      "Epoch [1/1]\n",
      "\tBatch [476]:\n",
      "\t Loss: 0.5511\n",
      "Epoch [1/1]\n",
      "\tBatch [477]:\n",
      "\t Loss: 0.5920\n",
      "Epoch [1/1]\n",
      "\tBatch [478]:\n",
      "\t Loss: 0.4877\n",
      "Epoch [1/1]\n",
      "\tBatch [479]:\n",
      "\t Loss: 0.5130\n",
      "Epoch [1/1]\n",
      "\tBatch [480]:\n",
      "\t Loss: 0.6406\n",
      "Epoch [1/1]\n",
      "\tBatch [481]:\n",
      "\t Loss: 0.6131\n",
      "Epoch [1/1]\n",
      "\tBatch [482]:\n",
      "\t Loss: 0.6372\n",
      "Epoch [1/1]\n",
      "\tBatch [483]:\n",
      "\t Loss: 0.6078\n",
      "Epoch [1/1]\n",
      "\tBatch [484]:\n",
      "\t Loss: 0.5146\n",
      "Epoch [1/1]\n",
      "\tBatch [485]:\n",
      "\t Loss: 0.5270\n",
      "Epoch [1/1]\n",
      "\tBatch [486]:\n",
      "\t Loss: 0.5961\n",
      "Epoch [1/1]\n",
      "\tBatch [487]:\n",
      "\t Loss: 0.5623\n",
      "Epoch [1/1]\n",
      "\tBatch [488]:\n",
      "\t Loss: 0.4969\n",
      "Epoch [1/1]\n",
      "\tBatch [489]:\n",
      "\t Loss: 0.5832\n",
      "Epoch [1/1]\n",
      "\tBatch [490]:\n",
      "\t Loss: 0.6223\n",
      "Epoch [1/1]\n",
      "\tBatch [491]:\n",
      "\t Loss: 0.5641\n",
      "Epoch [1/1]\n",
      "\tBatch [492]:\n",
      "\t Loss: 0.5933\n",
      "Epoch [1/1]\n",
      "\tBatch [493]:\n",
      "\t Loss: 0.5030\n",
      "Epoch [1/1]\n",
      "\tBatch [494]:\n",
      "\t Loss: 0.5465\n",
      "Epoch [1/1]\n",
      "\tBatch [495]:\n",
      "\t Loss: 0.5643\n",
      "Epoch [1/1]\n",
      "\tBatch [496]:\n",
      "\t Loss: 0.5734\n",
      "Epoch [1/1]\n",
      "\tBatch [497]:\n",
      "\t Loss: 0.5729\n",
      "Epoch [1/1]\n",
      "\tBatch [498]:\n",
      "\t Loss: 0.5848\n",
      "Epoch [1/1]\n",
      "\tBatch [499]:\n",
      "\t Loss: 0.6388\n",
      "Epoch [1/1]\n",
      "\tBatch [500]:\n",
      "\t Loss: 0.6377\n",
      "Epoch [1/1]\n",
      "\tBatch [501]:\n",
      "\t Loss: 0.5948\n",
      "Epoch [1/1]\n",
      "\tBatch [502]:\n",
      "\t Loss: 0.5313\n",
      "Epoch [1/1]\n",
      "\tBatch [503]:\n",
      "\t Loss: 0.5523\n",
      "Epoch [1/1]\n",
      "\tBatch [504]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [505]:\n",
      "\t Loss: 0.5739\n",
      "Epoch [1/1]\n",
      "\tBatch [506]:\n",
      "\t Loss: 0.5306\n",
      "Epoch [1/1]\n",
      "\tBatch [507]:\n",
      "\t Loss: 0.5729\n",
      "Epoch [1/1]\n",
      "\tBatch [508]:\n",
      "\t Loss: 0.5408\n",
      "Epoch [1/1]\n",
      "\tBatch [509]:\n",
      "\t Loss: 0.6378\n",
      "Epoch [1/1]\n",
      "\tBatch [510]:\n",
      "\t Loss: 0.5191\n",
      "Epoch [1/1]\n",
      "\tBatch [511]:\n",
      "\t Loss: 0.5291\n",
      "Epoch [1/1]\n",
      "\tBatch [512]:\n",
      "\t Loss: 0.5186\n",
      "Epoch [1/1]\n",
      "\tBatch [513]:\n",
      "\t Loss: 0.5874\n",
      "Epoch [1/1]\n",
      "\tBatch [514]:\n",
      "\t Loss: 0.5526\n",
      "Epoch [1/1]\n",
      "\tBatch [515]:\n",
      "\t Loss: 0.5638\n",
      "Epoch [1/1]\n",
      "\tBatch [516]:\n",
      "\t Loss: 0.5399\n",
      "Epoch [1/1]\n",
      "\tBatch [517]:\n",
      "\t Loss: 0.5395\n",
      "Epoch [1/1]\n",
      "\tBatch [518]:\n",
      "\t Loss: 0.5027\n",
      "Epoch [1/1]\n",
      "\tBatch [519]:\n",
      "\t Loss: 0.6235\n",
      "Epoch [1/1]\n",
      "\tBatch [520]:\n",
      "\t Loss: 0.6713\n",
      "Epoch [1/1]\n",
      "\tBatch [521]:\n",
      "\t Loss: 0.5620\n",
      "Epoch [1/1]\n",
      "\tBatch [522]:\n",
      "\t Loss: 0.6186\n",
      "Epoch [1/1]\n",
      "\tBatch [523]:\n",
      "\t Loss: 0.6293\n",
      "Epoch [1/1]\n",
      "\tBatch [524]:\n",
      "\t Loss: 0.5122\n",
      "Epoch [1/1]\n",
      "\tBatch [525]:\n",
      "\t Loss: 0.5832\n",
      "Epoch [1/1]\n",
      "\tBatch [526]:\n",
      "\t Loss: 0.5454\n",
      "Epoch [1/1]\n",
      "\tBatch [527]:\n",
      "\t Loss: 0.5836\n",
      "Epoch [1/1]\n",
      "\tBatch [528]:\n",
      "\t Loss: 0.6116\n",
      "Epoch [1/1]\n",
      "\tBatch [529]:\n",
      "\t Loss: 0.5479\n",
      "Epoch [1/1]\n",
      "\tBatch [530]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [531]:\n",
      "\t Loss: 0.5932\n",
      "Epoch [1/1]\n",
      "\tBatch [532]:\n",
      "\t Loss: 0.6773\n",
      "Epoch [1/1]\n",
      "\tBatch [533]:\n",
      "\t Loss: 0.5831\n",
      "Epoch [1/1]\n",
      "\tBatch [534]:\n",
      "\t Loss: 0.5828\n",
      "Epoch [1/1]\n",
      "\tBatch [535]:\n",
      "\t Loss: 0.5226\n",
      "Epoch [1/1]\n",
      "\tBatch [536]:\n",
      "\t Loss: 0.5295\n",
      "Epoch [1/1]\n",
      "\tBatch [537]:\n",
      "\t Loss: 0.5643\n",
      "Epoch [1/1]\n",
      "\tBatch [538]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [539]:\n",
      "\t Loss: 0.6144\n",
      "Epoch [1/1]\n",
      "\tBatch [540]:\n",
      "\t Loss: 0.5732\n",
      "Epoch [1/1]\n",
      "\tBatch [541]:\n",
      "\t Loss: 0.5947\n",
      "Epoch [1/1]\n",
      "\tBatch [542]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [543]:\n",
      "\t Loss: 0.6178\n",
      "Epoch [1/1]\n",
      "\tBatch [544]:\n",
      "\t Loss: 0.5966\n",
      "Epoch [1/1]\n",
      "\tBatch [545]:\n",
      "\t Loss: 0.5077\n",
      "Epoch [1/1]\n",
      "\tBatch [546]:\n",
      "\t Loss: 0.4853\n",
      "Epoch [1/1]\n",
      "\tBatch [547]:\n",
      "\t Loss: 0.6179\n",
      "Epoch [1/1]\n",
      "\tBatch [548]:\n",
      "\t Loss: 0.5285\n",
      "Epoch [1/1]\n",
      "\tBatch [549]:\n",
      "\t Loss: 0.6881\n",
      "Epoch [1/1]\n",
      "\tBatch [550]:\n",
      "\t Loss: 0.5393\n",
      "Epoch [1/1]\n",
      "\tBatch [551]:\n",
      "\t Loss: 0.6165\n",
      "Epoch [1/1]\n",
      "\tBatch [552]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [553]:\n",
      "\t Loss: 0.5109\n",
      "Epoch [1/1]\n",
      "\tBatch [554]:\n",
      "\t Loss: 0.5838\n",
      "Epoch [1/1]\n",
      "\tBatch [555]:\n",
      "\t Loss: 0.5509\n",
      "Epoch [1/1]\n",
      "\tBatch [556]:\n",
      "\t Loss: 0.6970\n",
      "Epoch [1/1]\n",
      "\tBatch [557]:\n",
      "\t Loss: 0.6326\n",
      "Epoch [1/1]\n",
      "\tBatch [558]:\n",
      "\t Loss: 0.6021\n",
      "Epoch [1/1]\n",
      "\tBatch [559]:\n",
      "\t Loss: 0.5093\n",
      "Epoch [1/1]\n",
      "\tBatch [560]:\n",
      "\t Loss: 0.5752\n",
      "Epoch [1/1]\n",
      "\tBatch [561]:\n",
      "\t Loss: 0.5666\n",
      "Epoch [1/1]\n",
      "\tBatch [562]:\n",
      "\t Loss: 0.6476\n",
      "Epoch [1/1]\n",
      "\tBatch [563]:\n",
      "\t Loss: 0.5466\n",
      "Epoch [1/1]\n",
      "\tBatch [564]:\n",
      "\t Loss: 0.6292\n",
      "Epoch [1/1]\n",
      "\tBatch [565]:\n",
      "\t Loss: 0.5553\n",
      "Epoch [1/1]\n",
      "\tBatch [566]:\n",
      "\t Loss: 0.5469\n",
      "Epoch [1/1]\n",
      "\tBatch [567]:\n",
      "\t Loss: 0.5368\n",
      "Epoch [1/1]\n",
      "\tBatch [568]:\n",
      "\t Loss: 0.5341\n",
      "Epoch [1/1]\n",
      "\tBatch [569]:\n",
      "\t Loss: 0.6251\n",
      "Epoch [1/1]\n",
      "\tBatch [570]:\n",
      "\t Loss: 0.6161\n",
      "Epoch [1/1]\n",
      "\tBatch [571]:\n",
      "\t Loss: 0.6613\n",
      "Epoch [1/1]\n",
      "\tBatch [572]:\n",
      "\t Loss: 0.5955\n",
      "Epoch [1/1]\n",
      "\tBatch [573]:\n",
      "\t Loss: 0.5837\n",
      "Epoch [1/1]\n",
      "\tBatch [574]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [575]:\n",
      "\t Loss: 0.5633\n",
      "Epoch [1/1]\n",
      "\tBatch [576]:\n",
      "\t Loss: 0.6037\n",
      "Epoch [1/1]\n",
      "\tBatch [577]:\n",
      "\t Loss: 0.5529\n",
      "Epoch [1/1]\n",
      "\tBatch [578]:\n",
      "\t Loss: 0.5445\n",
      "Epoch [1/1]\n",
      "\tBatch [579]:\n",
      "\t Loss: 0.5527\n",
      "Epoch [1/1]\n",
      "\tBatch [580]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [581]:\n",
      "\t Loss: 0.5331\n",
      "Epoch [1/1]\n",
      "\tBatch [582]:\n",
      "\t Loss: 0.5940\n",
      "Epoch [1/1]\n",
      "\tBatch [583]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [584]:\n",
      "\t Loss: 0.5625\n",
      "Epoch [1/1]\n",
      "\tBatch [585]:\n",
      "\t Loss: 0.5163\n",
      "Epoch [1/1]\n",
      "\tBatch [586]:\n",
      "\t Loss: 0.6083\n",
      "Epoch [1/1]\n",
      "\tBatch [587]:\n",
      "\t Loss: 0.5508\n",
      "Epoch [1/1]\n",
      "\tBatch [588]:\n",
      "\t Loss: 0.5390\n",
      "Epoch [1/1]\n",
      "\tBatch [589]:\n",
      "\t Loss: 0.5394\n",
      "Epoch [1/1]\n",
      "\tBatch [590]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [591]:\n",
      "\t Loss: 0.5628\n",
      "Epoch [1/1]\n",
      "\tBatch [592]:\n",
      "\t Loss: 0.5270\n",
      "Epoch [1/1]\n",
      "\tBatch [593]:\n",
      "\t Loss: 0.5642\n",
      "Epoch [1/1]\n",
      "\tBatch [594]:\n",
      "\t Loss: 0.6118\n",
      "Epoch [1/1]\n",
      "\tBatch [595]:\n",
      "\t Loss: 0.6110\n",
      "Epoch [1/1]\n",
      "\tBatch [596]:\n",
      "\t Loss: 0.6206\n",
      "Epoch [1/1]\n",
      "\tBatch [597]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [598]:\n",
      "\t Loss: 0.5621\n",
      "Epoch [1/1]\n",
      "\tBatch [599]:\n",
      "\t Loss: 0.6155\n",
      "Epoch [1/1]\n",
      "\tBatch [600]:\n",
      "\t Loss: 0.5239\n",
      "Epoch [1/1]\n",
      "\tBatch [601]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [602]:\n",
      "\t Loss: 0.5559\n",
      "Epoch [1/1]\n",
      "\tBatch [603]:\n",
      "\t Loss: 0.5556\n",
      "Epoch [1/1]\n",
      "\tBatch [604]:\n",
      "\t Loss: 0.5279\n",
      "Epoch [1/1]\n",
      "\tBatch [605]:\n",
      "\t Loss: 0.5836\n",
      "Epoch [1/1]\n",
      "\tBatch [606]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [607]:\n",
      "\t Loss: 0.4634\n",
      "Epoch [1/1]\n",
      "\tBatch [608]:\n",
      "\t Loss: 0.5938\n",
      "Epoch [1/1]\n",
      "\tBatch [609]:\n",
      "\t Loss: 0.5947\n",
      "Epoch [1/1]\n",
      "\tBatch [610]:\n",
      "\t Loss: 0.5856\n",
      "Epoch [1/1]\n",
      "\tBatch [611]:\n",
      "\t Loss: 0.6536\n",
      "Epoch [1/1]\n",
      "\tBatch [612]:\n",
      "\t Loss: 0.6425\n",
      "Epoch [1/1]\n",
      "\tBatch [613]:\n",
      "\t Loss: 0.5619\n",
      "Epoch [1/1]\n",
      "\tBatch [614]:\n",
      "\t Loss: 0.5402\n",
      "Epoch [1/1]\n",
      "\tBatch [615]:\n",
      "\t Loss: 0.6464\n",
      "Epoch [1/1]\n",
      "\tBatch [616]:\n",
      "\t Loss: 0.6338\n",
      "Epoch [1/1]\n",
      "\tBatch [617]:\n",
      "\t Loss: 0.5259\n",
      "Epoch [1/1]\n",
      "\tBatch [618]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [619]:\n",
      "\t Loss: 0.5571\n",
      "Epoch [1/1]\n",
      "\tBatch [620]:\n",
      "\t Loss: 0.5931\n",
      "Epoch [1/1]\n",
      "\tBatch [621]:\n",
      "\t Loss: 0.5665\n",
      "Epoch [1/1]\n",
      "\tBatch [622]:\n",
      "\t Loss: 0.5935\n",
      "Epoch [1/1]\n",
      "\tBatch [623]:\n",
      "\t Loss: 0.5474\n",
      "Epoch [1/1]\n",
      "\tBatch [624]:\n",
      "\t Loss: 0.5743\n",
      "Epoch [1/1]\n",
      "\tBatch [625]:\n",
      "\t Loss: 0.5251\n",
      "Epoch [1/1]\n",
      "\tBatch [626]:\n",
      "\t Loss: 0.5325\n",
      "Epoch [1/1]\n",
      "\tBatch [627]:\n",
      "\t Loss: 0.5626\n",
      "Epoch [1/1]\n",
      "\tBatch [628]:\n",
      "\t Loss: 0.6179\n",
      "Epoch [1/1]\n",
      "\tBatch [629]:\n",
      "\t Loss: 0.5741\n",
      "Epoch [1/1]\n",
      "\tBatch [630]:\n",
      "\t Loss: 0.5742\n",
      "Epoch [1/1]\n",
      "\tBatch [631]:\n",
      "\t Loss: 0.5975\n",
      "Epoch [1/1]\n",
      "\tBatch [632]:\n",
      "\t Loss: 0.5858\n",
      "Epoch [1/1]\n",
      "\tBatch [633]:\n",
      "\t Loss: 0.5874\n",
      "Epoch [1/1]\n",
      "\tBatch [634]:\n",
      "\t Loss: 0.5510\n",
      "Epoch [1/1]\n",
      "\tBatch [635]:\n",
      "\t Loss: 0.5399\n",
      "Epoch [1/1]\n",
      "\tBatch [636]:\n",
      "\t Loss: 0.5510\n",
      "Epoch [1/1]\n",
      "\tBatch [637]:\n",
      "\t Loss: 0.6177\n",
      "Epoch [1/1]\n",
      "\tBatch [638]:\n",
      "\t Loss: 0.4865\n",
      "Epoch [1/1]\n",
      "\tBatch [639]:\n",
      "\t Loss: 0.5178\n",
      "Epoch [1/1]\n",
      "\tBatch [640]:\n",
      "\t Loss: 0.6291\n",
      "Epoch [1/1]\n",
      "\tBatch [641]:\n",
      "\t Loss: 0.6183\n",
      "Epoch [1/1]\n",
      "\tBatch [642]:\n",
      "\t Loss: 0.5843\n",
      "Epoch [1/1]\n",
      "\tBatch [643]:\n",
      "\t Loss: 0.6258\n",
      "Epoch [1/1]\n",
      "\tBatch [644]:\n",
      "\t Loss: 0.6052\n",
      "Epoch [1/1]\n",
      "\tBatch [645]:\n",
      "\t Loss: 0.6707\n",
      "Epoch [1/1]\n",
      "\tBatch [646]:\n",
      "\t Loss: 0.6390\n",
      "Epoch [1/1]\n",
      "\tBatch [647]:\n",
      "\t Loss: 0.5945\n",
      "Epoch [1/1]\n",
      "\tBatch [648]:\n",
      "\t Loss: 0.6598\n",
      "Epoch [1/1]\n",
      "\tBatch [649]:\n",
      "\t Loss: 0.6117\n",
      "Epoch [1/1]\n",
      "\tBatch [650]:\n",
      "\t Loss: 0.5588\n",
      "Epoch [1/1]\n",
      "\tBatch [651]:\n",
      "\t Loss: 0.5604\n",
      "Epoch [1/1]\n",
      "\tBatch [652]:\n",
      "\t Loss: 0.5782\n",
      "Epoch [1/1]\n",
      "\tBatch [653]:\n",
      "\t Loss: 0.5675\n",
      "Epoch [1/1]\n",
      "\tBatch [654]:\n",
      "\t Loss: 0.5948\n",
      "Epoch [1/1]\n",
      "\tBatch [655]:\n",
      "\t Loss: 0.5674\n",
      "Epoch [1/1]\n",
      "\tBatch [656]:\n",
      "\t Loss: 0.5366\n",
      "Epoch [1/1]\n",
      "\tBatch [657]:\n",
      "\t Loss: 0.4824\n",
      "Epoch [1/1]\n",
      "\tBatch [658]:\n",
      "\t Loss: 0.5307\n",
      "Epoch [1/1]\n",
      "\tBatch [659]:\n",
      "\t Loss: 0.6105\n",
      "Epoch [1/1]\n",
      "\tBatch [660]:\n",
      "\t Loss: 0.6160\n",
      "Epoch [1/1]\n",
      "\tBatch [661]:\n",
      "\t Loss: 0.6186\n",
      "Epoch [1/1]\n",
      "\tBatch [662]:\n",
      "\t Loss: 0.6337\n",
      "Epoch [1/1]\n",
      "\tBatch [663]:\n",
      "\t Loss: 0.5133\n",
      "Epoch [1/1]\n",
      "\tBatch [664]:\n",
      "\t Loss: 0.3959\n",
      "Epoch [1/1]\n",
      "\tBatch [665]:\n",
      "\t Loss: 0.6327\n",
      "Epoch [1/1]\n",
      "\tBatch [666]:\n",
      "\t Loss: 0.6729\n",
      "Epoch [1/1]\n",
      "\tBatch [667]:\n",
      "\t Loss: 0.5903\n",
      "Epoch [1/1]\n",
      "\tBatch [668]:\n",
      "\t Loss: 0.5510\n",
      "Epoch [1/1]\n",
      "\tBatch [669]:\n",
      "\t Loss: 0.4804\n",
      "Epoch [1/1]\n",
      "\tBatch [670]:\n",
      "\t Loss: 0.5975\n",
      "Epoch [1/1]\n",
      "\tBatch [671]:\n",
      "\t Loss: 0.6517\n",
      "Epoch [1/1]\n",
      "\tBatch [672]:\n",
      "\t Loss: 0.5947\n",
      "Epoch [1/1]\n",
      "\tBatch [673]:\n",
      "\t Loss: 0.5526\n",
      "Epoch [1/1]\n",
      "\tBatch [674]:\n",
      "\t Loss: 0.5542\n",
      "Epoch [1/1]\n",
      "\tBatch [675]:\n",
      "\t Loss: 0.5644\n",
      "Epoch [1/1]\n",
      "\tBatch [676]:\n",
      "\t Loss: 0.5470\n",
      "Epoch [1/1]\n",
      "\tBatch [677]:\n",
      "\t Loss: 0.5377\n",
      "Epoch [1/1]\n",
      "\tBatch [678]:\n",
      "\t Loss: 0.6205\n",
      "Epoch [1/1]\n",
      "\tBatch [679]:\n",
      "\t Loss: 0.5457\n",
      "Epoch [1/1]\n",
      "\tBatch [680]:\n",
      "\t Loss: 0.5645\n",
      "Epoch [1/1]\n",
      "\tBatch [681]:\n",
      "\t Loss: 0.5147\n",
      "Epoch [1/1]\n",
      "\tBatch [682]:\n",
      "\t Loss: 0.5216\n",
      "Epoch [1/1]\n",
      "\tBatch [683]:\n",
      "\t Loss: 0.5521\n",
      "Epoch [1/1]\n",
      "\tBatch [684]:\n",
      "\t Loss: 0.5518\n",
      "Epoch [1/1]\n",
      "\tBatch [685]:\n",
      "\t Loss: 0.5049\n",
      "Epoch [1/1]\n",
      "\tBatch [686]:\n",
      "\t Loss: 0.6001\n",
      "Epoch [1/1]\n",
      "\tBatch [687]:\n",
      "\t Loss: 0.6262\n",
      "Epoch [1/1]\n",
      "\tBatch [688]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [689]:\n",
      "\t Loss: 0.5020\n",
      "Epoch [1/1]\n",
      "\tBatch [690]:\n",
      "\t Loss: 0.6153\n",
      "Epoch [1/1]\n",
      "\tBatch [691]:\n",
      "\t Loss: 0.5393\n",
      "Epoch [1/1]\n",
      "\tBatch [692]:\n",
      "\t Loss: 0.5631\n",
      "Epoch [1/1]\n",
      "\tBatch [693]:\n",
      "\t Loss: 0.5139\n",
      "Epoch [1/1]\n",
      "\tBatch [694]:\n",
      "\t Loss: 0.6124\n",
      "Epoch [1/1]\n",
      "\tBatch [695]:\n",
      "\t Loss: 0.5760\n",
      "Epoch [1/1]\n",
      "\tBatch [696]:\n",
      "\t Loss: 0.5858\n",
      "Epoch [1/1]\n",
      "\tBatch [697]:\n",
      "\t Loss: 0.5056\n",
      "Epoch [1/1]\n",
      "\tBatch [698]:\n",
      "\t Loss: 0.6061\n",
      "Epoch [1/1]\n",
      "\tBatch [699]:\n",
      "\t Loss: 0.5072\n",
      "Epoch [1/1]\n",
      "\tBatch [700]:\n",
      "\t Loss: 0.5732\n",
      "Epoch [1/1]\n",
      "\tBatch [701]:\n",
      "\t Loss: 0.5723\n",
      "Epoch [1/1]\n",
      "\tBatch [702]:\n",
      "\t Loss: 0.5725\n",
      "Epoch [1/1]\n",
      "\tBatch [703]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [704]:\n",
      "\t Loss: 0.5524\n",
      "Epoch [1/1]\n",
      "\tBatch [705]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [706]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [707]:\n",
      "\t Loss: 0.5523\n",
      "Epoch [1/1]\n",
      "\tBatch [708]:\n",
      "\t Loss: 0.5314\n",
      "Epoch [1/1]\n",
      "\tBatch [709]:\n",
      "\t Loss: 0.5212\n",
      "Epoch [1/1]\n",
      "\tBatch [710]:\n",
      "\t Loss: 0.6366\n",
      "Epoch [1/1]\n",
      "\tBatch [711]:\n",
      "\t Loss: 0.6368\n",
      "Epoch [1/1]\n",
      "\tBatch [712]:\n",
      "\t Loss: 0.5411\n",
      "Epoch [1/1]\n",
      "\tBatch [713]:\n",
      "\t Loss: 0.5421\n",
      "Epoch [1/1]\n",
      "\tBatch [714]:\n",
      "\t Loss: 0.5409\n",
      "Epoch [1/1]\n",
      "\tBatch [715]:\n",
      "\t Loss: 0.5946\n",
      "Epoch [1/1]\n",
      "\tBatch [716]:\n",
      "\t Loss: 0.5402\n",
      "Epoch [1/1]\n",
      "\tBatch [717]:\n",
      "\t Loss: 0.5066\n",
      "Epoch [1/1]\n",
      "\tBatch [718]:\n",
      "\t Loss: 0.5397\n",
      "Epoch [1/1]\n",
      "\tBatch [719]:\n",
      "\t Loss: 0.5857\n",
      "Epoch [1/1]\n",
      "\tBatch [720]:\n",
      "\t Loss: 0.5056\n",
      "Epoch [1/1]\n",
      "\tBatch [721]:\n",
      "\t Loss: 0.6565\n",
      "Epoch [1/1]\n",
      "\tBatch [722]:\n",
      "\t Loss: 0.5283\n",
      "Epoch [1/1]\n",
      "\tBatch [723]:\n",
      "\t Loss: 0.5052\n",
      "Epoch [1/1]\n",
      "\tBatch [724]:\n",
      "\t Loss: 0.5748\n",
      "Epoch [1/1]\n",
      "\tBatch [725]:\n",
      "\t Loss: 0.6106\n",
      "Epoch [1/1]\n",
      "\tBatch [726]:\n",
      "\t Loss: 0.6211\n",
      "Epoch [1/1]\n",
      "\tBatch [727]:\n",
      "\t Loss: 0.5172\n",
      "Epoch [1/1]\n",
      "\tBatch [728]:\n",
      "\t Loss: 0.6197\n",
      "Epoch [1/1]\n",
      "\tBatch [729]:\n",
      "\t Loss: 0.4743\n",
      "Epoch [1/1]\n",
      "\tBatch [730]:\n",
      "\t Loss: 0.6285\n",
      "Epoch [1/1]\n",
      "\tBatch [731]:\n",
      "\t Loss: 0.5192\n",
      "Epoch [1/1]\n",
      "\tBatch [732]:\n",
      "\t Loss: 0.5303\n",
      "Epoch [1/1]\n",
      "\tBatch [733]:\n",
      "\t Loss: 0.6276\n",
      "Epoch [1/1]\n",
      "\tBatch [734]:\n",
      "\t Loss: 0.6478\n",
      "Epoch [1/1]\n",
      "\tBatch [735]:\n",
      "\t Loss: 0.6262\n",
      "Epoch [1/1]\n",
      "\tBatch [736]:\n",
      "\t Loss: 0.5941\n",
      "Epoch [1/1]\n",
      "\tBatch [737]:\n",
      "\t Loss: 0.5828\n",
      "Epoch [1/1]\n",
      "\tBatch [738]:\n",
      "\t Loss: 0.5278\n",
      "Epoch [1/1]\n",
      "\tBatch [739]:\n",
      "\t Loss: 0.6296\n",
      "Epoch [1/1]\n",
      "\tBatch [740]:\n",
      "\t Loss: 0.6285\n",
      "Epoch [1/1]\n",
      "\tBatch [741]:\n",
      "\t Loss: 0.6097\n",
      "Epoch [1/1]\n",
      "\tBatch [742]:\n",
      "\t Loss: 0.5763\n",
      "Epoch [1/1]\n",
      "\tBatch [743]:\n",
      "\t Loss: 0.5598\n",
      "Epoch [1/1]\n",
      "\tBatch [744]:\n",
      "\t Loss: 0.5607\n",
      "Epoch [1/1]\n",
      "\tBatch [745]:\n",
      "\t Loss: 0.5498\n",
      "Epoch [1/1]\n",
      "\tBatch [746]:\n",
      "\t Loss: 0.5479\n",
      "Epoch [1/1]\n",
      "\tBatch [747]:\n",
      "\t Loss: 0.6396\n",
      "Epoch [1/1]\n",
      "\tBatch [748]:\n",
      "\t Loss: 0.5154\n",
      "Epoch [1/1]\n",
      "\tBatch [749]:\n",
      "\t Loss: 0.5936\n",
      "Epoch [1/1]\n",
      "\tBatch [750]:\n",
      "\t Loss: 0.5102\n",
      "Epoch [1/1]\n",
      "\tBatch [751]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [752]:\n",
      "\t Loss: 0.5627\n",
      "Epoch [1/1]\n",
      "\tBatch [753]:\n",
      "\t Loss: 0.5400\n",
      "Epoch [1/1]\n",
      "\tBatch [754]:\n",
      "\t Loss: 0.4906\n",
      "Epoch [1/1]\n",
      "\tBatch [755]:\n",
      "\t Loss: 0.5265\n",
      "Epoch [1/1]\n",
      "\tBatch [756]:\n",
      "\t Loss: 0.6557\n",
      "Epoch [1/1]\n",
      "\tBatch [757]:\n",
      "\t Loss: 0.6456\n",
      "Epoch [1/1]\n",
      "\tBatch [758]:\n",
      "\t Loss: 0.6169\n",
      "Epoch [1/1]\n",
      "\tBatch [759]:\n",
      "\t Loss: 0.5506\n",
      "Epoch [1/1]\n",
      "\tBatch [760]:\n",
      "\t Loss: 0.5625\n",
      "Epoch [1/1]\n",
      "\tBatch [761]:\n",
      "\t Loss: 0.5750\n",
      "Epoch [1/1]\n",
      "\tBatch [762]:\n",
      "\t Loss: 0.5399\n",
      "Epoch [1/1]\n",
      "\tBatch [763]:\n",
      "\t Loss: 0.4947\n",
      "Epoch [1/1]\n",
      "\tBatch [764]:\n",
      "\t Loss: 0.6171\n",
      "Epoch [1/1]\n",
      "\tBatch [765]:\n",
      "\t Loss: 0.5184\n",
      "Epoch [1/1]\n",
      "\tBatch [766]:\n",
      "\t Loss: 0.4885\n",
      "Epoch [1/1]\n",
      "\tBatch [767]:\n",
      "\t Loss: 0.5623\n",
      "Epoch [1/1]\n",
      "\tBatch [768]:\n",
      "\t Loss: 0.5193\n",
      "Epoch [1/1]\n",
      "\tBatch [769]:\n",
      "\t Loss: 0.4842\n",
      "Epoch [1/1]\n",
      "\tBatch [770]:\n",
      "\t Loss: 0.4832\n",
      "Epoch [1/1]\n",
      "\tBatch [771]:\n",
      "\t Loss: 0.5976\n",
      "Epoch [1/1]\n",
      "\tBatch [772]:\n",
      "\t Loss: 0.5034\n",
      "Epoch [1/1]\n",
      "\tBatch [773]:\n",
      "\t Loss: 0.5142\n",
      "Epoch [1/1]\n",
      "\tBatch [774]:\n",
      "\t Loss: 0.5133\n",
      "Epoch [1/1]\n",
      "\tBatch [775]:\n",
      "\t Loss: 0.5398\n",
      "Epoch [1/1]\n",
      "\tBatch [776]:\n",
      "\t Loss: 0.5667\n",
      "Epoch [1/1]\n",
      "\tBatch [777]:\n",
      "\t Loss: 0.6077\n",
      "Epoch [1/1]\n",
      "\tBatch [778]:\n",
      "\t Loss: 0.5934\n",
      "Epoch [1/1]\n",
      "\tBatch [779]:\n",
      "\t Loss: 0.5406\n",
      "Epoch [1/1]\n",
      "\tBatch [780]:\n",
      "\t Loss: 0.6183\n",
      "Epoch [1/1]\n",
      "\tBatch [781]:\n",
      "\t Loss: 0.5266\n",
      "Epoch [1/1]\n",
      "\tBatch [782]:\n",
      "\t Loss: 0.6004\n",
      "Epoch [1/1]\n",
      "\tBatch [783]:\n",
      "\t Loss: 0.5155\n",
      "Epoch [1/1]\n",
      "\tBatch [784]:\n",
      "\t Loss: 0.5393\n",
      "Epoch [1/1]\n",
      "\tBatch [785]:\n",
      "\t Loss: 0.4728\n",
      "Epoch [1/1]\n",
      "\tBatch [786]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [787]:\n",
      "\t Loss: 0.5960\n",
      "Epoch [1/1]\n",
      "\tBatch [788]:\n",
      "\t Loss: 0.5400\n",
      "Epoch [1/1]\n",
      "\tBatch [789]:\n",
      "\t Loss: 0.5517\n",
      "Epoch [1/1]\n",
      "\tBatch [790]:\n",
      "\t Loss: 0.5738\n",
      "Epoch [1/1]\n",
      "\tBatch [791]:\n",
      "\t Loss: 0.5406\n",
      "Epoch [1/1]\n",
      "\tBatch [792]:\n",
      "\t Loss: 0.5518\n",
      "Epoch [1/1]\n",
      "\tBatch [793]:\n",
      "\t Loss: 0.5721\n",
      "Epoch [1/1]\n",
      "\tBatch [794]:\n",
      "\t Loss: 0.5188\n",
      "Epoch [1/1]\n",
      "\tBatch [795]:\n",
      "\t Loss: 0.5402\n",
      "Epoch [1/1]\n",
      "\tBatch [796]:\n",
      "\t Loss: 0.5506\n",
      "Epoch [1/1]\n",
      "\tBatch [797]:\n",
      "\t Loss: 0.5160\n",
      "Epoch [1/1]\n",
      "\tBatch [798]:\n",
      "\t Loss: 0.4957\n",
      "Epoch [1/1]\n",
      "\tBatch [799]:\n",
      "\t Loss: 0.5866\n",
      "Epoch [1/1]\n",
      "\tBatch [800]:\n",
      "\t Loss: 0.5380\n",
      "Epoch [1/1]\n",
      "\tBatch [801]:\n",
      "\t Loss: 0.5762\n",
      "Epoch [1/1]\n",
      "\tBatch [802]:\n",
      "\t Loss: 0.6383\n",
      "Epoch [1/1]\n",
      "\tBatch [803]:\n",
      "\t Loss: 0.5395\n",
      "Epoch [1/1]\n",
      "\tBatch [804]:\n",
      "\t Loss: 0.5753\n",
      "Epoch [1/1]\n",
      "\tBatch [805]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [806]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [807]:\n",
      "\t Loss: 0.4934\n",
      "Epoch [1/1]\n",
      "\tBatch [808]:\n",
      "\t Loss: 0.5852\n",
      "Epoch [1/1]\n",
      "\tBatch [809]:\n",
      "\t Loss: 0.4604\n",
      "Epoch [1/1]\n",
      "\tBatch [810]:\n",
      "\t Loss: 0.5511\n",
      "Epoch [1/1]\n",
      "\tBatch [811]:\n",
      "\t Loss: 0.6093\n",
      "Epoch [1/1]\n",
      "\tBatch [812]:\n",
      "\t Loss: 0.6198\n",
      "Epoch [1/1]\n",
      "\tBatch [813]:\n",
      "\t Loss: 0.6070\n",
      "Epoch [1/1]\n",
      "\tBatch [814]:\n",
      "\t Loss: 0.5405\n",
      "Epoch [1/1]\n",
      "\tBatch [815]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [816]:\n",
      "\t Loss: 0.5304\n",
      "Epoch [1/1]\n",
      "\tBatch [817]:\n",
      "\t Loss: 0.5738\n",
      "Epoch [1/1]\n",
      "\tBatch [818]:\n",
      "\t Loss: 0.5620\n",
      "Epoch [1/1]\n",
      "\tBatch [819]:\n",
      "\t Loss: 0.6373\n",
      "Epoch [1/1]\n",
      "\tBatch [820]:\n",
      "\t Loss: 0.5724\n",
      "Epoch [1/1]\n",
      "\tBatch [821]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [822]:\n",
      "\t Loss: 0.5530\n",
      "Epoch [1/1]\n",
      "\tBatch [823]:\n",
      "\t Loss: 0.5423\n",
      "Epoch [1/1]\n",
      "\tBatch [824]:\n",
      "\t Loss: 0.6041\n",
      "Epoch [1/1]\n",
      "\tBatch [825]:\n",
      "\t Loss: 0.5939\n",
      "Epoch [1/1]\n",
      "\tBatch [826]:\n",
      "\t Loss: 0.5128\n",
      "Epoch [1/1]\n",
      "\tBatch [827]:\n",
      "\t Loss: 0.5333\n",
      "Epoch [1/1]\n",
      "\tBatch [828]:\n",
      "\t Loss: 0.5934\n",
      "Epoch [1/1]\n",
      "\tBatch [829]:\n",
      "\t Loss: 0.5415\n",
      "Epoch [1/1]\n",
      "\tBatch [830]:\n",
      "\t Loss: 0.6159\n",
      "Epoch [1/1]\n",
      "\tBatch [831]:\n",
      "\t Loss: 0.6163\n",
      "Epoch [1/1]\n",
      "\tBatch [832]:\n",
      "\t Loss: 0.5301\n",
      "Epoch [1/1]\n",
      "\tBatch [833]:\n",
      "\t Loss: 0.5084\n",
      "Epoch [1/1]\n",
      "\tBatch [834]:\n",
      "\t Loss: 0.5514\n",
      "Epoch [1/1]\n",
      "\tBatch [835]:\n",
      "\t Loss: 0.5509\n",
      "Epoch [1/1]\n",
      "\tBatch [836]:\n",
      "\t Loss: 0.5744\n",
      "Epoch [1/1]\n",
      "\tBatch [837]:\n",
      "\t Loss: 0.6645\n",
      "Epoch [1/1]\n",
      "\tBatch [838]:\n",
      "\t Loss: 0.5968\n",
      "Epoch [1/1]\n",
      "\tBatch [839]:\n",
      "\t Loss: 0.5072\n",
      "Epoch [1/1]\n",
      "\tBatch [840]:\n",
      "\t Loss: 0.5068\n",
      "Epoch [1/1]\n",
      "\tBatch [841]:\n",
      "\t Loss: 0.5960\n",
      "Epoch [1/1]\n",
      "\tBatch [842]:\n",
      "\t Loss: 0.4739\n",
      "Epoch [1/1]\n",
      "\tBatch [843]:\n",
      "\t Loss: 0.4611\n",
      "Epoch [1/1]\n",
      "\tBatch [844]:\n",
      "\t Loss: 0.5620\n",
      "Epoch [1/1]\n",
      "\tBatch [845]:\n",
      "\t Loss: 0.5397\n",
      "Epoch [1/1]\n",
      "\tBatch [846]:\n",
      "\t Loss: 0.4543\n",
      "Epoch [1/1]\n",
      "\tBatch [847]:\n",
      "\t Loss: 0.5388\n",
      "Epoch [1/1]\n",
      "\tBatch [848]:\n",
      "\t Loss: 0.5018\n",
      "Epoch [1/1]\n",
      "\tBatch [849]:\n",
      "\t Loss: 0.5401\n",
      "Epoch [1/1]\n",
      "\tBatch [850]:\n",
      "\t Loss: 0.5271\n",
      "Epoch [1/1]\n",
      "\tBatch [851]:\n",
      "\t Loss: 0.5416\n",
      "Epoch [1/1]\n",
      "\tBatch [852]:\n",
      "\t Loss: 0.5002\n",
      "Epoch [1/1]\n",
      "\tBatch [853]:\n",
      "\t Loss: 0.6670\n",
      "Epoch [1/1]\n",
      "\tBatch [854]:\n",
      "\t Loss: 0.4313\n",
      "Epoch [1/1]\n",
      "\tBatch [855]:\n",
      "\t Loss: 0.6384\n",
      "Epoch [1/1]\n",
      "\tBatch [856]:\n",
      "\t Loss: 0.4871\n",
      "Epoch [1/1]\n",
      "\tBatch [857]:\n",
      "\t Loss: 0.5000\n",
      "Epoch [1/1]\n",
      "\tBatch [858]:\n",
      "\t Loss: 0.6440\n",
      "Epoch [1/1]\n",
      "\tBatch [859]:\n",
      "\t Loss: 0.5143\n",
      "Epoch [1/1]\n",
      "\tBatch [860]:\n",
      "\t Loss: 0.5392\n",
      "Epoch [1/1]\n",
      "\tBatch [861]:\n",
      "\t Loss: 0.4913\n",
      "Epoch [1/1]\n",
      "\tBatch [862]:\n",
      "\t Loss: 0.5859\n",
      "Epoch [1/1]\n",
      "\tBatch [863]:\n",
      "\t Loss: 0.5981\n",
      "Epoch [1/1]\n",
      "\tBatch [864]:\n",
      "\t Loss: 0.6531\n",
      "Epoch [1/1]\n",
      "\tBatch [865]:\n",
      "\t Loss: 0.6389\n",
      "Epoch [1/1]\n",
      "\tBatch [866]:\n",
      "\t Loss: 0.6143\n",
      "Epoch [1/1]\n",
      "\tBatch [867]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [868]:\n",
      "\t Loss: 0.5650\n",
      "Epoch [1/1]\n",
      "\tBatch [869]:\n",
      "\t Loss: 0.6207\n",
      "Epoch [1/1]\n",
      "\tBatch [870]:\n",
      "\t Loss: 0.5945\n",
      "Epoch [1/1]\n",
      "\tBatch [871]:\n",
      "\t Loss: 0.6192\n",
      "Epoch [1/1]\n",
      "\tBatch [872]:\n",
      "\t Loss: 0.5634\n",
      "Epoch [1/1]\n",
      "\tBatch [873]:\n",
      "\t Loss: 0.5789\n",
      "Epoch [1/1]\n",
      "\tBatch [874]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [875]:\n",
      "\t Loss: 0.6190\n",
      "Epoch [1/1]\n",
      "\tBatch [876]:\n",
      "\t Loss: 0.5940\n",
      "Epoch [1/1]\n",
      "\tBatch [877]:\n",
      "\t Loss: 0.5760\n",
      "Epoch [1/1]\n",
      "\tBatch [878]:\n",
      "\t Loss: 0.5134\n",
      "Epoch [1/1]\n",
      "\tBatch [879]:\n",
      "\t Loss: 0.4993\n",
      "Epoch [1/1]\n",
      "\tBatch [880]:\n",
      "\t Loss: 0.5633\n",
      "Epoch [1/1]\n",
      "\tBatch [881]:\n",
      "\t Loss: 0.6072\n",
      "Epoch [1/1]\n",
      "\tBatch [882]:\n",
      "\t Loss: 0.5630\n",
      "Epoch [1/1]\n",
      "\tBatch [883]:\n",
      "\t Loss: 0.5732\n",
      "Epoch [1/1]\n",
      "\tBatch [884]:\n",
      "\t Loss: 0.5266\n",
      "Epoch [1/1]\n",
      "\tBatch [885]:\n",
      "\t Loss: 0.6496\n",
      "Epoch [1/1]\n",
      "\tBatch [886]:\n",
      "\t Loss: 0.6387\n",
      "Epoch [1/1]\n",
      "\tBatch [887]:\n",
      "\t Loss: 0.4778\n",
      "Epoch [1/1]\n",
      "\tBatch [888]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [889]:\n",
      "\t Loss: 0.4040\n",
      "Epoch [1/1]\n",
      "\tBatch [890]:\n",
      "\t Loss: 0.5767\n",
      "Epoch [1/1]\n",
      "\tBatch [891]:\n",
      "\t Loss: 0.5146\n",
      "Epoch [1/1]\n",
      "\tBatch [892]:\n",
      "\t Loss: 0.6043\n",
      "Epoch [1/1]\n",
      "\tBatch [893]:\n",
      "\t Loss: 0.5908\n",
      "Epoch [1/1]\n",
      "\tBatch [894]:\n",
      "\t Loss: 0.6151\n",
      "Epoch [1/1]\n",
      "\tBatch [895]:\n",
      "\t Loss: 0.6139\n",
      "Epoch [1/1]\n",
      "\tBatch [896]:\n",
      "\t Loss: 0.5157\n",
      "Epoch [1/1]\n",
      "\tBatch [897]:\n",
      "\t Loss: 0.5985\n",
      "Epoch [1/1]\n",
      "\tBatch [898]:\n",
      "\t Loss: 0.5852\n",
      "Epoch [1/1]\n",
      "\tBatch [899]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [900]:\n",
      "\t Loss: 0.4778\n",
      "Epoch [1/1]\n",
      "\tBatch [901]:\n",
      "\t Loss: 0.5837\n",
      "Epoch [1/1]\n",
      "\tBatch [902]:\n",
      "\t Loss: 0.6036\n",
      "Epoch [1/1]\n",
      "\tBatch [903]:\n",
      "\t Loss: 0.5630\n",
      "Epoch [1/1]\n",
      "\tBatch [904]:\n",
      "\t Loss: 0.5731\n",
      "Epoch [1/1]\n",
      "\tBatch [905]:\n",
      "\t Loss: 0.4825\n",
      "Epoch [1/1]\n",
      "\tBatch [906]:\n",
      "\t Loss: 0.4920\n",
      "Epoch [1/1]\n",
      "\tBatch [907]:\n",
      "\t Loss: 0.6154\n",
      "Epoch [1/1]\n",
      "\tBatch [908]:\n",
      "\t Loss: 0.6266\n",
      "Epoch [1/1]\n",
      "\tBatch [909]:\n",
      "\t Loss: 0.6479\n",
      "Epoch [1/1]\n",
      "\tBatch [910]:\n",
      "\t Loss: 0.6153\n",
      "Epoch [1/1]\n",
      "\tBatch [911]:\n",
      "\t Loss: 0.6147\n",
      "Epoch [1/1]\n",
      "\tBatch [912]:\n",
      "\t Loss: 0.5429\n",
      "Epoch [1/1]\n",
      "\tBatch [913]:\n",
      "\t Loss: 0.5833\n",
      "Epoch [1/1]\n",
      "\tBatch [914]:\n",
      "\t Loss: 0.6026\n",
      "Epoch [1/1]\n",
      "\tBatch [915]:\n",
      "\t Loss: 0.5739\n",
      "Epoch [1/1]\n",
      "\tBatch [916]:\n",
      "\t Loss: 0.5257\n",
      "Epoch [1/1]\n",
      "\tBatch [917]:\n",
      "\t Loss: 0.5930\n",
      "Epoch [1/1]\n",
      "\tBatch [918]:\n",
      "\t Loss: 0.5929\n",
      "Epoch [1/1]\n",
      "\tBatch [919]:\n",
      "\t Loss: 0.6896\n",
      "Epoch [1/1]\n",
      "\tBatch [920]:\n",
      "\t Loss: 0.5743\n",
      "Epoch [1/1]\n",
      "\tBatch [921]:\n",
      "\t Loss: 0.4993\n",
      "Epoch [1/1]\n",
      "\tBatch [922]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [923]:\n",
      "\t Loss: 0.6213\n",
      "Epoch [1/1]\n",
      "\tBatch [924]:\n",
      "\t Loss: 0.5355\n",
      "Epoch [1/1]\n",
      "\tBatch [925]:\n",
      "\t Loss: 0.5258\n",
      "Epoch [1/1]\n",
      "\tBatch [926]:\n",
      "\t Loss: 0.5243\n",
      "Epoch [1/1]\n",
      "\tBatch [927]:\n",
      "\t Loss: 0.6143\n",
      "Epoch [1/1]\n",
      "\tBatch [928]:\n",
      "\t Loss: 0.5312\n",
      "Epoch [1/1]\n",
      "\tBatch [929]:\n",
      "\t Loss: 0.5840\n",
      "Epoch [1/1]\n",
      "\tBatch [930]:\n",
      "\t Loss: 0.6404\n",
      "Epoch [1/1]\n",
      "\tBatch [931]:\n",
      "\t Loss: 0.6068\n",
      "Epoch [1/1]\n",
      "\tBatch [932]:\n",
      "\t Loss: 0.5737\n",
      "Epoch [1/1]\n",
      "\tBatch [933]:\n",
      "\t Loss: 0.5288\n",
      "Epoch [1/1]\n",
      "\tBatch [934]:\n",
      "\t Loss: 0.5620\n",
      "Epoch [1/1]\n",
      "\tBatch [935]:\n",
      "\t Loss: 0.5843\n",
      "Epoch [1/1]\n",
      "\tBatch [936]:\n",
      "\t Loss: 0.5734\n",
      "Epoch [1/1]\n",
      "\tBatch [937]:\n",
      "\t Loss: 0.6506\n",
      "Epoch [1/1]\n",
      "\tBatch [938]:\n",
      "\t Loss: 0.6385\n",
      "Epoch [1/1]\n",
      "\tBatch [939]:\n",
      "\t Loss: 0.6151\n",
      "Epoch [1/1]\n",
      "\tBatch [940]:\n",
      "\t Loss: 0.6130\n",
      "Epoch [1/1]\n",
      "\tBatch [941]:\n",
      "\t Loss: 0.5836\n",
      "Epoch [1/1]\n",
      "\tBatch [942]:\n",
      "\t Loss: 0.6024\n",
      "Epoch [1/1]\n",
      "\tBatch [943]:\n",
      "\t Loss: 0.6197\n",
      "Epoch [1/1]\n",
      "\tBatch [944]:\n",
      "\t Loss: 0.5593\n",
      "Epoch [1/1]\n",
      "\tBatch [945]:\n",
      "\t Loss: 0.5436\n",
      "Epoch [1/1]\n",
      "\tBatch [946]:\n",
      "\t Loss: 0.4841\n",
      "Epoch [1/1]\n",
      "\tBatch [947]:\n",
      "\t Loss: 0.5934\n",
      "Epoch [1/1]\n",
      "\tBatch [948]:\n",
      "\t Loss: 0.5930\n",
      "Epoch [1/1]\n",
      "\tBatch [949]:\n",
      "\t Loss: 0.5834\n",
      "Epoch [1/1]\n",
      "\tBatch [950]:\n",
      "\t Loss: 0.6030\n",
      "Epoch [1/1]\n",
      "\tBatch [951]:\n",
      "\t Loss: 0.5735\n",
      "Epoch [1/1]\n",
      "\tBatch [952]:\n",
      "\t Loss: 0.5227\n",
      "Epoch [1/1]\n",
      "\tBatch [953]:\n",
      "\t Loss: 0.5420\n",
      "Epoch [1/1]\n",
      "\tBatch [954]:\n",
      "\t Loss: 0.5303\n",
      "Epoch [1/1]\n",
      "\tBatch [955]:\n",
      "\t Loss: 0.5957\n",
      "Epoch [1/1]\n",
      "\tBatch [956]:\n",
      "\t Loss: 0.6309\n",
      "Epoch [1/1]\n",
      "\tBatch [957]:\n",
      "\t Loss: 0.5974\n",
      "Epoch [1/1]\n",
      "\tBatch [958]:\n",
      "\t Loss: 0.6095\n",
      "Epoch [1/1]\n",
      "\tBatch [959]:\n",
      "\t Loss: 0.5973\n",
      "Epoch [1/1]\n",
      "\tBatch [960]:\n",
      "\t Loss: 0.5172\n",
      "Epoch [1/1]\n",
      "\tBatch [961]:\n",
      "\t Loss: 0.5398\n",
      "Epoch [1/1]\n",
      "\tBatch [962]:\n",
      "\t Loss: 0.5848\n",
      "Epoch [1/1]\n",
      "\tBatch [963]:\n",
      "\t Loss: 0.6624\n",
      "Epoch [1/1]\n",
      "\tBatch [964]:\n",
      "\t Loss: 0.5185\n",
      "Epoch [1/1]\n",
      "\tBatch [965]:\n",
      "\t Loss: 0.5626\n",
      "Epoch [1/1]\n",
      "\tBatch [966]:\n",
      "\t Loss: 0.4986\n",
      "Epoch [1/1]\n",
      "\tBatch [967]:\n",
      "\t Loss: 0.5943\n",
      "Epoch [1/1]\n",
      "\tBatch [968]:\n",
      "\t Loss: 0.6157\n",
      "Epoch [1/1]\n",
      "\tBatch [969]:\n",
      "\t Loss: 0.5310\n",
      "Epoch [1/1]\n",
      "\tBatch [970]:\n",
      "\t Loss: 0.5206\n",
      "Epoch [1/1]\n",
      "\tBatch [971]:\n",
      "\t Loss: 0.6257\n",
      "Epoch [1/1]\n",
      "\tBatch [972]:\n",
      "\t Loss: 0.5313\n",
      "Epoch [1/1]\n",
      "\tBatch [973]:\n",
      "\t Loss: 0.6259\n",
      "Epoch [1/1]\n",
      "\tBatch [974]:\n",
      "\t Loss: 0.5729\n",
      "Epoch [1/1]\n",
      "\tBatch [975]:\n",
      "\t Loss: 0.6045\n",
      "Epoch [1/1]\n",
      "\tBatch [976]:\n",
      "\t Loss: 0.6349\n",
      "Epoch [1/1]\n",
      "\tBatch [977]:\n",
      "\t Loss: 0.5529\n",
      "Epoch [1/1]\n",
      "\tBatch [978]:\n",
      "\t Loss: 0.5033\n",
      "Epoch [1/1]\n",
      "\tBatch [979]:\n",
      "\t Loss: 0.4832\n",
      "Epoch [1/1]\n",
      "\tBatch [980]:\n",
      "\t Loss: 0.5727\n",
      "Epoch [1/1]\n",
      "\tBatch [981]:\n",
      "\t Loss: 0.5416\n",
      "Epoch [1/1]\n",
      "\tBatch [982]:\n",
      "\t Loss: 0.6381\n",
      "Epoch [1/1]\n",
      "\tBatch [983]:\n",
      "\t Loss: 0.4976\n",
      "Epoch [1/1]\n",
      "\tBatch [984]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [985]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [986]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [987]:\n",
      "\t Loss: 0.5973\n",
      "Epoch [1/1]\n",
      "\tBatch [988]:\n",
      "\t Loss: 0.5047\n",
      "Epoch [1/1]\n",
      "\tBatch [989]:\n",
      "\t Loss: 0.5152\n",
      "Epoch [1/1]\n",
      "\tBatch [990]:\n",
      "\t Loss: 0.6795\n",
      "Epoch [1/1]\n",
      "\tBatch [991]:\n",
      "\t Loss: 0.5511\n",
      "Epoch [1/1]\n",
      "\tBatch [992]:\n",
      "\t Loss: 0.5740\n",
      "Epoch [1/1]\n",
      "\tBatch [993]:\n",
      "\t Loss: 0.4826\n",
      "Epoch [1/1]\n",
      "\tBatch [994]:\n",
      "\t Loss: 0.6644\n",
      "Epoch [1/1]\n",
      "\tBatch [995]:\n",
      "\t Loss: 0.6202\n",
      "Epoch [1/1]\n",
      "\tBatch [996]:\n",
      "\t Loss: 0.5515\n",
      "Epoch [1/1]\n",
      "\tBatch [997]:\n",
      "\t Loss: 0.6606\n",
      "Epoch [1/1]\n",
      "\tBatch [998]:\n",
      "\t Loss: 0.5935\n",
      "Epoch [1/1]\n",
      "\tBatch [999]:\n",
      "\t Loss: 0.5930\n",
      "Epoch [1/1]\n",
      "\tBatch [1000]:\n",
      "\t Loss: 0.5455\n",
      "Epoch [1/1]\n",
      "\tBatch [1001]:\n",
      "\t Loss: 0.6400\n",
      "Epoch [1/1]\n",
      "\tBatch [1002]:\n",
      "\t Loss: 0.5029\n",
      "Epoch [1/1]\n",
      "\tBatch [1003]:\n",
      "\t Loss: 0.5842\n",
      "Epoch [1/1]\n",
      "\tBatch [1004]:\n",
      "\t Loss: 0.6017\n",
      "Epoch [1/1]\n",
      "\tBatch [1005]:\n",
      "\t Loss: 0.6110\n",
      "Epoch [1/1]\n",
      "\tBatch [1006]:\n",
      "\t Loss: 0.5488\n",
      "Epoch [1/1]\n",
      "\tBatch [1007]:\n",
      "\t Loss: 0.6204\n",
      "Epoch [1/1]\n",
      "\tBatch [1008]:\n",
      "\t Loss: 0.5751\n",
      "Epoch [1/1]\n",
      "\tBatch [1009]:\n",
      "\t Loss: 0.6020\n",
      "Epoch [1/1]\n",
      "\tBatch [1010]:\n",
      "\t Loss: 0.5651\n",
      "Epoch [1/1]\n",
      "\tBatch [1011]:\n",
      "\t Loss: 0.5926\n",
      "Epoch [1/1]\n",
      "\tBatch [1012]:\n",
      "\t Loss: 0.5552\n",
      "Epoch [1/1]\n",
      "\tBatch [1013]:\n",
      "\t Loss: 0.5057\n",
      "Epoch [1/1]\n",
      "\tBatch [1014]:\n",
      "\t Loss: 0.5431\n",
      "Epoch [1/1]\n",
      "\tBatch [1015]:\n",
      "\t Loss: 0.5523\n",
      "Epoch [1/1]\n",
      "\tBatch [1016]:\n",
      "\t Loss: 0.5729\n",
      "Epoch [1/1]\n",
      "\tBatch [1017]:\n",
      "\t Loss: 0.5625\n",
      "Epoch [1/1]\n",
      "\tBatch [1018]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [1019]:\n",
      "\t Loss: 0.5042\n",
      "Epoch [1/1]\n",
      "\tBatch [1020]:\n",
      "\t Loss: 0.6113\n",
      "Epoch [1/1]\n",
      "\tBatch [1021]:\n",
      "\t Loss: 0.5758\n",
      "Epoch [1/1]\n",
      "\tBatch [1022]:\n",
      "\t Loss: 0.6134\n",
      "Epoch [1/1]\n",
      "\tBatch [1023]:\n",
      "\t Loss: 0.5017\n",
      "Epoch [1/1]\n",
      "\tBatch [1024]:\n",
      "\t Loss: 0.6001\n",
      "Epoch [1/1]\n",
      "\tBatch [1025]:\n",
      "\t Loss: 0.5271\n",
      "Epoch [1/1]\n",
      "\tBatch [1026]:\n",
      "\t Loss: 0.5148\n",
      "Epoch [1/1]\n",
      "\tBatch [1027]:\n",
      "\t Loss: 0.5274\n",
      "Epoch [1/1]\n",
      "\tBatch [1028]:\n",
      "\t Loss: 0.5754\n",
      "Epoch [1/1]\n",
      "\tBatch [1029]:\n",
      "\t Loss: 0.4914\n",
      "Epoch [1/1]\n",
      "\tBatch [1030]:\n",
      "\t Loss: 0.5276\n",
      "Epoch [1/1]\n",
      "\tBatch [1031]:\n",
      "\t Loss: 0.4785\n",
      "Epoch [1/1]\n",
      "\tBatch [1032]:\n",
      "\t Loss: 0.6491\n",
      "Epoch [1/1]\n",
      "\tBatch [1033]:\n",
      "\t Loss: 0.5634\n",
      "Epoch [1/1]\n",
      "\tBatch [1034]:\n",
      "\t Loss: 0.5633\n",
      "Epoch [1/1]\n",
      "\tBatch [1035]:\n",
      "\t Loss: 0.5036\n",
      "Epoch [1/1]\n",
      "\tBatch [1036]:\n",
      "\t Loss: 0.6705\n",
      "Epoch [1/1]\n",
      "\tBatch [1037]:\n",
      "\t Loss: 0.5976\n",
      "Epoch [1/1]\n",
      "\tBatch [1038]:\n",
      "\t Loss: 0.6192\n",
      "Epoch [1/1]\n",
      "\tBatch [1039]:\n",
      "\t Loss: 0.5402\n",
      "Epoch [1/1]\n",
      "\tBatch [1040]:\n",
      "\t Loss: 0.5091\n",
      "Epoch [1/1]\n",
      "\tBatch [1041]:\n",
      "\t Loss: 0.5630\n",
      "Epoch [1/1]\n",
      "\tBatch [1042]:\n",
      "\t Loss: 0.5830\n",
      "Epoch [1/1]\n",
      "\tBatch [1043]:\n",
      "\t Loss: 0.5118\n",
      "Epoch [1/1]\n",
      "\tBatch [1044]:\n",
      "\t Loss: 0.5827\n",
      "Epoch [1/1]\n",
      "\tBatch [1045]:\n",
      "\t Loss: 0.5628\n",
      "Epoch [1/1]\n",
      "\tBatch [1046]:\n",
      "\t Loss: 0.5219\n",
      "Epoch [1/1]\n",
      "\tBatch [1047]:\n",
      "\t Loss: 0.5936\n",
      "Epoch [1/1]\n",
      "\tBatch [1048]:\n",
      "\t Loss: 0.5834\n",
      "Epoch [1/1]\n",
      "\tBatch [1049]:\n",
      "\t Loss: 0.5308\n",
      "Epoch [1/1]\n",
      "\tBatch [1050]:\n",
      "\t Loss: 0.5308\n",
      "Epoch [1/1]\n",
      "\tBatch [1051]:\n",
      "\t Loss: 0.5614\n",
      "Epoch [1/1]\n",
      "\tBatch [1052]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [1053]:\n",
      "\t Loss: 0.5184\n",
      "Epoch [1/1]\n",
      "\tBatch [1054]:\n",
      "\t Loss: 0.7221\n",
      "Epoch [1/1]\n",
      "\tBatch [1055]:\n",
      "\t Loss: 0.5284\n",
      "Epoch [1/1]\n",
      "\tBatch [1056]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [1057]:\n",
      "\t Loss: 0.5958\n",
      "Epoch [1/1]\n",
      "\tBatch [1058]:\n",
      "\t Loss: 0.4955\n",
      "Epoch [1/1]\n",
      "\tBatch [1059]:\n",
      "\t Loss: 0.6517\n",
      "Epoch [1/1]\n",
      "\tBatch [1060]:\n",
      "\t Loss: 0.5072\n",
      "Epoch [1/1]\n",
      "\tBatch [1061]:\n",
      "\t Loss: 0.5520\n",
      "Epoch [1/1]\n",
      "\tBatch [1062]:\n",
      "\t Loss: 0.4854\n",
      "Epoch [1/1]\n",
      "\tBatch [1063]:\n",
      "\t Loss: 0.6174\n",
      "Epoch [1/1]\n",
      "\tBatch [1064]:\n",
      "\t Loss: 0.4953\n",
      "Epoch [1/1]\n",
      "\tBatch [1065]:\n",
      "\t Loss: 0.5627\n",
      "Epoch [1/1]\n",
      "\tBatch [1066]:\n",
      "\t Loss: 0.4946\n",
      "Epoch [1/1]\n",
      "\tBatch [1067]:\n",
      "\t Loss: 0.5629\n",
      "Epoch [1/1]\n",
      "\tBatch [1068]:\n",
      "\t Loss: 0.5049\n",
      "Epoch [1/1]\n",
      "\tBatch [1069]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [1070]:\n",
      "\t Loss: 0.5635\n",
      "Epoch [1/1]\n",
      "\tBatch [1071]:\n",
      "\t Loss: 0.5512\n",
      "Epoch [1/1]\n",
      "\tBatch [1072]:\n",
      "\t Loss: 0.5023\n",
      "Epoch [1/1]\n",
      "\tBatch [1073]:\n",
      "\t Loss: 0.6152\n",
      "Epoch [1/1]\n",
      "\tBatch [1074]:\n",
      "\t Loss: 0.5643\n",
      "Epoch [1/1]\n",
      "\tBatch [1075]:\n",
      "\t Loss: 0.5389\n",
      "Epoch [1/1]\n",
      "\tBatch [1076]:\n",
      "\t Loss: 0.5637\n",
      "Epoch [1/1]\n",
      "\tBatch [1077]:\n",
      "\t Loss: 0.5761\n",
      "Epoch [1/1]\n",
      "\tBatch [1078]:\n",
      "\t Loss: 0.5751\n",
      "Epoch [1/1]\n",
      "\tBatch [1079]:\n",
      "\t Loss: 0.5746\n",
      "Epoch [1/1]\n",
      "\tBatch [1080]:\n",
      "\t Loss: 0.5859\n",
      "Epoch [1/1]\n",
      "\tBatch [1081]:\n",
      "\t Loss: 0.5059\n",
      "Epoch [1/1]\n",
      "\tBatch [1082]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [1083]:\n",
      "\t Loss: 0.6169\n",
      "Epoch [1/1]\n",
      "\tBatch [1084]:\n",
      "\t Loss: 0.6064\n",
      "Epoch [1/1]\n",
      "\tBatch [1085]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [1086]:\n",
      "\t Loss: 0.6554\n",
      "Epoch [1/1]\n",
      "\tBatch [1087]:\n",
      "\t Loss: 0.6230\n",
      "Epoch [1/1]\n",
      "\tBatch [1088]:\n",
      "\t Loss: 0.5930\n",
      "Epoch [1/1]\n",
      "\tBatch [1089]:\n",
      "\t Loss: 0.6195\n",
      "Epoch [1/1]\n",
      "\tBatch [1090]:\n",
      "\t Loss: 0.5498\n",
      "Epoch [1/1]\n",
      "\tBatch [1091]:\n",
      "\t Loss: 0.5514\n",
      "Epoch [1/1]\n",
      "\tBatch [1092]:\n",
      "\t Loss: 0.5095\n",
      "Epoch [1/1]\n",
      "\tBatch [1093]:\n",
      "\t Loss: 0.5331\n",
      "Epoch [1/1]\n",
      "\tBatch [1094]:\n",
      "\t Loss: 0.6019\n",
      "Epoch [1/1]\n",
      "\tBatch [1095]:\n",
      "\t Loss: 0.5283\n",
      "Epoch [1/1]\n",
      "\tBatch [1096]:\n",
      "\t Loss: 0.4482\n",
      "Epoch [1/1]\n",
      "\tBatch [1097]:\n",
      "\t Loss: 0.5522\n",
      "Epoch [1/1]\n",
      "\tBatch [1098]:\n",
      "\t Loss: 0.5409\n",
      "Epoch [1/1]\n",
      "\tBatch [1099]:\n",
      "\t Loss: 0.4705\n",
      "Epoch [1/1]\n",
      "\tBatch [1100]:\n",
      "\t Loss: 0.5879\n",
      "Epoch [1/1]\n",
      "\tBatch [1101]:\n",
      "\t Loss: 0.5274\n",
      "Epoch [1/1]\n",
      "\tBatch [1102]:\n",
      "\t Loss: 0.5142\n",
      "Epoch [1/1]\n",
      "\tBatch [1103]:\n",
      "\t Loss: 0.5276\n",
      "Epoch [1/1]\n",
      "\tBatch [1104]:\n",
      "\t Loss: 0.5982\n",
      "Epoch [1/1]\n",
      "\tBatch [1105]:\n",
      "\t Loss: 0.6268\n",
      "Epoch [1/1]\n",
      "\tBatch [1106]:\n",
      "\t Loss: 0.6833\n",
      "Epoch [1/1]\n",
      "\tBatch [1107]:\n",
      "\t Loss: 0.5278\n",
      "Epoch [1/1]\n",
      "\tBatch [1108]:\n",
      "\t Loss: 0.5684\n",
      "Epoch [1/1]\n",
      "\tBatch [1109]:\n",
      "\t Loss: 0.6855\n",
      "Epoch [1/1]\n",
      "\tBatch [1110]:\n",
      "\t Loss: 0.5274\n",
      "Epoch [1/1]\n",
      "\tBatch [1111]:\n",
      "\t Loss: 0.5393\n",
      "Epoch [1/1]\n",
      "\tBatch [1112]:\n",
      "\t Loss: 0.4926\n",
      "Epoch [1/1]\n",
      "\tBatch [1113]:\n",
      "\t Loss: 0.6191\n",
      "Epoch [1/1]\n",
      "\tBatch [1114]:\n",
      "\t Loss: 0.5507\n",
      "Epoch [1/1]\n",
      "\tBatch [1115]:\n",
      "\t Loss: 0.5514\n",
      "Epoch [1/1]\n",
      "\tBatch [1116]:\n",
      "\t Loss: 0.5624\n",
      "Epoch [1/1]\n",
      "\tBatch [1117]:\n",
      "\t Loss: 0.4814\n",
      "Epoch [1/1]\n",
      "\tBatch [1118]:\n",
      "\t Loss: 0.5840\n",
      "Epoch [1/1]\n",
      "\tBatch [1119]:\n",
      "\t Loss: 0.5431\n",
      "Epoch [1/1]\n",
      "\tBatch [1120]:\n",
      "\t Loss: 0.6139\n",
      "Epoch [1/1]\n",
      "\tBatch [1121]:\n",
      "\t Loss: 0.5531\n",
      "Epoch [1/1]\n",
      "\tBatch [1122]:\n",
      "\t Loss: 0.6752\n",
      "Epoch [1/1]\n",
      "\tBatch [1123]:\n",
      "\t Loss: 0.5829\n",
      "Epoch [1/1]\n",
      "\tBatch [1124]:\n",
      "\t Loss: 0.5043\n",
      "Epoch [1/1]\n",
      "\tBatch [1125]:\n",
      "\t Loss: 0.6231\n",
      "Epoch [1/1]\n",
      "\tBatch [1126]:\n",
      "\t Loss: 0.5632\n",
      "Epoch [1/1]\n",
      "\tBatch [1127]:\n",
      "\t Loss: 0.5539\n",
      "Epoch [1/1]\n",
      "\tBatch [1128]:\n",
      "\t Loss: 0.6326\n",
      "Epoch [1/1]\n",
      "\tBatch [1129]:\n",
      "\t Loss: 0.6721\n",
      "Epoch [1/1]\n",
      "\tBatch [1130]:\n",
      "\t Loss: 0.5736\n",
      "Epoch [1/1]\n",
      "\tBatch [1131]:\n",
      "\t Loss: 0.5927\n",
      "Epoch [1/1]\n",
      "\tBatch [1132]:\n",
      "\t Loss: 0.6306\n",
      "Epoch [1/1]\n",
      "\tBatch [1133]:\n",
      "\t Loss: 0.5471\n",
      "Epoch [1/1]\n",
      "\tBatch [1134]:\n",
      "\t Loss: 0.5659\n",
      "Epoch [1/1]\n",
      "\tBatch [1135]:\n",
      "\t Loss: 0.5658\n",
      "Epoch [1/1]\n",
      "\tBatch [1136]:\n",
      "\t Loss: 0.5835\n",
      "Epoch [1/1]\n",
      "\tBatch [1137]:\n",
      "\t Loss: 0.5188\n",
      "Epoch [1/1]\n",
      "\tBatch [1138]:\n",
      "\t Loss: 0.6308\n",
      "Epoch [1/1]\n",
      "\tBatch [1139]:\n",
      "\t Loss: 0.5737\n",
      "Epoch [1/1]\n",
      "\tBatch [1140]:\n",
      "\t Loss: 0.5640\n",
      "Epoch [1/1]\n",
      "\tBatch [1141]:\n",
      "\t Loss: 0.4943\n",
      "Epoch [1/1]\n",
      "\tBatch [1142]:\n",
      "\t Loss: 0.5627\n",
      "Epoch [1/1]\n",
      "\tBatch [1143]:\n",
      "\t Loss: 0.5417\n",
      "Epoch [1/1]\n",
      "\tBatch [1144]:\n",
      "\t Loss: 0.5080\n",
      "Epoch [1/1]\n",
      "\tBatch [1145]:\n",
      "\t Loss: 0.5513\n",
      "Epoch [1/1]\n",
      "\tBatch [1146]:\n",
      "\t Loss: 0.5163\n",
      "Epoch [1/1]\n",
      "\tBatch [1147]:\n",
      "\t Loss: 0.5394\n",
      "Epoch [1/1]\n",
      "\tBatch [1148]:\n",
      "\t Loss: 0.5517\n",
      "Epoch [1/1]\n",
      "\tBatch [1149]:\n",
      "\t Loss: 0.5018\n",
      "Epoch [1/1]\n",
      "\tBatch [1150]:\n",
      "\t Loss: 0.5010\n",
      "Epoch [1/1]\n",
      "\tBatch [1151]:\n",
      "\t Loss: 0.5534\n",
      "Epoch [1/1], Average Loss: 0.567149\n"
     ]
    }
   ],
   "source": [
    "# Training Loops\n",
    "model = ClassifierTransformer(d_b_size, class_number, head_number, encoder_layers, vocab_size = whole_data.voc_size(), feedfwd_dim = ffn_dim)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# Training the model\n",
    "train_loss_list = train_model(model, train_load, criterion, optimizer, Epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notices:\n",
    "- Average loss is around $0.5$ and fluctuates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch [1]:\n",
      "\tLoss: 0.5139, Accuracy 79.00%\n",
      "\n",
      "Test Batch [2]:\n",
      "\tLoss: 0.6211, Accuracy 75.00%\n",
      "\n",
      "Test Batch [3]:\n",
      "\tLoss: 0.6346, Accuracy 73.33%\n",
      "\n",
      "Test Batch [4]:\n",
      "\tLoss: 0.5944, Accuracy 73.25%\n",
      "\n",
      "Test Batch [5]:\n",
      "\tLoss: 0.5141, Accuracy 74.40%\n",
      "\n",
      "Test Batch [6]:\n",
      "\tLoss: 0.5942, Accuracy 74.17%\n",
      "\n",
      "Test Batch [7]:\n",
      "\tLoss: 0.5407, Accuracy 74.57%\n",
      "\n",
      "Test Batch [8]:\n",
      "\tLoss: 0.6884, Accuracy 73.50%\n",
      "\n",
      "Test Batch [9]:\n",
      "\tLoss: 0.7423, Accuracy 72.22%\n",
      "\n",
      "Test Batch [10]:\n",
      "\tLoss: 0.5678, Accuracy 72.50%\n",
      "\n",
      "Test Batch [11]:\n",
      "\tLoss: 0.6615, Accuracy 72.09%\n",
      "\n",
      "Test Batch [12]:\n",
      "\tLoss: 0.6614, Accuracy 71.75%\n",
      "\n",
      "Test Batch [13]:\n",
      "\tLoss: 0.5945, Accuracy 71.85%\n",
      "\n",
      "Test Batch [14]:\n",
      "\tLoss: 0.4737, Accuracy 72.57%\n",
      "\n",
      "Test Batch [15]:\n",
      "\tLoss: 0.6077, Accuracy 72.53%\n",
      "\n",
      "Test Batch [16]:\n",
      "\tLoss: 0.6078, Accuracy 72.50%\n",
      "\n",
      "Test Batch [17]:\n",
      "\tLoss: 0.5676, Accuracy 72.65%\n",
      "\n",
      "Test Batch [18]:\n",
      "\tLoss: 0.4872, Accuracy 73.11%\n",
      "\n",
      "Test Batch [19]:\n",
      "\tLoss: 0.5811, Accuracy 73.16%\n",
      "\n",
      "Test Batch [20]:\n",
      "\tLoss: 0.5675, Accuracy 73.25%\n",
      "\n",
      "Test Batch [21]:\n",
      "\tLoss: 0.5545, Accuracy 73.38%\n",
      "\n",
      "Test Batch [22]:\n",
      "\tLoss: 0.5408, Accuracy 73.55%\n",
      "\n",
      "Test Batch [23]:\n",
      "\tLoss: 0.6078, Accuracy 73.48%\n",
      "\n",
      "Test Batch [24]:\n",
      "\tLoss: 0.6345, Accuracy 73.33%\n",
      "\n",
      "Test Batch [25]:\n",
      "\tLoss: 0.5407, Accuracy 73.48%\n",
      "\n",
      "Test Batch [26]:\n",
      "\tLoss: 0.5407, Accuracy 73.62%\n",
      "\n",
      "Test Batch [27]:\n",
      "\tLoss: 0.5273, Accuracy 73.78%\n",
      "\n",
      "Test Batch [28]:\n",
      "\tLoss: 0.5943, Accuracy 73.75%\n",
      "\n",
      "Test Batch [29]:\n",
      "\tLoss: 0.6081, Accuracy 73.69%\n",
      "\n",
      "Test Batch [30]:\n",
      "\tLoss: 0.5274, Accuracy 73.83%\n",
      "\n",
      "Test Batch [31]:\n",
      "\tLoss: 0.5675, Accuracy 73.87%\n",
      "\n",
      "Test Batch [32]:\n",
      "\tLoss: 0.5138, Accuracy 74.03%\n",
      "\n",
      "Test Batch [33]:\n",
      "\tLoss: 0.6747, Accuracy 73.82%\n",
      "\n",
      "Test Batch [34]:\n",
      "\tLoss: 0.5137, Accuracy 73.97%\n",
      "\n",
      "Test Batch [35]:\n",
      "\tLoss: 0.5542, Accuracy 74.03%\n",
      "\n",
      "Test Batch [36]:\n",
      "\tLoss: 0.6347, Accuracy 73.92%\n",
      "\n",
      "Test Batch [37]:\n",
      "\tLoss: 0.6346, Accuracy 73.81%\n",
      "\n",
      "Test Batch [38]:\n",
      "\tLoss: 0.5410, Accuracy 73.89%\n",
      "\n",
      "Test Batch [39]:\n",
      "\tLoss: 0.6346, Accuracy 73.79%\n",
      "\n",
      "Test Batch [40]:\n",
      "\tLoss: 0.5675, Accuracy 73.83%\n",
      "\n",
      "Test Batch [41]:\n",
      "\tLoss: 0.5942, Accuracy 73.80%\n",
      "\n",
      "Test Batch [42]:\n",
      "\tLoss: 0.6077, Accuracy 73.76%\n",
      "\n",
      "Test Batch [43]:\n",
      "\tLoss: 0.5674, Accuracy 73.79%\n",
      "\n",
      "Test Batch [44]:\n",
      "\tLoss: 0.6479, Accuracy 73.68%\n",
      "\n",
      "Test Batch [45]:\n",
      "\tLoss: 0.5540, Accuracy 73.73%\n",
      "\n",
      "Test Batch [46]:\n",
      "\tLoss: 0.5679, Accuracy 73.76%\n",
      "\n",
      "Test Batch [47]:\n",
      "\tLoss: 0.5409, Accuracy 73.83%\n",
      "\n",
      "Test Batch [48]:\n",
      "\tLoss: 0.5944, Accuracy 73.81%\n",
      "\n",
      "Test Batch [49]:\n",
      "\tLoss: 0.5810, Accuracy 73.82%\n",
      "\n",
      "Test Batch [50]:\n",
      "\tLoss: 0.5676, Accuracy 73.84%\n",
      "\n",
      "Test Batch [51]:\n",
      "\tLoss: 0.4469, Accuracy 74.04%\n",
      "\n",
      "Test Batch [52]:\n",
      "\tLoss: 0.6213, Accuracy 73.98%\n",
      "\n",
      "Test Batch [53]:\n",
      "\tLoss: 0.4605, Accuracy 74.15%\n",
      "\n",
      "Test Batch [54]:\n",
      "\tLoss: 0.5810, Accuracy 74.15%\n",
      "\n",
      "Test Batch [55]:\n",
      "\tLoss: 0.5676, Accuracy 74.16%\n",
      "\n",
      "Test Batch [56]:\n",
      "\tLoss: 0.5139, Accuracy 74.25%\n",
      "\n",
      "Test Batch [57]:\n",
      "\tLoss: 0.5676, Accuracy 74.26%\n",
      "\n",
      "Test Batch [58]:\n",
      "\tLoss: 0.5676, Accuracy 74.28%\n",
      "\n",
      "Test Batch [59]:\n",
      "\tLoss: 0.6077, Accuracy 74.24%\n",
      "\n",
      "Test Batch [60]:\n",
      "\tLoss: 0.5140, Accuracy 74.32%\n",
      "\n",
      "Test Batch [61]:\n",
      "\tLoss: 0.5274, Accuracy 74.38%\n",
      "\n",
      "Test Batch [62]:\n",
      "\tLoss: 0.5408, Accuracy 74.42%\n",
      "\n",
      "Test Batch [63]:\n",
      "\tLoss: 0.6346, Accuracy 74.35%\n",
      "\n",
      "Test Batch [64]:\n",
      "\tLoss: 0.6211, Accuracy 74.30%\n",
      "\n",
      "Test Batch [65]:\n",
      "\tLoss: 0.6346, Accuracy 74.23%\n",
      "\n",
      "Test Batch [66]:\n",
      "\tLoss: 0.6348, Accuracy 74.17%\n",
      "\n",
      "Test Batch [67]:\n",
      "\tLoss: 0.6747, Accuracy 74.06%\n",
      "\n",
      "Test Batch [68]:\n",
      "\tLoss: 0.6079, Accuracy 74.03%\n",
      "\n",
      "Test Batch [69]:\n",
      "\tLoss: 0.5542, Accuracy 74.06%\n",
      "\n",
      "Test Batch [70]:\n",
      "\tLoss: 0.5675, Accuracy 74.07%\n",
      "\n",
      "Test Batch [71]:\n",
      "\tLoss: 0.5810, Accuracy 74.07%\n",
      "\n",
      "Test Batch [72]:\n",
      "\tLoss: 0.6345, Accuracy 74.01%\n",
      "\n",
      "Test Batch [73]:\n",
      "\tLoss: 0.6480, Accuracy 73.95%\n",
      "\n",
      "Test Batch [74]:\n",
      "\tLoss: 0.5273, Accuracy 74.00%\n",
      "\n",
      "Test Batch [75]:\n",
      "\tLoss: 0.5139, Accuracy 74.07%\n",
      "\n",
      "Test Batch [76]:\n",
      "\tLoss: 0.5944, Accuracy 74.05%\n",
      "\n",
      "Test Batch [77]:\n",
      "\tLoss: 0.4873, Accuracy 74.14%\n",
      "\n",
      "Test Batch [78]:\n",
      "\tLoss: 0.5407, Accuracy 74.18%\n",
      "\n",
      "Test Batch [79]:\n",
      "\tLoss: 0.5810, Accuracy 74.18%\n",
      "\n",
      "Test Batch [80]:\n",
      "\tLoss: 0.5140, Accuracy 74.24%\n",
      "\n",
      "Test Batch [81]:\n",
      "\tLoss: 0.5543, Accuracy 74.26%\n",
      "\n",
      "Test Batch [82]:\n",
      "\tLoss: 0.6345, Accuracy 74.21%\n",
      "\n",
      "Test Batch [83]:\n",
      "\tLoss: 0.5005, Accuracy 74.28%\n",
      "\n",
      "Test Batch [84]:\n",
      "\tLoss: 0.6212, Accuracy 74.24%\n",
      "\n",
      "Test Batch [85]:\n",
      "\tLoss: 0.5407, Accuracy 74.27%\n",
      "\n",
      "Test Batch [86]:\n",
      "\tLoss: 0.5001, Accuracy 74.34%\n",
      "\n",
      "Test Batch [87]:\n",
      "\tLoss: 0.5673, Accuracy 74.34%\n",
      "\n",
      "Test Batch [88]:\n",
      "\tLoss: 0.6481, Accuracy 74.28%\n",
      "\n",
      "Test Batch [89]:\n",
      "\tLoss: 0.6345, Accuracy 74.24%\n",
      "\n",
      "Test Batch [90]:\n",
      "\tLoss: 0.4873, Accuracy 74.31%\n",
      "\n",
      "Test Batch [91]:\n",
      "\tLoss: 0.7015, Accuracy 74.21%\n",
      "\n",
      "Test Batch [92]:\n",
      "\tLoss: 0.5812, Accuracy 74.21%\n",
      "\n",
      "Test Batch [93]:\n",
      "\tLoss: 0.5672, Accuracy 74.22%\n",
      "\n",
      "Test Batch [94]:\n",
      "\tLoss: 0.5808, Accuracy 74.21%\n",
      "\n",
      "Test Batch [95]:\n",
      "\tLoss: 0.5410, Accuracy 74.24%\n",
      "\n",
      "Test Batch [96]:\n",
      "\tLoss: 0.5673, Accuracy 74.25%\n",
      "\n",
      "Test Batch [97]:\n",
      "\tLoss: 0.6749, Accuracy 74.18%\n",
      "\n",
      "Test Batch [98]:\n",
      "\tLoss: 0.6344, Accuracy 74.13%\n",
      "\n",
      "Test Batch [99]:\n",
      "\tLoss: 0.5275, Accuracy 74.17%\n",
      "\n",
      "Test Batch [100]:\n",
      "\tLoss: 0.5542, Accuracy 74.19%\n",
      "\n",
      "Test Batch [101]:\n",
      "\tLoss: 0.5408, Accuracy 74.22%\n",
      "\n",
      "Test Batch [102]:\n",
      "\tLoss: 0.6346, Accuracy 74.18%\n",
      "\n",
      "Test Batch [103]:\n",
      "\tLoss: 0.6346, Accuracy 74.14%\n",
      "\n",
      "Test Batch [104]:\n",
      "\tLoss: 0.5408, Accuracy 74.16%\n",
      "\n",
      "Test Batch [105]:\n",
      "\tLoss: 0.5677, Accuracy 74.17%\n",
      "\n",
      "Test Batch [106]:\n",
      "\tLoss: 0.5947, Accuracy 74.16%\n",
      "\n",
      "Test Batch [107]:\n",
      "\tLoss: 0.6077, Accuracy 74.14%\n",
      "\n",
      "Test Batch [108]:\n",
      "\tLoss: 0.4603, Accuracy 74.22%\n",
      "\n",
      "Test Batch [109]:\n",
      "\tLoss: 0.6211, Accuracy 74.19%\n",
      "\n",
      "Test Batch [110]:\n",
      "\tLoss: 0.5408, Accuracy 74.22%\n",
      "\n",
      "Test Batch [111]:\n",
      "\tLoss: 0.5675, Accuracy 74.23%\n",
      "\n",
      "Test Batch [112]:\n",
      "\tLoss: 0.5279, Accuracy 74.26%\n",
      "\n",
      "Test Batch [113]:\n",
      "\tLoss: 0.5683, Accuracy 74.27%\n",
      "\n",
      "Test Batch [114]:\n",
      "\tLoss: 0.6612, Accuracy 74.21%\n",
      "\n",
      "Test Batch [115]:\n",
      "\tLoss: 0.5539, Accuracy 74.23%\n",
      "\n",
      "Test Batch [116]:\n",
      "\tLoss: 0.5274, Accuracy 74.26%\n",
      "\n",
      "Test Batch [117]:\n",
      "\tLoss: 0.4737, Accuracy 74.32%\n",
      "\n",
      "Test Batch [118]:\n",
      "\tLoss: 0.5944, Accuracy 74.31%\n",
      "\n",
      "Test Batch [119]:\n",
      "\tLoss: 0.5543, Accuracy 74.33%\n",
      "\n",
      "Test Batch [120]:\n",
      "\tLoss: 0.5273, Accuracy 74.36%\n",
      "\n",
      "Test Batch [121]:\n",
      "\tLoss: 0.6077, Accuracy 74.34%\n",
      "\n",
      "Test Batch [122]:\n",
      "\tLoss: 0.5004, Accuracy 74.39%\n",
      "\n",
      "Test Batch [123]:\n",
      "\tLoss: 0.5407, Accuracy 74.41%\n",
      "\n",
      "Test Batch [124]:\n",
      "\tLoss: 0.5409, Accuracy 74.43%\n",
      "\n",
      "Test Batch [125]:\n",
      "\tLoss: 0.5406, Accuracy 74.45%\n",
      "\n",
      "Test Batch [126]:\n",
      "\tLoss: 0.5540, Accuracy 74.46%\n",
      "\n",
      "Test Batch [127]:\n",
      "\tLoss: 0.5140, Accuracy 74.50%\n",
      "\n",
      "Test Batch [128]:\n",
      "\tLoss: 0.5005, Accuracy 74.54%\n",
      "\n",
      "Test Batch [129]:\n",
      "\tLoss: 0.5006, Accuracy 74.58%\n",
      "\n",
      "Test Batch [130]:\n",
      "\tLoss: 0.5543, Accuracy 74.59%\n",
      "\n",
      "Test Batch [131]:\n",
      "\tLoss: 0.5541, Accuracy 74.60%\n",
      "\n",
      "Test Batch [132]:\n",
      "\tLoss: 0.6212, Accuracy 74.58%\n",
      "\n",
      "Test Batch [133]:\n",
      "\tLoss: 0.6216, Accuracy 74.55%\n",
      "\n",
      "Test Batch [134]:\n",
      "\tLoss: 0.6882, Accuracy 74.49%\n",
      "\n",
      "Test Batch [135]:\n",
      "\tLoss: 0.5674, Accuracy 74.49%\n",
      "\n",
      "Test Batch [136]:\n",
      "\tLoss: 0.5542, Accuracy 74.50%\n",
      "\n",
      "Test Batch [137]:\n",
      "\tLoss: 0.4740, Accuracy 74.55%\n",
      "\n",
      "Test Batch [138]:\n",
      "\tLoss: 0.6348, Accuracy 74.52%\n",
      "\n",
      "Test Batch [139]:\n",
      "\tLoss: 0.6209, Accuracy 74.50%\n",
      "\n",
      "Test Batch [140]:\n",
      "\tLoss: 0.5675, Accuracy 74.50%\n",
      "\n",
      "Test Batch [141]:\n",
      "\tLoss: 0.5943, Accuracy 74.49%\n",
      "\n",
      "Test Batch [142]:\n",
      "\tLoss: 0.5949, Accuracy 74.48%\n",
      "\n",
      "Test Batch [143]:\n",
      "\tLoss: 0.5543, Accuracy 74.49%\n",
      "\n",
      "Test Batch [144]:\n",
      "\tLoss: 0.6211, Accuracy 74.47%\n",
      "\n",
      "Test Batch [145]:\n",
      "\tLoss: 0.5673, Accuracy 74.47%\n",
      "\n",
      "Test Batch [146]:\n",
      "\tLoss: 0.6479, Accuracy 74.43%\n",
      "\n",
      "Test Batch [147]:\n",
      "\tLoss: 0.6617, Accuracy 74.39%\n",
      "\n",
      "Test Batch [148]:\n",
      "\tLoss: 0.5141, Accuracy 74.42%\n",
      "\n",
      "Test Batch [149]:\n",
      "\tLoss: 0.5012, Accuracy 74.46%\n",
      "\n",
      "Test Batch [150]:\n",
      "\tLoss: 0.5813, Accuracy 74.45%\n",
      "\n",
      "Test Batch [151]:\n",
      "\tLoss: 0.6613, Accuracy 74.41%\n",
      "\n",
      "Test Batch [152]:\n",
      "\tLoss: 0.5677, Accuracy 74.41%\n",
      "\n",
      "Test Batch [153]:\n",
      "\tLoss: 0.4871, Accuracy 74.46%\n",
      "\n",
      "Test Batch [154]:\n",
      "\tLoss: 0.5676, Accuracy 74.46%\n",
      "\n",
      "Test Batch [155]:\n",
      "\tLoss: 0.6478, Accuracy 74.43%\n",
      "\n",
      "Test Batch [156]:\n",
      "\tLoss: 0.5672, Accuracy 74.43%\n",
      "\n",
      "Test Batch [157]:\n",
      "\tLoss: 0.6079, Accuracy 74.41%\n",
      "\n",
      "Test Batch [158]:\n",
      "\tLoss: 0.4739, Accuracy 74.46%\n",
      "\n",
      "Test Batch [159]:\n",
      "\tLoss: 0.6346, Accuracy 74.43%\n",
      "\n",
      "Test Batch [160]:\n",
      "\tLoss: 0.6879, Accuracy 74.38%\n",
      "\n",
      "Test Batch [161]:\n",
      "\tLoss: 0.6079, Accuracy 74.37%\n",
      "\n",
      "Test Batch [162]:\n",
      "\tLoss: 0.5945, Accuracy 74.36%\n",
      "\n",
      "Test Batch [163]:\n",
      "\tLoss: 0.4738, Accuracy 74.40%\n",
      "\n",
      "Test Batch [164]:\n",
      "\tLoss: 0.6077, Accuracy 74.39%\n",
      "\n",
      "Test Batch [165]:\n",
      "\tLoss: 0.5273, Accuracy 74.41%\n",
      "\n",
      "Test Batch [166]:\n",
      "\tLoss: 0.4872, Accuracy 74.45%\n",
      "\n",
      "Test Batch [167]:\n",
      "\tLoss: 0.6882, Accuracy 74.40%\n",
      "\n",
      "Test Batch [168]:\n",
      "\tLoss: 0.5678, Accuracy 74.40%\n",
      "\n",
      "Test Batch [169]:\n",
      "\tLoss: 0.5943, Accuracy 74.40%\n",
      "\n",
      "Test Batch [170]:\n",
      "\tLoss: 0.5274, Accuracy 74.42%\n",
      "\n",
      "Test Batch [171]:\n",
      "\tLoss: 0.6342, Accuracy 74.39%\n",
      "\n",
      "Test Batch [172]:\n",
      "\tLoss: 0.6076, Accuracy 74.38%\n",
      "\n",
      "Test Batch [173]:\n",
      "\tLoss: 0.5809, Accuracy 74.38%\n",
      "\n",
      "Test Batch [174]:\n",
      "\tLoss: 0.6346, Accuracy 74.35%\n",
      "\n",
      "Test Batch [175]:\n",
      "\tLoss: 0.5139, Accuracy 74.38%\n",
      "\n",
      "Test Batch [176]:\n",
      "\tLoss: 0.5672, Accuracy 74.38%\n",
      "\n",
      "Test Batch [177]:\n",
      "\tLoss: 0.5004, Accuracy 74.41%\n",
      "\n",
      "Test Batch [178]:\n",
      "\tLoss: 0.5408, Accuracy 74.43%\n",
      "\n",
      "Test Batch [179]:\n",
      "\tLoss: 0.6210, Accuracy 74.41%\n",
      "\n",
      "Test Batch [180]:\n",
      "\tLoss: 0.4738, Accuracy 74.45%\n",
      "\n",
      "Test Batch [181]:\n",
      "\tLoss: 0.5140, Accuracy 74.48%\n",
      "\n",
      "Test Batch [182]:\n",
      "\tLoss: 0.5272, Accuracy 74.49%\n",
      "\n",
      "Test Batch [183]:\n",
      "\tLoss: 0.6613, Accuracy 74.46%\n",
      "\n",
      "Test Batch [184]:\n",
      "\tLoss: 0.5943, Accuracy 74.45%\n",
      "\n",
      "Test Batch [185]:\n",
      "\tLoss: 0.5944, Accuracy 74.44%\n",
      "\n",
      "Test Batch [186]:\n",
      "\tLoss: 0.5812, Accuracy 74.44%\n",
      "\n",
      "Test Batch [187]:\n",
      "\tLoss: 0.5011, Accuracy 74.47%\n",
      "\n",
      "Test Batch [188]:\n",
      "\tLoss: 0.7014, Accuracy 74.42%\n",
      "\n",
      "Test Batch [189]:\n",
      "\tLoss: 0.6346, Accuracy 74.40%\n",
      "\n",
      "Test Batch [190]:\n",
      "\tLoss: 0.4738, Accuracy 74.44%\n",
      "\n",
      "Test Batch [191]:\n",
      "\tLoss: 0.5140, Accuracy 74.46%\n",
      "\n",
      "Test Batch [192]:\n",
      "\tLoss: 0.5542, Accuracy 74.47%\n",
      "\n",
      "Test Batch [193]:\n",
      "\tLoss: 0.5140, Accuracy 74.49%\n",
      "\n",
      "Test Batch [194]:\n",
      "\tLoss: 0.5942, Accuracy 74.48%\n",
      "\n",
      "Test Batch [195]:\n",
      "\tLoss: 0.5943, Accuracy 74.48%\n",
      "\n",
      "Test Batch [196]:\n",
      "\tLoss: 0.7150, Accuracy 74.42%\n",
      "\n",
      "Test Batch [197]:\n",
      "\tLoss: 0.5541, Accuracy 74.43%\n",
      "\n",
      "Test Batch [198]:\n",
      "\tLoss: 0.6479, Accuracy 74.40%\n",
      "\n",
      "Test Batch [199]:\n",
      "\tLoss: 0.5542, Accuracy 74.41%\n",
      "\n",
      "Test Batch [200]:\n",
      "\tLoss: 0.5539, Accuracy 74.42%\n",
      "\n",
      "Test Batch [201]:\n",
      "\tLoss: 0.6336, Accuracy 74.40%\n",
      "\n",
      "Test Batch [202]:\n",
      "\tLoss: 0.5944, Accuracy 74.39%\n",
      "\n",
      "Test Batch [203]:\n",
      "\tLoss: 0.5140, Accuracy 74.41%\n",
      "\n",
      "Test Batch [204]:\n",
      "\tLoss: 0.5139, Accuracy 74.44%\n",
      "\n",
      "Test Batch [205]:\n",
      "\tLoss: 0.6344, Accuracy 74.41%\n",
      "\n",
      "Test Batch [206]:\n",
      "\tLoss: 0.6348, Accuracy 74.39%\n",
      "\n",
      "Test Batch [207]:\n",
      "\tLoss: 0.7284, Accuracy 74.34%\n",
      "\n",
      "Test Batch [208]:\n",
      "\tLoss: 0.5676, Accuracy 74.34%\n",
      "\n",
      "Test Batch [209]:\n",
      "\tLoss: 0.5139, Accuracy 74.36%\n",
      "\n",
      "Test Batch [210]:\n",
      "\tLoss: 0.6883, Accuracy 74.32%\n",
      "\n",
      "Test Batch [211]:\n",
      "\tLoss: 0.6076, Accuracy 74.31%\n",
      "\n",
      "Test Batch [212]:\n",
      "\tLoss: 0.5810, Accuracy 74.31%\n",
      "\n",
      "Test Batch [213]:\n",
      "\tLoss: 0.6212, Accuracy 74.30%\n",
      "\n",
      "Test Batch [214]:\n",
      "\tLoss: 0.6479, Accuracy 74.27%\n",
      "\n",
      "Test Batch [215]:\n",
      "\tLoss: 0.6213, Accuracy 74.26%\n",
      "\n",
      "Test Batch [216]:\n",
      "\tLoss: 0.5944, Accuracy 74.25%\n",
      "\n",
      "Test Batch [217]:\n",
      "\tLoss: 0.5542, Accuracy 74.26%\n",
      "\n",
      "Test Batch [218]:\n",
      "\tLoss: 0.5809, Accuracy 74.26%\n",
      "\n",
      "Test Batch [219]:\n",
      "\tLoss: 0.5140, Accuracy 74.28%\n",
      "\n",
      "Test Batch [220]:\n",
      "\tLoss: 0.5944, Accuracy 74.27%\n",
      "\n",
      "Test Batch [221]:\n",
      "\tLoss: 0.6077, Accuracy 74.26%\n",
      "\n",
      "Test Batch [222]:\n",
      "\tLoss: 0.5810, Accuracy 74.26%\n",
      "\n",
      "Test Batch [223]:\n",
      "\tLoss: 0.4873, Accuracy 74.29%\n",
      "\n",
      "Test Batch [224]:\n",
      "\tLoss: 0.5408, Accuracy 74.30%\n",
      "\n",
      "Test Batch [225]:\n",
      "\tLoss: 0.5936, Accuracy 74.30%\n",
      "\n",
      "Test Batch [226]:\n",
      "\tLoss: 0.6347, Accuracy 74.28%\n",
      "\n",
      "Test Batch [227]:\n",
      "\tLoss: 0.5409, Accuracy 74.29%\n",
      "\n",
      "Test Batch [228]:\n",
      "\tLoss: 0.4874, Accuracy 74.32%\n",
      "\n",
      "Test Batch [229]:\n",
      "\tLoss: 0.6613, Accuracy 74.29%\n",
      "\n",
      "Test Batch [230]:\n",
      "\tLoss: 0.6077, Accuracy 74.28%\n",
      "\n",
      "Test Batch [231]:\n",
      "\tLoss: 0.5542, Accuracy 74.29%\n",
      "\n",
      "Test Batch [232]:\n",
      "\tLoss: 0.4469, Accuracy 74.33%\n",
      "\n",
      "Test Batch [233]:\n",
      "\tLoss: 0.5676, Accuracy 74.33%\n",
      "\n",
      "Test Batch [234]:\n",
      "\tLoss: 0.6476, Accuracy 74.31%\n",
      "\n",
      "Test Batch [235]:\n",
      "\tLoss: 0.5810, Accuracy 74.31%\n",
      "\n",
      "Test Batch [236]:\n",
      "\tLoss: 0.5677, Accuracy 74.31%\n",
      "\n",
      "Test Batch [237]:\n",
      "\tLoss: 0.6745, Accuracy 74.28%\n",
      "\n",
      "Test Batch [238]:\n",
      "\tLoss: 0.4738, Accuracy 74.32%\n",
      "\n",
      "Test Batch [239]:\n",
      "\tLoss: 0.5139, Accuracy 74.33%\n",
      "\n",
      "Test Batch [240]:\n",
      "\tLoss: 0.5275, Accuracy 74.35%\n",
      "\n",
      "Test Batch [241]:\n",
      "\tLoss: 0.5407, Accuracy 74.36%\n",
      "\n",
      "Test Batch [242]:\n",
      "\tLoss: 0.5270, Accuracy 74.38%\n",
      "\n",
      "Test Batch [243]:\n",
      "\tLoss: 0.5809, Accuracy 74.37%\n",
      "\n",
      "Test Batch [244]:\n",
      "\tLoss: 0.5138, Accuracy 74.39%\n",
      "\n",
      "Test Batch [245]:\n",
      "\tLoss: 0.5946, Accuracy 74.39%\n",
      "\n",
      "Test Batch [246]:\n",
      "\tLoss: 0.5405, Accuracy 74.40%\n",
      "\n",
      "Test Batch [247]:\n",
      "\tLoss: 0.5274, Accuracy 74.41%\n",
      "\n",
      "Test Batch [248]:\n",
      "\tLoss: 0.4872, Accuracy 74.44%\n",
      "\n",
      "Test Batch [249]:\n",
      "\tLoss: 0.6211, Accuracy 74.43%\n",
      "\n",
      "Test Batch [250]:\n",
      "\tLoss: 0.5675, Accuracy 74.43%\n",
      "\n",
      "Test Batch [251]:\n",
      "\tLoss: 0.5141, Accuracy 74.45%\n",
      "\n",
      "Test Batch [252]:\n",
      "\tLoss: 0.6614, Accuracy 74.42%\n",
      "\n",
      "Test Batch [253]:\n",
      "\tLoss: 0.6613, Accuracy 74.40%\n",
      "\n",
      "Test Batch [254]:\n",
      "\tLoss: 0.6480, Accuracy 74.37%\n",
      "\n",
      "Test Batch [255]:\n",
      "\tLoss: 0.5676, Accuracy 74.38%\n",
      "\n",
      "Test Batch [256]:\n",
      "\tLoss: 0.5810, Accuracy 74.38%\n",
      "\n",
      "Test Batch [257]:\n",
      "\tLoss: 0.5408, Accuracy 74.39%\n",
      "\n",
      "Test Batch [258]:\n",
      "\tLoss: 0.5542, Accuracy 74.39%\n",
      "\n",
      "Test Batch [259]:\n",
      "\tLoss: 0.7013, Accuracy 74.36%\n",
      "\n",
      "Test Batch [260]:\n",
      "\tLoss: 0.6216, Accuracy 74.34%\n",
      "\n",
      "Test Batch [261]:\n",
      "\tLoss: 0.4874, Accuracy 74.37%\n",
      "\n",
      "Test Batch [262]:\n",
      "\tLoss: 0.5006, Accuracy 74.39%\n",
      "\n",
      "Test Batch [263]:\n",
      "\tLoss: 0.5812, Accuracy 74.39%\n",
      "\n",
      "Test Batch [264]:\n",
      "\tLoss: 0.5407, Accuracy 74.40%\n",
      "\n",
      "Test Batch [265]:\n",
      "\tLoss: 0.5943, Accuracy 74.39%\n",
      "\n",
      "Test Batch [266]:\n",
      "\tLoss: 0.5679, Accuracy 74.39%\n",
      "\n",
      "Test Batch [267]:\n",
      "\tLoss: 0.5142, Accuracy 74.41%\n",
      "\n",
      "Test Batch [268]:\n",
      "\tLoss: 0.5140, Accuracy 74.43%\n",
      "\n",
      "Test Batch [269]:\n",
      "\tLoss: 0.5407, Accuracy 74.44%\n",
      "\n",
      "Test Batch [270]:\n",
      "\tLoss: 0.5274, Accuracy 74.45%\n",
      "\n",
      "Test Batch [271]:\n",
      "\tLoss: 0.5677, Accuracy 74.45%\n",
      "\n",
      "Test Batch [272]:\n",
      "\tLoss: 0.5140, Accuracy 74.47%\n",
      "\n",
      "Test Batch [273]:\n",
      "\tLoss: 0.6211, Accuracy 74.46%\n",
      "\n",
      "Test Batch [274]:\n",
      "\tLoss: 0.5270, Accuracy 74.47%\n",
      "\n",
      "Test Batch [275]:\n",
      "\tLoss: 0.5944, Accuracy 74.47%\n",
      "\n",
      "Test Batch [276]:\n",
      "\tLoss: 0.5273, Accuracy 74.48%\n",
      "\n",
      "Test Batch [277]:\n",
      "\tLoss: 0.6746, Accuracy 74.45%\n",
      "\n",
      "Test Batch [278]:\n",
      "\tLoss: 0.4741, Accuracy 74.48%\n",
      "\n",
      "Test Batch [279]:\n",
      "\tLoss: 0.6210, Accuracy 74.47%\n",
      "\n",
      "Test Batch [280]:\n",
      "\tLoss: 0.5407, Accuracy 74.48%\n",
      "\n",
      "Test Batch [281]:\n",
      "\tLoss: 0.4738, Accuracy 74.50%\n",
      "\n",
      "Test Batch [282]:\n",
      "\tLoss: 0.5408, Accuracy 74.51%\n",
      "\n",
      "Test Batch [283]:\n",
      "\tLoss: 0.5139, Accuracy 74.53%\n",
      "\n",
      "Test Batch [284]:\n",
      "\tLoss: 0.5274, Accuracy 74.54%\n",
      "\n",
      "Test Batch [285]:\n",
      "\tLoss: 0.6078, Accuracy 74.53%\n",
      "\n",
      "Test Batch [286]:\n",
      "\tLoss: 0.6210, Accuracy 74.52%\n",
      "\n",
      "Test Batch [287]:\n",
      "\tLoss: 0.4470, Accuracy 74.55%\n",
      "\n",
      "Test Batch [288]:\n",
      "\tLoss: 0.6212, Accuracy 74.54%\n",
      "\n",
      "Test Batch [289]:\n",
      "\tLoss: 0.5807, Accuracy 74.54%\n",
      "\n",
      "Test Batch [290]:\n",
      "\tLoss: 0.5140, Accuracy 74.55%\n",
      "\n",
      "Test Batch [291]:\n",
      "\tLoss: 0.6077, Accuracy 74.54%\n",
      "\n",
      "Test Batch [292]:\n",
      "\tLoss: 0.5814, Accuracy 74.54%\n",
      "\n",
      "Test Batch [293]:\n",
      "\tLoss: 0.4868, Accuracy 74.56%\n",
      "\n",
      "Test Batch [294]:\n",
      "\tLoss: 0.5945, Accuracy 74.56%\n",
      "\n",
      "Test Batch [295]:\n",
      "\tLoss: 0.5810, Accuracy 74.56%\n",
      "\n",
      "Test Batch [296]:\n",
      "\tLoss: 0.6079, Accuracy 74.55%\n",
      "\n",
      "Test Batch [297]:\n",
      "\tLoss: 0.4871, Accuracy 74.57%\n",
      "\n",
      "Test Batch [298]:\n",
      "\tLoss: 0.5006, Accuracy 74.59%\n",
      "\n",
      "Test Batch [299]:\n",
      "\tLoss: 0.5273, Accuracy 74.60%\n",
      "\n",
      "Test Batch [300]:\n",
      "\tLoss: 0.5675, Accuracy 74.60%\n",
      "\n",
      "Test Batch [301]:\n",
      "\tLoss: 0.5405, Accuracy 74.61%\n",
      "\n",
      "Test Batch [302]:\n",
      "\tLoss: 0.6880, Accuracy 74.58%\n",
      "\n",
      "Test Batch [303]:\n",
      "\tLoss: 0.5543, Accuracy 74.58%\n",
      "\n",
      "Test Batch [304]:\n",
      "\tLoss: 0.5677, Accuracy 74.59%\n",
      "\n",
      "Test Batch [305]:\n",
      "\tLoss: 0.7283, Accuracy 74.55%\n",
      "\n",
      "Test Batch [306]:\n",
      "\tLoss: 0.5137, Accuracy 74.56%\n",
      "\n",
      "Test Batch [307]:\n",
      "\tLoss: 0.6481, Accuracy 74.54%\n",
      "\n",
      "Test Batch [308]:\n",
      "\tLoss: 0.5544, Accuracy 74.55%\n",
      "\n",
      "Test Batch [309]:\n",
      "\tLoss: 0.5006, Accuracy 74.57%\n",
      "\n",
      "Test Batch [310]:\n",
      "\tLoss: 0.5944, Accuracy 74.56%\n",
      "\n",
      "Test Batch [311]:\n",
      "\tLoss: 0.6480, Accuracy 74.54%\n",
      "\n",
      "Test Batch [312]:\n",
      "\tLoss: 0.5943, Accuracy 74.54%\n",
      "\n",
      "Test Batch [313]:\n",
      "\tLoss: 0.6480, Accuracy 74.52%\n",
      "\n",
      "Test Batch [314]:\n",
      "\tLoss: 0.5274, Accuracy 74.53%\n",
      "\n",
      "Test Batch [315]:\n",
      "\tLoss: 0.6747, Accuracy 74.51%\n",
      "\n",
      "Test Batch [316]:\n",
      "\tLoss: 0.5274, Accuracy 74.52%\n",
      "\n",
      "Test Batch [317]:\n",
      "\tLoss: 0.5006, Accuracy 74.54%\n",
      "\n",
      "Test Batch [318]:\n",
      "\tLoss: 0.6483, Accuracy 74.52%\n",
      "\n",
      "Test Batch [319]:\n",
      "\tLoss: 0.5541, Accuracy 74.52%\n",
      "\n",
      "Test Batch [320]:\n",
      "\tLoss: 0.6080, Accuracy 74.52%\n",
      "\n",
      "Test Batch [321]:\n",
      "\tLoss: 0.5141, Accuracy 74.53%\n",
      "\n",
      "Test Batch [322]:\n",
      "\tLoss: 0.6346, Accuracy 74.52%\n",
      "\n",
      "Test Batch [323]:\n",
      "\tLoss: 0.6212, Accuracy 74.50%\n",
      "\n",
      "Test Batch [324]:\n",
      "\tLoss: 0.5943, Accuracy 74.50%\n",
      "\n",
      "Test Batch [325]:\n",
      "\tLoss: 0.6346, Accuracy 74.49%\n",
      "\n",
      "Test Batch [326]:\n",
      "\tLoss: 0.6210, Accuracy 74.48%\n",
      "\n",
      "Test Batch [327]:\n",
      "\tLoss: 0.5812, Accuracy 74.47%\n",
      "\n",
      "Test Batch [328]:\n",
      "\tLoss: 0.4872, Accuracy 74.49%\n",
      "\n",
      "Test Batch [329]:\n",
      "\tLoss: 0.5810, Accuracy 74.49%\n",
      "\n",
      "Test Batch [330]:\n",
      "\tLoss: 0.5138, Accuracy 74.51%\n",
      "\n",
      "Test Batch [331]:\n",
      "\tLoss: 0.5545, Accuracy 74.51%\n",
      "\n",
      "Test Batch [332]:\n",
      "\tLoss: 0.6213, Accuracy 74.50%\n",
      "\n",
      "Test Batch [333]:\n",
      "\tLoss: 0.5541, Accuracy 74.50%\n",
      "\n",
      "Test Batch [334]:\n",
      "\tLoss: 0.4739, Accuracy 74.53%\n",
      "\n",
      "Test Batch [335]:\n",
      "\tLoss: 0.6211, Accuracy 74.52%\n",
      "\n",
      "Test Batch [336]:\n",
      "\tLoss: 0.6076, Accuracy 74.51%\n",
      "\n",
      "Test Batch [337]:\n",
      "\tLoss: 0.6211, Accuracy 74.50%\n",
      "\n",
      "Test Batch [338]:\n",
      "\tLoss: 0.5675, Accuracy 74.50%\n",
      "\n",
      "Test Batch [339]:\n",
      "\tLoss: 0.5541, Accuracy 74.50%\n",
      "\n",
      "Test Batch [340]:\n",
      "\tLoss: 0.5274, Accuracy 74.51%\n",
      "\n",
      "Test Batch [341]:\n",
      "\tLoss: 0.6613, Accuracy 74.50%\n",
      "\n",
      "Test Batch [342]:\n",
      "\tLoss: 0.5809, Accuracy 74.49%\n",
      "\n",
      "Test Batch [343]:\n",
      "\tLoss: 0.5273, Accuracy 74.50%\n",
      "\n",
      "Test Batch [344]:\n",
      "\tLoss: 0.5674, Accuracy 74.51%\n",
      "\n",
      "Test Batch [345]:\n",
      "\tLoss: 0.5809, Accuracy 74.50%\n",
      "\n",
      "Test Batch [346]:\n",
      "\tLoss: 0.5540, Accuracy 74.51%\n",
      "\n",
      "Test Batch [347]:\n",
      "\tLoss: 0.5676, Accuracy 74.51%\n",
      "\n",
      "Test Batch [348]:\n",
      "\tLoss: 0.5941, Accuracy 74.51%\n",
      "\n",
      "Test Batch [349]:\n",
      "\tLoss: 0.5408, Accuracy 74.51%\n",
      "\n",
      "Test Batch [350]:\n",
      "\tLoss: 0.6343, Accuracy 74.50%\n",
      "\n",
      "Test Batch [351]:\n",
      "\tLoss: 0.4873, Accuracy 74.52%\n",
      "\n",
      "Test Batch [352]:\n",
      "\tLoss: 0.5407, Accuracy 74.53%\n",
      "\n",
      "Test Batch [353]:\n",
      "\tLoss: 0.5271, Accuracy 74.54%\n",
      "\n",
      "Test Batch [354]:\n",
      "\tLoss: 0.5541, Accuracy 74.54%\n",
      "\n",
      "Test Batch [355]:\n",
      "\tLoss: 0.5541, Accuracy 74.54%\n",
      "\n",
      "Test Batch [356]:\n",
      "\tLoss: 0.6748, Accuracy 74.52%\n",
      "\n",
      "Test Batch [357]:\n",
      "\tLoss: 0.4872, Accuracy 74.54%\n",
      "\n",
      "Test Batch [358]:\n",
      "\tLoss: 0.5808, Accuracy 74.54%\n",
      "\n",
      "Test Batch [359]:\n",
      "\tLoss: 0.5406, Accuracy 74.55%\n",
      "\n",
      "Test Batch [360]:\n",
      "\tLoss: 0.5943, Accuracy 74.54%\n",
      "\n",
      "Test Batch [361]:\n",
      "\tLoss: 0.5811, Accuracy 74.54%\n",
      "\n",
      "Test Batch [362]:\n",
      "\tLoss: 0.6479, Accuracy 74.52%\n",
      "\n",
      "Test Batch [363]:\n",
      "\tLoss: 0.6348, Accuracy 74.51%\n",
      "\n",
      "Test Batch [364]:\n",
      "\tLoss: 0.6078, Accuracy 74.51%\n",
      "\n",
      "Test Batch [365]:\n",
      "\tLoss: 0.5138, Accuracy 74.52%\n",
      "\n",
      "Test Batch [366]:\n",
      "\tLoss: 0.5676, Accuracy 74.52%\n",
      "\n",
      "Test Batch [367]:\n",
      "\tLoss: 0.5543, Accuracy 74.52%\n",
      "\n",
      "Test Batch [368]:\n",
      "\tLoss: 0.6080, Accuracy 74.52%\n",
      "\n",
      "Test Batch [369]:\n",
      "\tLoss: 0.6210, Accuracy 74.51%\n",
      "\n",
      "Test Batch [370]:\n",
      "\tLoss: 0.5948, Accuracy 74.50%\n",
      "\n",
      "Test Batch [371]:\n",
      "\tLoss: 0.6212, Accuracy 74.49%\n",
      "\n",
      "Test Batch [372]:\n",
      "\tLoss: 0.5678, Accuracy 74.49%\n",
      "\n",
      "Test Batch [373]:\n",
      "\tLoss: 0.5541, Accuracy 74.50%\n",
      "\n",
      "Test Batch [374]:\n",
      "\tLoss: 0.5945, Accuracy 74.49%\n",
      "\n",
      "Test Batch [375]:\n",
      "\tLoss: 0.6480, Accuracy 74.48%\n",
      "\n",
      "Test Batch [376]:\n",
      "\tLoss: 0.5006, Accuracy 74.49%\n",
      "\n",
      "Test Batch [377]:\n",
      "\tLoss: 0.5944, Accuracy 74.49%\n",
      "\n",
      "Test Batch [378]:\n",
      "\tLoss: 0.5941, Accuracy 74.49%\n",
      "\n",
      "Test Batch [379]:\n",
      "\tLoss: 0.5138, Accuracy 74.50%\n",
      "\n",
      "Test Batch [380]:\n",
      "\tLoss: 0.5410, Accuracy 74.51%\n",
      "\n",
      "Test Batch [381]:\n",
      "\tLoss: 0.5409, Accuracy 74.51%\n",
      "\n",
      "Test Batch [382]:\n",
      "\tLoss: 0.5944, Accuracy 74.51%\n",
      "\n",
      "Test Batch [383]:\n",
      "\tLoss: 0.5141, Accuracy 74.52%\n",
      "\n",
      "Test Batch [384]:\n",
      "\tLoss: 0.4872, Accuracy 74.54%\n",
      "\n",
      "Test Batch [385]:\n",
      "\tLoss: 0.5141, Accuracy 74.55%\n",
      "\n",
      "Test Batch [386]:\n",
      "\tLoss: 0.4737, Accuracy 74.57%\n",
      "\n",
      "Test Batch [387]:\n",
      "\tLoss: 0.5682, Accuracy 74.57%\n",
      "\n",
      "Test Batch [388]:\n",
      "\tLoss: 0.6211, Accuracy 74.56%\n",
      "\n",
      "Test Batch [389]:\n",
      "\tLoss: 0.5676, Accuracy 74.56%\n",
      "\n",
      "Test Batch [390]:\n",
      "\tLoss: 0.6212, Accuracy 74.55%\n",
      "\n",
      "Test Batch [391]:\n",
      "\tLoss: 0.5809, Accuracy 74.55%\n",
      "\n",
      "Test Batch [392]:\n",
      "\tLoss: 0.5809, Accuracy 74.55%\n",
      "\n",
      "Test Batch [393]:\n",
      "\tLoss: 0.5407, Accuracy 74.55%\n",
      "\n",
      "Test Batch [394]:\n",
      "\tLoss: 0.5676, Accuracy 74.56%\n",
      "\n",
      "Test Batch [395]:\n",
      "\tLoss: 0.5679, Accuracy 74.56%\n",
      "\n",
      "Test Batch [396]:\n",
      "\tLoss: 0.5944, Accuracy 74.55%\n",
      "\n",
      "Test Batch [397]:\n",
      "\tLoss: 0.5804, Accuracy 74.55%\n",
      "\n",
      "Test Batch [398]:\n",
      "\tLoss: 0.6348, Accuracy 74.54%\n",
      "\n",
      "Test Batch [399]:\n",
      "\tLoss: 0.6479, Accuracy 74.53%\n",
      "\n",
      "Test Batch [400]:\n",
      "\tLoss: 0.5679, Accuracy 74.53%\n",
      "\n",
      "Test Batch [401]:\n",
      "\tLoss: 0.6344, Accuracy 74.52%\n",
      "\n",
      "Test Batch [402]:\n",
      "\tLoss: 0.6348, Accuracy 74.50%\n",
      "\n",
      "Test Batch [403]:\n",
      "\tLoss: 0.6075, Accuracy 74.50%\n",
      "\n",
      "Test Batch [404]:\n",
      "\tLoss: 0.4737, Accuracy 74.52%\n",
      "\n",
      "Test Batch [405]:\n",
      "\tLoss: 0.4738, Accuracy 74.54%\n",
      "\n",
      "Test Batch [406]:\n",
      "\tLoss: 0.5808, Accuracy 74.53%\n",
      "\n",
      "Test Batch [407]:\n",
      "\tLoss: 0.5412, Accuracy 74.54%\n",
      "\n",
      "Test Batch [408]:\n",
      "\tLoss: 0.5677, Accuracy 74.54%\n",
      "\n",
      "Test Batch [409]:\n",
      "\tLoss: 0.5409, Accuracy 74.55%\n",
      "\n",
      "Test Batch [410]:\n",
      "\tLoss: 0.6211, Accuracy 74.54%\n",
      "\n",
      "Test Batch [411]:\n",
      "\tLoss: 0.7014, Accuracy 74.52%\n",
      "\n",
      "Test Batch [412]:\n",
      "\tLoss: 0.6076, Accuracy 74.51%\n",
      "\n",
      "Test Batch [413]:\n",
      "\tLoss: 0.6078, Accuracy 74.50%\n",
      "\n",
      "Test Batch [414]:\n",
      "\tLoss: 0.5943, Accuracy 74.50%\n",
      "\n",
      "Test Batch [415]:\n",
      "\tLoss: 0.5811, Accuracy 74.50%\n",
      "\n",
      "Test Batch [416]:\n",
      "\tLoss: 0.5272, Accuracy 74.51%\n",
      "\n",
      "Test Batch [417]:\n",
      "\tLoss: 0.6076, Accuracy 74.50%\n",
      "\n",
      "Test Batch [418]:\n",
      "\tLoss: 0.5675, Accuracy 74.50%\n",
      "\n",
      "Test Batch [419]:\n",
      "\tLoss: 0.5943, Accuracy 74.50%\n",
      "\n",
      "Test Batch [420]:\n",
      "\tLoss: 0.6346, Accuracy 74.49%\n",
      "\n",
      "Test Batch [421]:\n",
      "\tLoss: 0.5004, Accuracy 74.50%\n",
      "\n",
      "Test Batch [422]:\n",
      "\tLoss: 0.5274, Accuracy 74.51%\n",
      "\n",
      "Test Batch [423]:\n",
      "\tLoss: 0.5138, Accuracy 74.52%\n",
      "\n",
      "Test Batch [424]:\n",
      "\tLoss: 0.4470, Accuracy 74.54%\n",
      "\n",
      "Test Batch [425]:\n",
      "\tLoss: 0.5542, Accuracy 74.55%\n",
      "\n",
      "Test Batch [426]:\n",
      "\tLoss: 0.5943, Accuracy 74.54%\n",
      "\n",
      "Test Batch [427]:\n",
      "\tLoss: 0.5407, Accuracy 74.55%\n",
      "\n",
      "Test Batch [428]:\n",
      "\tLoss: 0.5543, Accuracy 74.55%\n",
      "\n",
      "Test Batch [429]:\n",
      "\tLoss: 0.5273, Accuracy 74.56%\n",
      "\n",
      "Test Batch [430]:\n",
      "\tLoss: 0.5003, Accuracy 74.57%\n",
      "\n",
      "Test Batch [431]:\n",
      "\tLoss: 0.6078, Accuracy 74.57%\n",
      "\n",
      "Test Batch [432]:\n",
      "\tLoss: 0.6077, Accuracy 74.56%\n",
      "\n",
      "Test Batch [433]:\n",
      "\tLoss: 0.6613, Accuracy 74.55%\n",
      "\n",
      "Test Batch [434]:\n",
      "\tLoss: 0.6212, Accuracy 74.54%\n",
      "\n",
      "Test Batch [435]:\n",
      "\tLoss: 0.5542, Accuracy 74.54%\n",
      "\n",
      "Test Batch [436]:\n",
      "\tLoss: 0.5274, Accuracy 74.55%\n",
      "\n",
      "Test Batch [437]:\n",
      "\tLoss: 0.4870, Accuracy 74.56%\n",
      "\n",
      "Test Batch [438]:\n",
      "\tLoss: 0.5541, Accuracy 74.57%\n",
      "\n",
      "Test Batch [439]:\n",
      "\tLoss: 0.5809, Accuracy 74.56%\n",
      "\n",
      "Test Batch [440]:\n",
      "\tLoss: 0.5140, Accuracy 74.58%\n",
      "\n",
      "Test Batch [441]:\n",
      "\tLoss: 0.5006, Accuracy 74.59%\n",
      "\n",
      "Test Batch [442]:\n",
      "\tLoss: 0.5945, Accuracy 74.58%\n",
      "\n",
      "Test Batch [443]:\n",
      "\tLoss: 0.4470, Accuracy 74.60%\n",
      "\n",
      "Test Batch [444]:\n",
      "\tLoss: 0.6076, Accuracy 74.60%\n",
      "\n",
      "Test Batch [445]:\n",
      "\tLoss: 0.5275, Accuracy 74.61%\n",
      "\n",
      "Test Batch [446]:\n",
      "\tLoss: 0.5943, Accuracy 74.60%\n",
      "\n",
      "Test Batch [447]:\n",
      "\tLoss: 0.5274, Accuracy 74.61%\n",
      "\n",
      "Test Batch [448]:\n",
      "\tLoss: 0.6480, Accuracy 74.60%\n",
      "\n",
      "Test Batch [449]:\n",
      "\tLoss: 0.5675, Accuracy 74.60%\n",
      "\n",
      "Test Batch [450]:\n",
      "\tLoss: 0.7006, Accuracy 74.58%\n",
      "\n",
      "Test Batch [451]:\n",
      "\tLoss: 0.6344, Accuracy 74.57%\n",
      "\n",
      "Test Batch [452]:\n",
      "\tLoss: 0.5407, Accuracy 74.57%\n",
      "\n",
      "Test Batch [453]:\n",
      "\tLoss: 0.6881, Accuracy 74.55%\n",
      "\n",
      "Test Batch [454]:\n",
      "\tLoss: 0.5944, Accuracy 74.55%\n",
      "\n",
      "Test Batch [455]:\n",
      "\tLoss: 0.6346, Accuracy 74.54%\n",
      "\n",
      "Test Batch [456]:\n",
      "\tLoss: 0.6479, Accuracy 74.53%\n",
      "\n",
      "Test Batch [457]:\n",
      "\tLoss: 0.6211, Accuracy 74.52%\n",
      "\n",
      "Test Batch [458]:\n",
      "\tLoss: 0.6213, Accuracy 74.51%\n",
      "\n",
      "Test Batch [459]:\n",
      "\tLoss: 0.6080, Accuracy 74.51%\n",
      "\n",
      "Test Batch [460]:\n",
      "\tLoss: 0.5408, Accuracy 74.51%\n",
      "\n",
      "Test Batch [461]:\n",
      "\tLoss: 0.5142, Accuracy 74.52%\n",
      "\n",
      "Test Batch [462]:\n",
      "\tLoss: 0.5007, Accuracy 74.53%\n",
      "\n",
      "Test Batch [463]:\n",
      "\tLoss: 0.5406, Accuracy 74.54%\n",
      "\n",
      "Test Batch [464]:\n",
      "\tLoss: 0.6078, Accuracy 74.53%\n",
      "\n",
      "Test Batch [465]:\n",
      "\tLoss: 0.6612, Accuracy 74.52%\n",
      "\n",
      "Test Batch [466]:\n",
      "\tLoss: 0.5408, Accuracy 74.53%\n",
      "\n",
      "Test Batch [467]:\n",
      "\tLoss: 0.4871, Accuracy 74.54%\n",
      "\n",
      "Test Batch [468]:\n",
      "\tLoss: 0.6611, Accuracy 74.53%\n",
      "\n",
      "Test Batch [469]:\n",
      "\tLoss: 0.6748, Accuracy 74.51%\n",
      "\n",
      "Test Batch [470]:\n",
      "\tLoss: 0.5274, Accuracy 74.52%\n",
      "\n",
      "Test Batch [471]:\n",
      "\tLoss: 0.5274, Accuracy 74.52%\n",
      "\n",
      "Test Batch [472]:\n",
      "\tLoss: 0.5004, Accuracy 74.54%\n",
      "\n",
      "Test Batch [473]:\n",
      "\tLoss: 0.5676, Accuracy 74.54%\n",
      "\n",
      "Test Batch [474]:\n",
      "\tLoss: 0.5944, Accuracy 74.53%\n",
      "\n",
      "Test Batch [475]:\n",
      "\tLoss: 0.7150, Accuracy 74.51%\n",
      "\n",
      "Test Batch [476]:\n",
      "\tLoss: 0.6072, Accuracy 74.51%\n",
      "\n",
      "Test Batch [477]:\n",
      "\tLoss: 0.6213, Accuracy 74.50%\n",
      "\n",
      "Test Batch [478]:\n",
      "\tLoss: 0.5539, Accuracy 74.50%\n",
      "\n",
      "Test Batch [479]:\n",
      "\tLoss: 0.5809, Accuracy 74.50%\n",
      "\n",
      "Test Batch [480]:\n",
      "\tLoss: 0.6351, Accuracy 74.49%\n",
      "\n",
      "Test Batch [481]:\n",
      "\tLoss: 0.5406, Accuracy 74.50%\n",
      "\n",
      "Test Batch [482]:\n",
      "\tLoss: 0.6219, Accuracy 74.49%\n",
      "\n",
      "Test Batch [483]:\n",
      "\tLoss: 0.6211, Accuracy 74.48%\n",
      "\n",
      "Test Batch [484]:\n",
      "\tLoss: 0.5674, Accuracy 74.48%\n",
      "\n",
      "Test Batch [485]:\n",
      "\tLoss: 0.6875, Accuracy 74.47%\n",
      "\n",
      "Test Batch [486]:\n",
      "\tLoss: 0.5407, Accuracy 74.47%\n",
      "\n",
      "Test Batch [487]:\n",
      "\tLoss: 0.5818, Accuracy 74.47%\n",
      "\n",
      "Test Batch [488]:\n",
      "\tLoss: 0.6347, Accuracy 74.46%\n",
      "\n",
      "Test Batch [489]:\n",
      "\tLoss: 0.6883, Accuracy 74.44%\n",
      "\n",
      "Test Batch [490]:\n",
      "\tLoss: 0.5941, Accuracy 74.44%\n",
      "\n",
      "Test Batch [491]:\n",
      "\tLoss: 0.6610, Accuracy 74.43%\n",
      "\n",
      "Test Batch [492]:\n",
      "\tLoss: 0.5140, Accuracy 74.44%\n",
      "\n",
      "Test Batch [493]:\n",
      "\tLoss: 0.4739, Accuracy 74.45%\n",
      "\n",
      "Test Batch [494]:\n",
      "\tLoss: 0.5809, Accuracy 74.45%\n",
      "\n",
      "Test Batch [495]:\n",
      "\tLoss: 0.5944, Accuracy 74.45%\n",
      "\n",
      "Test Batch [496]:\n",
      "\tLoss: 0.4739, Accuracy 74.46%\n",
      "\n",
      "Test Batch [497]:\n",
      "\tLoss: 0.5542, Accuracy 74.47%\n",
      "\n",
      "Test Batch [498]:\n",
      "\tLoss: 0.5541, Accuracy 74.47%\n",
      "\n",
      "Test Batch [499]:\n",
      "\tLoss: 0.5675, Accuracy 74.47%\n",
      "\n",
      "Test Batch [500]:\n",
      "\tLoss: 0.5002, Accuracy 74.48%\n",
      "\n",
      "Test Batch [501]:\n",
      "\tLoss: 0.5408, Accuracy 74.49%\n",
      "\n",
      "Test Batch [502]:\n",
      "\tLoss: 0.5139, Accuracy 74.50%\n",
      "\n",
      "Test Batch [503]:\n",
      "\tLoss: 0.4871, Accuracy 74.51%\n",
      "\n",
      "Test Batch [504]:\n",
      "\tLoss: 0.6480, Accuracy 74.50%\n",
      "\n",
      "Test Batch [505]:\n",
      "\tLoss: 0.5943, Accuracy 74.50%\n",
      "\n",
      "Test Batch [506]:\n",
      "\tLoss: 0.4606, Accuracy 74.51%\n",
      "\n",
      "Test Batch [507]:\n",
      "\tLoss: 0.5139, Accuracy 74.52%\n",
      "\n",
      "Test Batch [508]:\n",
      "\tLoss: 0.5409, Accuracy 74.53%\n",
      "\n",
      "Test Batch [509]:\n",
      "\tLoss: 0.4871, Accuracy 74.54%\n",
      "\n",
      "Test Batch [510]:\n",
      "\tLoss: 0.5409, Accuracy 74.54%\n",
      "\n",
      "Test Batch [511]:\n",
      "\tLoss: 0.6079, Accuracy 74.54%\n",
      "\n",
      "Test Batch [512]:\n",
      "\tLoss: 0.4739, Accuracy 74.55%\n",
      "\n",
      "Test Batch [513]:\n",
      "\tLoss: 0.5537, Accuracy 74.56%\n",
      "\n",
      "Test Batch [514]:\n",
      "\tLoss: 0.5143, Accuracy 74.56%\n",
      "\n",
      "Test Batch [515]:\n",
      "\tLoss: 0.5673, Accuracy 74.57%\n",
      "\n",
      "Test Batch [516]:\n",
      "\tLoss: 0.6481, Accuracy 74.55%\n",
      "\n",
      "Test Batch [517]:\n",
      "\tLoss: 0.5808, Accuracy 74.55%\n",
      "\n",
      "Test Batch [518]:\n",
      "\tLoss: 0.5677, Accuracy 74.55%\n",
      "\n",
      "Test Batch [519]:\n",
      "\tLoss: 0.5143, Accuracy 74.56%\n",
      "\n",
      "Test Batch [520]:\n",
      "\tLoss: 0.5543, Accuracy 74.57%\n",
      "\n",
      "Test Batch [521]:\n",
      "\tLoss: 0.6614, Accuracy 74.55%\n",
      "\n",
      "Test Batch [522]:\n",
      "\tLoss: 0.5673, Accuracy 74.55%\n",
      "\n",
      "Test Batch [523]:\n",
      "\tLoss: 0.5140, Accuracy 74.56%\n",
      "\n",
      "Test Batch [524]:\n",
      "\tLoss: 0.6879, Accuracy 74.55%\n",
      "\n",
      "Test Batch [525]:\n",
      "\tLoss: 0.6480, Accuracy 74.54%\n",
      "\n",
      "Test Batch [526]:\n",
      "\tLoss: 0.6345, Accuracy 74.53%\n",
      "\n",
      "Test Batch [527]:\n",
      "\tLoss: 0.5943, Accuracy 74.52%\n",
      "\n",
      "Test Batch [528]:\n",
      "\tLoss: 0.4872, Accuracy 74.54%\n",
      "\n",
      "Test Batch [529]:\n",
      "\tLoss: 0.5671, Accuracy 74.54%\n",
      "\n",
      "Test Batch [530]:\n",
      "\tLoss: 0.5143, Accuracy 74.55%\n",
      "\n",
      "Test Batch [531]:\n",
      "\tLoss: 0.5274, Accuracy 74.55%\n",
      "\n",
      "Test Batch [532]:\n",
      "\tLoss: 0.5677, Accuracy 74.55%\n",
      "\n",
      "Test Batch [533]:\n",
      "\tLoss: 0.5411, Accuracy 74.56%\n",
      "\n",
      "Test Batch [534]:\n",
      "\tLoss: 0.5542, Accuracy 74.56%\n",
      "\n",
      "Test Batch [535]:\n",
      "\tLoss: 0.6213, Accuracy 74.55%\n",
      "\n",
      "Test Batch [536]:\n",
      "\tLoss: 0.5942, Accuracy 74.55%\n",
      "\n",
      "Test Batch [537]:\n",
      "\tLoss: 0.6347, Accuracy 74.54%\n",
      "\n",
      "Test Batch [538]:\n",
      "\tLoss: 0.5811, Accuracy 74.54%\n",
      "\n",
      "Test Batch [539]:\n",
      "\tLoss: 0.5542, Accuracy 74.54%\n",
      "\n",
      "Test Batch [540]:\n",
      "\tLoss: 0.4871, Accuracy 74.56%\n",
      "\n",
      "Test Batch [541]:\n",
      "\tLoss: 0.5676, Accuracy 74.56%\n",
      "\n",
      "Test Batch [542]:\n",
      "\tLoss: 0.5407, Accuracy 74.56%\n",
      "\n",
      "Test Batch [543]:\n",
      "\tLoss: 0.5809, Accuracy 74.56%\n",
      "\n",
      "Test Batch [544]:\n",
      "\tLoss: 0.5943, Accuracy 74.56%\n",
      "\n",
      "Test Batch [545]:\n",
      "\tLoss: 0.5679, Accuracy 74.56%\n",
      "\n",
      "Test Batch [546]:\n",
      "\tLoss: 0.6745, Accuracy 74.54%\n",
      "\n",
      "Test Batch [547]:\n",
      "\tLoss: 0.4870, Accuracy 74.56%\n",
      "\n",
      "Test Batch [548]:\n",
      "\tLoss: 0.6482, Accuracy 74.55%\n",
      "\n",
      "Test Batch [549]:\n",
      "\tLoss: 0.5140, Accuracy 74.55%\n",
      "\n",
      "Test Batch [550]:\n",
      "\tLoss: 0.6344, Accuracy 74.55%\n",
      "\n",
      "Test Batch [551]:\n",
      "\tLoss: 0.5673, Accuracy 74.55%\n",
      "\n",
      "Test Batch [552]:\n",
      "\tLoss: 0.5675, Accuracy 74.55%\n",
      "\n",
      "Test Batch [553]:\n",
      "\tLoss: 0.5140, Accuracy 74.56%\n",
      "\n",
      "Test Batch [554]:\n",
      "\tLoss: 0.5408, Accuracy 74.56%\n",
      "\n",
      "Test Batch [555]:\n",
      "\tLoss: 0.5541, Accuracy 74.56%\n",
      "\n",
      "Test Batch [556]:\n",
      "\tLoss: 0.4739, Accuracy 74.58%\n",
      "\n",
      "Test Batch [557]:\n",
      "\tLoss: 0.5543, Accuracy 74.58%\n",
      "\n",
      "Test Batch [558]:\n",
      "\tLoss: 0.6346, Accuracy 74.57%\n",
      "\n",
      "Test Batch [559]:\n",
      "\tLoss: 0.5274, Accuracy 74.58%\n",
      "\n",
      "Test Batch [560]:\n",
      "\tLoss: 0.5810, Accuracy 74.58%\n",
      "\n",
      "Test Batch [561]:\n",
      "\tLoss: 0.7015, Accuracy 74.56%\n",
      "\n",
      "Test Batch [562]:\n",
      "\tLoss: 0.6211, Accuracy 74.55%\n",
      "\n",
      "Test Batch [563]:\n",
      "\tLoss: 0.5274, Accuracy 74.56%\n",
      "\n",
      "Test Batch [564]:\n",
      "\tLoss: 0.5675, Accuracy 74.56%\n",
      "\n",
      "Test Batch [565]:\n",
      "\tLoss: 0.5672, Accuracy 74.56%\n",
      "\n",
      "Test Batch [566]:\n",
      "\tLoss: 0.7414, Accuracy 74.54%\n",
      "\n",
      "Test Batch [567]:\n",
      "\tLoss: 0.5541, Accuracy 74.54%\n",
      "\n",
      "Test Batch [568]:\n",
      "\tLoss: 0.5142, Accuracy 74.55%\n",
      "\n",
      "Test Batch [569]:\n",
      "\tLoss: 0.6078, Accuracy 74.54%\n",
      "\n",
      "Test Batch [570]:\n",
      "\tLoss: 0.5676, Accuracy 74.54%\n",
      "\n",
      "Test Batch [571]:\n",
      "\tLoss: 0.4470, Accuracy 74.56%\n",
      "\n",
      "Test Batch [572]:\n",
      "\tLoss: 0.5676, Accuracy 74.56%\n",
      "\n",
      "Test Batch [573]:\n",
      "\tLoss: 0.5409, Accuracy 74.57%\n",
      "\n",
      "Test Batch [574]:\n",
      "\tLoss: 0.6345, Accuracy 74.56%\n",
      "\n",
      "Test Batch [575]:\n",
      "\tLoss: 0.5009, Accuracy 74.57%\n",
      "\n",
      "Test Batch [576]:\n",
      "\tLoss: 0.5538, Accuracy 74.57%\n",
      "\n",
      "Test Batch [577]:\n",
      "\tLoss: 0.5006, Accuracy 74.58%\n",
      "\n",
      "Test Batch [578]:\n",
      "\tLoss: 0.5944, Accuracy 74.58%\n",
      "\n",
      "Test Batch [579]:\n",
      "\tLoss: 0.5814, Accuracy 74.58%\n",
      "\n",
      "Test Batch [580]:\n",
      "\tLoss: 0.5542, Accuracy 74.58%\n",
      "\n",
      "Test Batch [581]:\n",
      "\tLoss: 0.6747, Accuracy 74.56%\n",
      "\n",
      "Test Batch [582]:\n",
      "\tLoss: 0.5542, Accuracy 74.57%\n",
      "\n",
      "Test Batch [583]:\n",
      "\tLoss: 0.5810, Accuracy 74.57%\n",
      "\n",
      "Test Batch [584]:\n",
      "\tLoss: 0.5944, Accuracy 74.56%\n",
      "\n",
      "Test Batch [585]:\n",
      "\tLoss: 0.5412, Accuracy 74.57%\n",
      "\n",
      "Test Batch [586]:\n",
      "\tLoss: 0.6078, Accuracy 74.56%\n",
      "\n",
      "Test Batch [587]:\n",
      "\tLoss: 0.5273, Accuracy 74.57%\n",
      "\n",
      "Test Batch [588]:\n",
      "\tLoss: 0.5270, Accuracy 74.57%\n",
      "\n",
      "Test Batch [589]:\n",
      "\tLoss: 0.6611, Accuracy 74.56%\n",
      "\n",
      "Test Batch [590]:\n",
      "\tLoss: 0.5139, Accuracy 74.57%\n",
      "\n",
      "Test Batch [591]:\n",
      "\tLoss: 0.5811, Accuracy 74.57%\n",
      "\n",
      "Test Batch [592]:\n",
      "\tLoss: 0.6346, Accuracy 74.56%\n",
      "\n",
      "Test Batch [593]:\n",
      "\tLoss: 0.5409, Accuracy 74.57%\n",
      "\n",
      "Test Batch [594]:\n",
      "\tLoss: 0.5140, Accuracy 74.57%\n",
      "\n",
      "Test Batch [595]:\n",
      "\tLoss: 0.5677, Accuracy 74.57%\n",
      "\n",
      "Test Batch [596]:\n",
      "\tLoss: 0.6344, Accuracy 74.57%\n",
      "\n",
      "Test Batch [597]:\n",
      "\tLoss: 0.6213, Accuracy 74.56%\n",
      "\n",
      "Test Batch [598]:\n",
      "\tLoss: 0.4467, Accuracy 74.58%\n",
      "\n",
      "Test Batch [599]:\n",
      "\tLoss: 0.5944, Accuracy 74.57%\n",
      "\n",
      "Test Batch [600]:\n",
      "\tLoss: 0.5677, Accuracy 74.58%\n",
      "\n",
      "Test Batch [601]:\n",
      "\tLoss: 0.6613, Accuracy 74.56%\n",
      "\n",
      "Test Batch [602]:\n",
      "\tLoss: 0.5006, Accuracy 74.57%\n",
      "\n",
      "Test Batch [603]:\n",
      "\tLoss: 0.6612, Accuracy 74.56%\n",
      "\n",
      "Test Batch [604]:\n",
      "\tLoss: 0.5942, Accuracy 74.56%\n",
      "\n",
      "Test Batch [605]:\n",
      "\tLoss: 0.6345, Accuracy 74.55%\n",
      "\n",
      "Test Batch [606]:\n",
      "\tLoss: 0.4872, Accuracy 74.56%\n",
      "\n",
      "Test Batch [607]:\n",
      "\tLoss: 0.4604, Accuracy 74.58%\n",
      "\n",
      "Test Batch [608]:\n",
      "\tLoss: 0.5141, Accuracy 74.58%\n",
      "\n",
      "Test Batch [609]:\n",
      "\tLoss: 0.5811, Accuracy 74.58%\n",
      "\n",
      "Test Batch [610]:\n",
      "\tLoss: 0.6209, Accuracy 74.58%\n",
      "\n",
      "Test Batch [611]:\n",
      "\tLoss: 0.6075, Accuracy 74.57%\n",
      "\n",
      "Test Batch [612]:\n",
      "\tLoss: 0.5140, Accuracy 74.58%\n",
      "\n",
      "Test Batch [613]:\n",
      "\tLoss: 0.5538, Accuracy 74.58%\n",
      "\n",
      "Test Batch [614]:\n",
      "\tLoss: 0.4738, Accuracy 74.59%\n",
      "\n",
      "Test Batch [615]:\n",
      "\tLoss: 0.6079, Accuracy 74.59%\n",
      "\n",
      "Test Batch [616]:\n",
      "\tLoss: 0.5005, Accuracy 74.60%\n",
      "\n",
      "Test Batch [617]:\n",
      "\tLoss: 0.5809, Accuracy 74.60%\n",
      "\n",
      "Test Batch [618]:\n",
      "\tLoss: 0.5542, Accuracy 74.60%\n",
      "\n",
      "Test Batch [619]:\n",
      "\tLoss: 0.5410, Accuracy 74.60%\n",
      "\n",
      "Test Batch [620]:\n",
      "\tLoss: 0.5275, Accuracy 74.61%\n",
      "\n",
      "Test Batch [621]:\n",
      "\tLoss: 0.5675, Accuracy 74.61%\n",
      "\n",
      "Test Batch [622]:\n",
      "\tLoss: 0.6746, Accuracy 74.60%\n",
      "\n",
      "Test Batch [623]:\n",
      "\tLoss: 0.7153, Accuracy 74.58%\n",
      "\n",
      "Test Batch [624]:\n",
      "\tLoss: 0.5936, Accuracy 74.58%\n",
      "\n",
      "Test Batch [625]:\n",
      "\tLoss: 0.5411, Accuracy 74.58%\n",
      "\n",
      "Test Batch [626]:\n",
      "\tLoss: 0.5808, Accuracy 74.58%\n",
      "\n",
      "Test Batch [627]:\n",
      "\tLoss: 0.6211, Accuracy 74.58%\n",
      "\n",
      "Test Batch [628]:\n",
      "\tLoss: 0.5679, Accuracy 74.58%\n",
      "\n",
      "Test Batch [629]:\n",
      "\tLoss: 0.5539, Accuracy 74.58%\n",
      "\n",
      "Test Batch [630]:\n",
      "\tLoss: 0.6076, Accuracy 74.57%\n",
      "\n",
      "Test Batch [631]:\n",
      "\tLoss: 0.6879, Accuracy 74.56%\n",
      "\n",
      "Test Batch [632]:\n",
      "\tLoss: 0.5544, Accuracy 74.56%\n",
      "\n",
      "Test Batch [633]:\n",
      "\tLoss: 0.5409, Accuracy 74.57%\n",
      "\n",
      "Test Batch [634]:\n",
      "\tLoss: 0.6077, Accuracy 74.56%\n",
      "\n",
      "Test Batch [635]:\n",
      "\tLoss: 0.5272, Accuracy 74.57%\n",
      "\n",
      "Test Batch [636]:\n",
      "\tLoss: 0.5272, Accuracy 74.57%\n",
      "\n",
      "Test Batch [637]:\n",
      "\tLoss: 0.5944, Accuracy 74.57%\n",
      "\n",
      "Test Batch [638]:\n",
      "\tLoss: 0.5544, Accuracy 74.57%\n",
      "\n",
      "Test Batch [639]:\n",
      "\tLoss: 0.5942, Accuracy 74.57%\n",
      "\n",
      "Test Batch [640]:\n",
      "\tLoss: 0.5406, Accuracy 74.58%\n",
      "\n",
      "Test Batch [641]:\n",
      "\tLoss: 0.5810, Accuracy 74.57%\n",
      "\n",
      "Test Batch [642]:\n",
      "\tLoss: 0.6348, Accuracy 74.57%\n",
      "\n",
      "Test Batch [643]:\n",
      "\tLoss: 0.5943, Accuracy 74.56%\n",
      "\n",
      "Test Batch [644]:\n",
      "\tLoss: 0.5941, Accuracy 74.56%\n",
      "\n",
      "Test Batch [645]:\n",
      "\tLoss: 0.6077, Accuracy 74.56%\n",
      "\n",
      "Test Batch [646]:\n",
      "\tLoss: 0.5674, Accuracy 74.56%\n",
      "\n",
      "Test Batch [647]:\n",
      "\tLoss: 0.5542, Accuracy 74.56%\n",
      "\n",
      "Test Batch [648]:\n",
      "\tLoss: 0.6079, Accuracy 74.56%\n",
      "\n",
      "Test Batch [649]:\n",
      "\tLoss: 0.5674, Accuracy 74.56%\n",
      "\n",
      "Test Batch [650]:\n",
      "\tLoss: 0.5140, Accuracy 74.56%\n",
      "\n",
      "Test Batch [651]:\n",
      "\tLoss: 0.6610, Accuracy 74.55%\n",
      "\n",
      "Test Batch [652]:\n",
      "\tLoss: 0.6211, Accuracy 74.55%\n",
      "\n",
      "Test Batch [653]:\n",
      "\tLoss: 0.5274, Accuracy 74.55%\n",
      "\n",
      "Test Batch [654]:\n",
      "\tLoss: 0.5140, Accuracy 74.56%\n",
      "\n",
      "Test Batch [655]:\n",
      "\tLoss: 0.6615, Accuracy 74.55%\n",
      "\n",
      "Test Batch [656]:\n",
      "\tLoss: 0.5274, Accuracy 74.56%\n",
      "\n",
      "Test Batch [657]:\n",
      "\tLoss: 0.5542, Accuracy 74.56%\n",
      "\n",
      "Test Batch [658]:\n",
      "\tLoss: 0.5672, Accuracy 74.56%\n",
      "\n",
      "Test Batch [659]:\n",
      "\tLoss: 0.5002, Accuracy 74.57%\n",
      "\n",
      "Test Batch [660]:\n",
      "\tLoss: 0.6346, Accuracy 74.56%\n",
      "\n",
      "Test Batch [661]:\n",
      "\tLoss: 0.7417, Accuracy 74.54%\n",
      "\n",
      "Test Batch [662]:\n",
      "\tLoss: 0.5945, Accuracy 74.54%\n",
      "\n",
      "Test Batch [663]:\n",
      "\tLoss: 0.6477, Accuracy 74.53%\n",
      "\n",
      "Test Batch [664]:\n",
      "\tLoss: 0.5542, Accuracy 74.53%\n",
      "\n",
      "Test Batch [665]:\n",
      "\tLoss: 0.5810, Accuracy 74.53%\n",
      "\n",
      "Test Batch [666]:\n",
      "\tLoss: 0.7151, Accuracy 74.52%\n",
      "\n",
      "Test Batch [667]:\n",
      "\tLoss: 0.5944, Accuracy 74.51%\n",
      "\n",
      "Test Batch [668]:\n",
      "\tLoss: 0.6612, Accuracy 74.50%\n",
      "\n",
      "Test Batch [669]:\n",
      "\tLoss: 0.5006, Accuracy 74.51%\n",
      "\n",
      "Test Batch [670]:\n",
      "\tLoss: 0.5406, Accuracy 74.52%\n",
      "\n",
      "Test Batch [671]:\n",
      "\tLoss: 0.6213, Accuracy 74.51%\n",
      "\n",
      "Test Batch [672]:\n",
      "\tLoss: 0.5672, Accuracy 74.51%\n",
      "\n",
      "Test Batch [673]:\n",
      "\tLoss: 0.5141, Accuracy 74.52%\n",
      "\n",
      "Test Batch [674]:\n",
      "\tLoss: 0.6748, Accuracy 74.51%\n",
      "\n",
      "Test Batch [675]:\n",
      "\tLoss: 0.6211, Accuracy 74.50%\n",
      "\n",
      "Test Batch [676]:\n",
      "\tLoss: 0.5944, Accuracy 74.50%\n",
      "\n",
      "Test Batch [677]:\n",
      "\tLoss: 0.6077, Accuracy 74.50%\n",
      "\n",
      "Test Batch [678]:\n",
      "\tLoss: 0.5811, Accuracy 74.50%\n",
      "\n",
      "Test Batch [679]:\n",
      "\tLoss: 0.6078, Accuracy 74.49%\n",
      "\n",
      "Test Batch [680]:\n",
      "\tLoss: 0.5675, Accuracy 74.49%\n",
      "\n",
      "Test Batch [681]:\n",
      "\tLoss: 0.4739, Accuracy 74.50%\n",
      "\n",
      "Test Batch [682]:\n",
      "\tLoss: 0.6212, Accuracy 74.50%\n",
      "\n",
      "Test Batch [683]:\n",
      "\tLoss: 0.5137, Accuracy 74.51%\n",
      "\n",
      "Test Batch [684]:\n",
      "\tLoss: 0.5541, Accuracy 74.51%\n",
      "\n",
      "Test Batch [685]:\n",
      "\tLoss: 0.4735, Accuracy 74.52%\n",
      "\n",
      "Test Batch [686]:\n",
      "\tLoss: 0.5409, Accuracy 74.52%\n",
      "\n",
      "Test Batch [687]:\n",
      "\tLoss: 0.6208, Accuracy 74.52%\n",
      "\n",
      "Test Batch [688]:\n",
      "\tLoss: 0.5274, Accuracy 74.52%\n",
      "\n",
      "Test Batch [689]:\n",
      "\tLoss: 0.5001, Accuracy 74.53%\n",
      "\n",
      "Test Batch [690]:\n",
      "\tLoss: 0.5273, Accuracy 74.53%\n",
      "\n",
      "Test Batch [691]:\n",
      "\tLoss: 0.6613, Accuracy 74.53%\n",
      "\n",
      "Test Batch [692]:\n",
      "\tLoss: 0.5274, Accuracy 74.53%\n",
      "\n",
      "Test Batch [693]:\n",
      "\tLoss: 0.5415, Accuracy 74.53%\n",
      "\n",
      "Test Batch [694]:\n",
      "\tLoss: 0.5810, Accuracy 74.53%\n",
      "\n",
      "Test Batch [695]:\n",
      "\tLoss: 0.5675, Accuracy 74.53%\n",
      "\n",
      "Test Batch [696]:\n",
      "\tLoss: 0.5138, Accuracy 74.54%\n",
      "\n",
      "Test Batch [697]:\n",
      "\tLoss: 0.5542, Accuracy 74.54%\n",
      "\n",
      "Test Batch [698]:\n",
      "\tLoss: 0.6479, Accuracy 74.53%\n",
      "\n",
      "Test Batch [699]:\n",
      "\tLoss: 0.4875, Accuracy 74.54%\n",
      "\n",
      "Test Batch [700]:\n",
      "\tLoss: 0.5811, Accuracy 74.54%\n",
      "\n",
      "Test Batch [701]:\n",
      "\tLoss: 0.6746, Accuracy 74.53%\n",
      "\n",
      "Test Batch [702]:\n",
      "\tLoss: 0.5808, Accuracy 74.53%\n",
      "\n",
      "Test Batch [703]:\n",
      "\tLoss: 0.6613, Accuracy 74.52%\n",
      "\n",
      "Test Batch [704]:\n",
      "\tLoss: 0.5540, Accuracy 74.52%\n",
      "\n",
      "Test Batch [705]:\n",
      "\tLoss: 0.7017, Accuracy 74.51%\n",
      "\n",
      "Test Batch [706]:\n",
      "\tLoss: 0.6617, Accuracy 74.50%\n",
      "\n",
      "Test Batch [707]:\n",
      "\tLoss: 0.5676, Accuracy 74.50%\n",
      "\n",
      "Test Batch [708]:\n",
      "\tLoss: 0.5541, Accuracy 74.50%\n",
      "\n",
      "Test Batch [709]:\n",
      "\tLoss: 0.4738, Accuracy 74.51%\n",
      "\n",
      "Test Batch [710]:\n",
      "\tLoss: 0.5138, Accuracy 74.52%\n",
      "\n",
      "Test Batch [711]:\n",
      "\tLoss: 0.6480, Accuracy 74.51%\n",
      "\n",
      "Test Batch [712]:\n",
      "\tLoss: 0.5543, Accuracy 74.52%\n",
      "\n",
      "Test Batch [713]:\n",
      "\tLoss: 0.5544, Accuracy 74.52%\n",
      "\n",
      "Test Batch [714]:\n",
      "\tLoss: 0.5943, Accuracy 74.52%\n",
      "\n",
      "Test Batch [715]:\n",
      "\tLoss: 0.5810, Accuracy 74.51%\n",
      "\n",
      "Test Batch [716]:\n",
      "\tLoss: 0.5943, Accuracy 74.51%\n",
      "\n",
      "Test Batch [717]:\n",
      "\tLoss: 0.7286, Accuracy 74.50%\n",
      "\n",
      "Test Batch [718]:\n",
      "\tLoss: 0.5810, Accuracy 74.50%\n",
      "\n",
      "Test Batch [719]:\n",
      "\tLoss: 0.5809, Accuracy 74.50%\n",
      "\n",
      "Test Batch [720]:\n",
      "\tLoss: 0.5676, Accuracy 74.50%\n",
      "\n",
      "Test Batch [721]:\n",
      "\tLoss: 0.4872, Accuracy 74.50%\n",
      "\n",
      "Test Batch [722]:\n",
      "\tLoss: 0.5410, Accuracy 74.51%\n",
      "\n",
      "Test Batch [723]:\n",
      "\tLoss: 0.5676, Accuracy 74.51%\n",
      "\n",
      "Test Batch [724]:\n",
      "\tLoss: 0.5274, Accuracy 74.51%\n",
      "\n",
      "Test Batch [725]:\n",
      "\tLoss: 0.6480, Accuracy 74.51%\n",
      "\n",
      "Test Batch [726]:\n",
      "\tLoss: 0.6748, Accuracy 74.50%\n",
      "\n",
      "Test Batch [727]:\n",
      "\tLoss: 0.6082, Accuracy 74.49%\n",
      "\n",
      "Test Batch [728]:\n",
      "\tLoss: 0.5543, Accuracy 74.49%\n",
      "\n",
      "Test Batch [729]:\n",
      "\tLoss: 0.6614, Accuracy 74.49%\n",
      "\n",
      "Test Batch [730]:\n",
      "\tLoss: 0.5272, Accuracy 74.49%\n",
      "\n",
      "Test Batch [731]:\n",
      "\tLoss: 0.4738, Accuracy 74.50%\n",
      "\n",
      "Test Batch [732]:\n",
      "\tLoss: 0.5543, Accuracy 74.50%\n",
      "\n",
      "Test Batch [733]:\n",
      "\tLoss: 0.5541, Accuracy 74.50%\n",
      "\n",
      "Test Batch [734]:\n",
      "\tLoss: 0.6748, Accuracy 74.49%\n",
      "\n",
      "Test Batch [735]:\n",
      "\tLoss: 0.5138, Accuracy 74.50%\n",
      "\n",
      "Test Batch [736]:\n",
      "\tLoss: 0.4872, Accuracy 74.51%\n",
      "\n",
      "Test Batch [737]:\n",
      "\tLoss: 0.5542, Accuracy 74.51%\n",
      "\n",
      "Test Batch [738]:\n",
      "\tLoss: 0.5542, Accuracy 74.51%\n",
      "\n",
      "Test Batch [739]:\n",
      "\tLoss: 0.5809, Accuracy 74.51%\n",
      "\n",
      "Test Batch [740]:\n",
      "\tLoss: 0.5541, Accuracy 74.51%\n",
      "\n",
      "Test Batch [741]:\n",
      "\tLoss: 0.5141, Accuracy 74.52%\n",
      "\n",
      "Test Batch [742]:\n",
      "\tLoss: 0.5541, Accuracy 74.52%\n",
      "\n",
      "Test Batch [743]:\n",
      "\tLoss: 0.5005, Accuracy 74.53%\n",
      "\n",
      "Test Batch [744]:\n",
      "\tLoss: 0.4872, Accuracy 74.54%\n",
      "\n",
      "Test Batch [745]:\n",
      "\tLoss: 0.5542, Accuracy 74.54%\n",
      "\n",
      "Test Batch [746]:\n",
      "\tLoss: 0.5543, Accuracy 74.54%\n",
      "\n",
      "Test Batch [747]:\n",
      "\tLoss: 0.5007, Accuracy 74.55%\n",
      "\n",
      "Test Batch [748]:\n",
      "\tLoss: 0.6078, Accuracy 74.55%\n",
      "\n",
      "Test Batch [749]:\n",
      "\tLoss: 0.5141, Accuracy 74.55%\n",
      "\n",
      "Test Batch [750]:\n",
      "\tLoss: 0.6614, Accuracy 74.54%\n",
      "\n",
      "Test Batch [751]:\n",
      "\tLoss: 0.5140, Accuracy 74.55%\n",
      "\n",
      "Test Batch [752]:\n",
      "\tLoss: 0.5274, Accuracy 74.55%\n",
      "\n",
      "Test Batch [753]:\n",
      "\tLoss: 0.5812, Accuracy 74.55%\n",
      "\n",
      "Test Batch [754]:\n",
      "\tLoss: 0.5678, Accuracy 74.55%\n",
      "\n",
      "Test Batch [755]:\n",
      "\tLoss: 0.6076, Accuracy 74.55%\n",
      "\n",
      "Test Batch [756]:\n",
      "\tLoss: 0.5542, Accuracy 74.55%\n",
      "\n",
      "Test Batch [757]:\n",
      "\tLoss: 0.5275, Accuracy 74.56%\n",
      "\n",
      "Test Batch [758]:\n",
      "\tLoss: 0.5275, Accuracy 74.56%\n",
      "\n",
      "Test Batch [759]:\n",
      "\tLoss: 0.6076, Accuracy 74.56%\n",
      "\n",
      "Test Batch [760]:\n",
      "\tLoss: 0.5003, Accuracy 74.57%\n",
      "\n",
      "Test Batch [761]:\n",
      "\tLoss: 0.5408, Accuracy 74.57%\n",
      "\n",
      "Test Batch [762]:\n",
      "\tLoss: 0.5542, Accuracy 74.57%\n",
      "\n",
      "Test Batch [763]:\n",
      "\tLoss: 0.5676, Accuracy 74.57%\n",
      "\n",
      "Test Batch [764]:\n",
      "\tLoss: 0.5408, Accuracy 74.57%\n",
      "\n",
      "Test Batch [765]:\n",
      "\tLoss: 0.5411, Accuracy 74.58%\n",
      "\n",
      "Test Batch [766]:\n",
      "\tLoss: 0.6345, Accuracy 74.57%\n",
      "\n",
      "Test Batch [767]:\n",
      "\tLoss: 0.5946, Accuracy 74.57%\n",
      "\n",
      "Test Batch [768]:\n",
      "\tLoss: 0.5141, Accuracy 74.58%\n",
      "\n",
      "After [769] Batches:\n",
      "\tAverage Test Loss: 0.5725, Accuracy: 74.58%\n",
      "  Over 76800 samples, 57274 were correct\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "list_of_accuracy, test_loss_list = evaluate_model(model, test_load, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notices:\n",
    "- Accuracy starts off quite high at $~79%$ and makes its way down till $74%$ and keeps fluctuating in first 2 decimal places.\n",
    "- The model seems to be able to generalise okay-ish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
