{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM From Scratch\n",
    "#### (By: Mark Ehab Aziz)\n",
    "##### (Built under Python 3.11.4)\n",
    "\n",
    "Graduation Project for ***Sprints***, to build a LLM (Large Language Model) from scratch without the use of any Transformer related libraries, in order to classify comments scraped from some given website.\n",
    "\n",
    "Notebook will include:\n",
    "- List of imported libraries.\n",
    "- Deep dive into the data.\n",
    "- Markdown cells to explain the detail of every step, along with reason.\n",
    "- Custom defined functions and classes.\n",
    "- Own Transformer from scratch.\n",
    "\n",
    "List of Libraries/Dependencies:\n",
    "- [Pandas](https://pandas.pydata.org/docs/index.html)\n",
    "- [NumPy](https://numpy.org/doc/stable/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - [NLTK Regex Tokenizer](https://www.nltk.org/howto/tokenize.html)\n",
    "    - [NLTK Snowball Stemmer](https://www.nltk.org/howto/stem.html)\n",
    "    - [NLTK WordNet Lemmatizer](https://www.nltk.org/howto/wordnet.html)\n",
    "    - [NLTK English Words (Stopwords too)](https://www.nltk.org/howto/corpus.html)\n",
    "- [PyTorch (`torch` and `torch.nn`)](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Note: NLTK will be used later for tokenization using RegEx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports\n",
    "Importing libraries that will be used in order to implement our own LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                             # Pandas for DataFrame manipulation\n",
    "import numpy as np                              # Linear Algebra and Mathematical Operations\n",
    "import nltk                                     # Downloading word sets\n",
    "from nltk.stem import SnowballStemmer           # Stemming\n",
    "from nltk.stem import WordNetLemmatizer         # Lemmatization (Better word yields)\n",
    "from nltk.corpus import stopwords               # Stopwords\n",
    "from nltk.tokenize import regexp_tokenize       # Tokenization using RegEx (Regular Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading external dependencies to be used\n",
    "# Such as Stopwords, English-only words\n",
    "\n",
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# English Only\n",
    "nltk.download('words')\n",
    "\n",
    "# Wordnet for Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setting constants for both normal and stopwords in English\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "ENGLISH_WORDS = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "Loading the data using Panda's `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data with 2 ways\n",
    "# 1 - Reading within my Github Repo\n",
    "txt_dat = pd.read_csv('../dataset/train.csv')\n",
    "\n",
    "# 2 - Reading within the same folder\n",
    "#txt_dat = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Looking at the data from multiple perspectives.\n",
    "\n",
    "Using `head()` and `tail()` methods to look at what columns there are within the dataframe, which may be useful and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining n rows\n",
    "n = 5\n",
    "\n",
    "# Calling and showing first and last n rows\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we can see that we have about $159571$ entries.\n",
    "We can also see that there are columns as such:\n",
    "- `id`: Shows message id code from whatever platform this was scraped from. (Will be dropped)\n",
    "- `comment_text`: main star of the show, text we have to process, it has upper-case letters, escape characters (`\\n`,`\\r`, etc.), and most likely special characters (non-latin-alphabet), URLs, IP addresses; removal needed.\n",
    "- `toxic` (And its derivatives): Labelling the comment if it were a toxic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining What To Clean\n",
    "In the above mentioned description of the data, there exists aspects that need removal, explanation will follow within this cell.\n",
    "\n",
    "- Removal of id: Due to the nature of what this project is about, and usually a general case, IDs are not usually if at all used to gain insights from data, due to them being unique per entry and being nothing but an enumeration of entries.\n",
    "- Cleaning within `comment_text`: Cleaning the comments will be separated into $x$ steps, namely:\n",
    "    - Space and Tab removal: Removing of Newline Characters (`\\n`) and Tabs (`\\t`) will help remove special characters from text and avoiding noise within.\n",
    "    - Uppercase: Due to standarisations within the NLP field, it has been agreed upon to change any uppercase letter into lowercase, to mitigate the face that words can be written using multiple permutations of the same letters but with different cases, so in order to for the machine to recognise the word and not have to account for $n^{52}$ different combinations for the word (Where $n$ is number of characters to represent a word, and $52$ is due to both upper and lower cases of a letter)\n",
    "    - URL: Removal of URLs will prove beneficial, as it doesn't contribute much to the corpus nor is considered a baseline for labelling the comment.\n",
    "    - IP Address: For security reasons.\n",
    "    - Special Characters: Due to the non-existence of any in the Latin Alphabet which English uses, it would be useless to bother with them, although if this was a multi-lingual dataset, some characters from different languages would be needed to keep.\n",
    "- Derivatives of `toxic`: For purposes of simplicity, I have decided to *\"collapse\"* the values that follow after the *`toxic`* column, as in summing the values into said column, then swapping values $>1$ to be just $1$ to indicate toxicity, implying that $0$ would be for non-toxic comments; as a result, this will water down into just a \"Binary Classification\" problem based on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsing the column values onto toxic\n",
    "txt_dat['toxic'] = txt_dat.iloc[:, 2:].sum(axis = 1)\n",
    "\n",
    "# Drop the collapsed columns\n",
    "# Along with the id column\n",
    "txt_dat.drop(columns = ['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the head and describing the toxic column\n",
    "display(txt_dat.head(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, due to collapsing the values onto `toxic`, values over 1 arose hence they will need handling in order to keep the labels as $1$ and $0$. A function doing just so will be implemented later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Patterns\n",
    "Identified Patterns that will be required to be used to capture specific instances of removable slices of text within the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newline, Tab spaces, etc.\n",
    "newline_tabspace = r'[\\r\\n\\t]'\n",
    "\n",
    "# Match words starting with Uppercase letters\n",
    "upper_words = r'([A-Z])\\w+'\n",
    "\n",
    "# Match Words that start with either Upper/lowercase letters\n",
    "upperlower_words = r'[A-Za-z]\\w+'\n",
    "\n",
    "# Sub/Superscript characters\n",
    "# Encountered previously\n",
    "sub_sup_scripts = r'\\w[²³¹⁰ⁱ⁴⁵⁶⁷⁸⁹⁺⁻⁼⁽⁾ⁿ]+'\n",
    "\n",
    "# Punctuation\n",
    "punc_pattern = r'[!\\?.,\\':;\"]'\n",
    "\n",
    "# Single Letters\n",
    "single_letter = r'((?<=^)|(?<= )).((?=$)|(?= ))'\n",
    "\n",
    "# Match URLs\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w+?\\.\\w+])+([a-zA-Z0-9\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)_\\-\\=\\+\\\\\\/\\?\\.\\:\\;\\'\\,]*)?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and such\n",
    "Functions to be used and applied on the dataframe.\n",
    "\n",
    "Each function will be commented, whilst also writing an explanation to a grouped cell of functions indicating their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df: pd.DataFrame, colname: str):\n",
    "    return np.where(df[colname] > 0, 1, 0)\n",
    "\n",
    "def clean_comment(df: pd.DataFrame, colname: str):\n",
    "    # Remove the newlines, tabs, etc.\n",
    "    df[colname].replace(newline_tabspace, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove URLs\n",
    "    df[colname].replace(url_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove superscript, subscript\n",
    "    df[colname].replace(sub_sup_scripts, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove punctuation\n",
    "    df[colname].replace(punc_pattern, ' ', regex = True, inplace = True)\n",
    "\n",
    "    # Remove Single Letters\n",
    "    df[colname].replace(single_letter, ' ', regex = True, inplace = True)\n",
    "\n",
    "    return df[colname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A walkthrough the above functions:\n",
    "- `binarize(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "Due to the rise of values larger than 1 after collapsing the other column values, a requirement for binarizing (Chose to make the labels binary, for simplicity's sake) the values, $1$ for Toxic and $0$ for Non-Toxic, this is achieved by assigning any value larger than 0 to 1 otherwise stays as 0.\n",
    "\n",
    "- `clean_comment(df, colname)`: Function that takes a `pd.DataFrame`, `string` for input, dictating which DataFrame and Column within said DataFrame to carry out the operations on.\n",
    "The nature of the data given, is that it has a lot of *whitespaces*, *tabs*, *special characters*, *urls*, *punctuations*, *single letters*, which are all bound to be removed.\n",
    "The function removes:\n",
    "    - Newlines\n",
    "    - Tabs\n",
    "    - URLs\n",
    "    - Sub/Super Scripts\n",
    "    - Punctuation\n",
    "    - Single Letters (From removal of some punctuation symbols)\n",
    "\n",
    "Both functions return the updated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dat['comment_text'] = clean_comment(txt_dat, 'comment_text')\n",
    "\n",
    "txt_dat['toxic'] = binarize(txt_dat, 'toxic')\n",
    "\n",
    "display(txt_dat.head(n), txt_dat.tail(n), txt_dat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no extra characters, like apostrophe's, extra punctuations, etc.\n",
    "\n",
    "As well as having the maximum value to be 1 within the `toxic` column, although the mean is closer to 0 which suggest that the data is really imbalanced, which will require fixing later down the line, ideally it would be better to be closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing\n",
    "Preprocessing the data with ways that are under the NLP PreProcessing Standards, as in:\n",
    "- Tokenizing.\n",
    "- Lowercasing.\n",
    "- Removal of Stopwords.\n",
    "- Removing Non-English words.\n",
    "- Lemmatization/Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting up a sentence or a corpus into plain words (tokens) for variable reasons:\n",
    "- To know which words are present.\n",
    "- Count them.\n",
    "- Et cetera.\n",
    "\n",
    "In my implementation, I will be using the `regexp_tokenizer()` in order to tokenize the text within the `comment_text` column.\n",
    "\n",
    "As well as keeping the tokens within the same dataframe as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a specific column within a passed DataFrame\n",
    "def tokenize(dataframe: pd.DataFrame, colname: str):\n",
    "    return [regexp_tokenize(row, upperlower_words) for row in dataframe[colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the tokenize() function on a new column within the main DataFrame\n",
    "txt_dat['tokens'] = tokenize(txt_dat, 'comment_text')\n",
    "\n",
    "# Checking if the function worked\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the sentences have been tokenised, each word is its own string, within a list of strings under the new `tokens` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case Words\n",
    "By conventional standards, changing the case of the words into lowercase has been agreed upon to negate the need to account for case sensitivity with the operations that follow to preprocess text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lowercase text\n",
    "def token_lower(token_list: list):\n",
    "    return [token.lower() for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the column\n",
    "# Iterable is of type \"list\"\n",
    "txt_dat['tokens'] = [token_lower(row) for row in txt_dat['tokens']]\n",
    "\n",
    "# Displaying results of applying the above function\n",
    "txt_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining how the function works first before observation.\n",
    "\n",
    "Take the column `tokens`, it contains lists of tokens for each comment like: `[Explanation, Why, the, edits, made, under, my...]`.\n",
    "\n",
    "Said list is what is being iterated on, which gets passed to the function, so for every iteration a new list of words gets passed to the function; but within the function, we iterate over each item within said list, so `'Explanantion` and `'Why'`, etc..\n",
    "\n",
    "Therefore, passing over each token, we apply the `.lower()` function for the `str` data type; thus we get a lowercase version of the string.\n",
    "\n",
    "After every iteration of lists, they get returned and reassigned where the original list used to be.\n",
    "\n",
    "As for observation, we can see that the words did indeed become lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "Removing of stopwords drastically decreases the amount of words that need to be taken into consideration by the model, and contribute little to nothing regarding meaning.\n",
    "\n",
    "Removing them would shave off redundant computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stopwords\n",
    "def stopword_remover(wordlist: list):\n",
    "    return [word for word in wordlist if word not in ENGLISH_STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying word count for first few lists\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "# Iterating over column\n",
    "# Iterable is the list of\n",
    "# Lowercase english words\n",
    "txt_dat['tokens'] = [stopword_remover(row) for row in txt_dat['tokens']]\n",
    "\n",
    "print([len(tokenlist) for tokenlist in txt_dat['tokens'].iloc[:5]])\n",
    "\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by the decrease in the word count per token list, there was indeed a removal of redundant words.\n",
    "\n",
    "As for how the function works, it is similar to how the previous ones work, passing the iterated list to iterate over the tokens within and apply a function; in our case it is a conditional to just include words that are not within the collection of words which are considered stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization & Stemming\n",
    "Lemmatising/Stemming both have the same target in mind, to reach the root of the word but their difference is the algorithm used.\n",
    "\n",
    "Stemming utilises a \"Porter\" algorithm which essentially just chops off common word endings, due to Stemmer using a crude old method, which is aimed for speed and efficiency, unlike lemmatizaton which morphologically analyses lexical changes in words to revert them back to their roots, unlike the chopping of \"commonly found prefixes/suffixes\" which stemming does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(wordlist: list):\n",
    "    return [WordNetLemmatizer().lemmatize(token) for token in wordlist]\n",
    "\n",
    "def stem(wordlist):\n",
    "    return [SnowballStemmer(\"english\").stem(token) for token in wordlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, after some look up, lemmatization is beneficial to reduce the word to its base, in a more human context and understanding, mainly used in chatbots, etc.\n",
    "\n",
    "Whilest the stemmer is used to purely chop off the words' endings to get to the base, used more in sentiment analysis, which is our current case with the binary classification, hence it will be used after lemmatizer and english word remover are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-English Words Removal\n",
    "Due to the nature of the internet, it is bound that people don't use proper English words, but due to current knowledge of NLP process, it would be better to remove words that do not belong to the English language as a whole.\n",
    "\n",
    "An important note to keep in mind is, calling this after lemmatization/stemming, due to `words` not having all forms of a singular word which may cause it to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Non-English word remover\n",
    "def englishify(wordlist: list):\n",
    "    return [word for word in wordlist if word in ENGLISH_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "txt_dat['tokens'] = [englishify(row) for row in txt_dat['tokens']]\n",
    "\n",
    "# Print some entries\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up NLP Preprocessing\n",
    "In this section, a whole function will take in a column in order to apply ***all*** the above functions in one go.\n",
    "\n",
    "Description and explanation for function order will be explained in either comments or a markdown cell following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, colname: str):\n",
    "    # List to hold the lists of tokens\n",
    "    tokenlist = tokenize(df, colname)\n",
    "\n",
    "    # Get number of tokens originally\n",
    "    token_num = sum([len(t_list) for t_list in tokenlist])\n",
    "\n",
    "    # Lowercase list of token lists\n",
    "    # t_list -> (t)oken_list\n",
    "    lower_tokens = [token_lower(t_list) for t_list in tokenlist]\n",
    "\n",
    "    # Stopword-free list\n",
    "    # tl_list -> (t)oken(l)lowercase_list\n",
    "    stopwordless = [stopword_remover(tl_list) for tl_list in lower_tokens]\n",
    "\n",
    "    # List of token lemmas\n",
    "    # tls_list -> (t)oken(l)owercase(s)topwordless_list\n",
    "    lemmas = [lemmatize(tls_list) for tls_list in stopwordless]\n",
    "\n",
    "    # List of Lemmas that only exist in english\n",
    "    # tlsl_list -> (t)oken(l)owercase(s)topwordless(l)lemma_list\n",
    "    englishified = [englishify(tlsl_list) for tlsl_list in lemmas]\n",
    "\n",
    "    # Get number of tokens after processing\n",
    "    # (Englishify is the last function to remove)\n",
    "    proc_num = sum([len(tlsle_list) for tlsle_list in englishified])\n",
    "\n",
    "    # Reduction\n",
    "    reduced_by = 1 - proc_num / token_num\n",
    "\n",
    "    # Print counts and percentage\n",
    "    print(\"Original Token Count : {}\\nProcessed Token Count: {}\\nReduction Percentage : {:.2%}\".format(token_num, proc_num, reduced_by))\n",
    "\n",
    "    # Return tuple of token list and processed tokens\n",
    "    return (tokenlist, [stem(word) for word in englishified])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation for how the wrap-up function works:\n",
    "- Passing the DataFrame to be processed, along with the column name to be tokenized.\n",
    "    - Each row within the column gets tokenized with use of the predefined RegEx expressions up above.\n",
    "- Calculating the number of tokens to get a feel for how much we'll be dealing with.\n",
    "- Applying a lowercase function to ever list within the list of lists assigned to the output of the tokenization function.\n",
    "- Removing stopwords from the tokens.\n",
    "- Lemmatizing the words, to reach the root of the word (Useful for removing non-English words later).\n",
    "- Removal of Non-English words.\n",
    "- Calculating the amount of tokens, `englishify()` is the last function that removes any tokens, afterwhich the reduction of tokens is calculated.\n",
    "- Printing the calculated numbers.\n",
    "- Returning a tuple of tokenised words and stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning output tuple to \"new\" column of tokens and new for processed\n",
    "txt_dat['tokens'], txt_dat['processed'] = preprocess(txt_dat, 'comment_text')\n",
    "\n",
    "# Printing to see progress\n",
    "txt_dat.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Using PyTorch, a transformer will be implemented from scratch to classify comments into toxic or non-toxic.\n",
    "\n",
    "Research Paper used as reference:\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "Website Articles that helped:\n",
    "- [The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "- [Text Classifier with Pytorch Transformer](https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/)\n",
    "- [Positional Encoding](https://theaisummer.com/positional-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch                                    # PyTorch\n",
    "import torch.nn    as nn                        # PyTorch Neural Networks\n",
    "import torch.optim as optim                     # Optimizers\n",
    "from torchtext import data                      # Text Data Handling\n",
    "from torch.nn  import functional as F           # Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
    "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.voc_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding(input) * (self.d_model ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add ***\"positional encodings\"*** to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed.\n",
    "\n",
    "Thus, we will be working with the following trig functions of different frequencies:\n",
    "$$ PE(pos, 2i) = sin(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "$$ PE(pos, 2i + 1) = cos(pos / 1000^{2\\cdot i / d_{model}}) $$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi^{[1]}$. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of\n",
    "$PE_{pos}$.\n",
    "\n",
    "---\n",
    "\n",
    "[1]: In this implementation, I will be using a logarithmic function instead of sinusoidal, due to finding that log tends to be more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        # https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Create a vector of shape (seq_len)\n",
    "        numerator = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "\n",
    "        # Create a vector of shape (d_model)\n",
    "        denominator = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(numerator * denominator)\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(numerator * denominator)\n",
    "\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        \n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "The main star of the show and the key that emphasizes why the transformer is the better architecture.\n",
    "\n",
    "Through remembering the prompts and contextually \"understanding\" the words whilst taking into consideration the meaning and value of the prior words.\n",
    "\n",
    "Hence, the Multi-Head Attetntion formula is described by the following equation:\n",
    "$$ MultiHeadAttention(Q, K, V) = Concat(head_1, \\ldots, head_h)W^O$$\n",
    "\n",
    "Where each $h_i$ is derived from the following formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{model}}})\\cdot V$$\n",
    "\n",
    "In this particular paper, the model has the hyperparameter of $h^{[1]}$ set to $8$, such that $d_k = d_v = d_{model} / h = 64$\n",
    "\n",
    "Softmax:\n",
    "$$ softmax(\\vec{z})_{i} = \\frac{e^{z_{i}}}{\\sum^{n}_{j=1} e^{z_{j}}}$$\n",
    "Where:\n",
    "- $\\vec{z} \\rightarrow$ Input Vector.\n",
    "- $e^{z_i} \\rightarrow$ Standard Exponentiation for Input vector.\n",
    "- $e^{z_j} \\rightarrow$ Standard Exponentiation for Output Vector.\n",
    "- $n \\rightarrow$ number of classes in multi-class classifier.\n",
    "\n",
    "---\n",
    "[1]: h is the number of attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key   = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$ FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is $d_{model} = {512}$, and the inner-layer has dimensionality\n",
    "$d_{ff} = {2048}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuidual Connection\n",
    "The skip layer between some MHA and FFN blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "Consisting of $6^{[1]}$ layers of the same process. From the original design we can also see that there is a skip forward from:\n",
    "- Before the MHA (Multi-Head Attention)\n",
    "- Before the FFN (Feed Forward Network)\n",
    "\n",
    "As well as consisting of $4$ blocks in total for each Encoder Block:\n",
    "- Multi-Head Attention.\n",
    "- First Normalization.\n",
    "- Feed-Foward Network.\n",
    "- Second Normalization.\n",
    "\n",
    "It would be pointless if we just continuously passed the output from one block without transforming it in any way or shape, hence the introudction of the $LayerNorm(x + SubLayer(x))$ function, where $SubLayer(x)$ is a function produced by the layer itself to facilitate the residual connections, and produces an output of dimension $d_{model} = 512$.\n",
    "\n",
    "---\n",
    "\n",
    "[1]: Included in the original Paper, Section 3.1; Encoder Paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of encoder\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "# N Layers of encoders\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two\n",
    "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
    "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
    "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
    "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of Decoder\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "# N layers of Decoders\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crescendo\n",
    "Bringing all the building blocks and pieces to shape together **The Transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, src_embed: InputEmbedding, src_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "def build_transformer(src_vocab_size: int, src_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the encoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, src_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, src_embed, src_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it All Together\n",
    "Dividing Data and turning it into pytorch tensor compatible input.\n",
    "\n",
    "Input into the transformer implemented above and evaluating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
